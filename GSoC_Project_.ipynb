{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic AI Tests 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML for conda env for Task 1\n",
    "\n",
    "# # environment.yml\n",
    "# # Conda environment definition for the Feynman Dataset Preprocessing script.\n",
    "# #\n",
    "# # This environment contains the packages needed to run the Python script that\n",
    "# # downloads the Feynman data, preprocesses equations, trains a tokenizer,\n",
    "# # and maps feature files.\n",
    "# #\n",
    "# # To create this environment:\n",
    "# # conda env create -f environment.yml\n",
    "# #\n",
    "# # To activate this environment:\n",
    "# # conda activate feynman_preprocess_env\n",
    "# #\n",
    "# # To update the environment using this file (after saving changes):\n",
    "# # conda activate feynman_preprocess_env\n",
    "# # conda env update --file environment.yml --prune\n",
    "# #\n",
    "# # To remove the environment:\n",
    "# # conda deactivate\n",
    "# # conda env remove -n feynman_preprocess_env\n",
    "\n",
    "# name: feynman_preprocess_env # Changed name to reflect the specific task\n",
    "\n",
    "# channels:\n",
    "#   - conda-forge   # Primary channel for broad package availability\n",
    "#   - defaults      # Default conda channel\n",
    "\n",
    "# dependencies:\n",
    "#   # --- Python Version ---\n",
    "#   # Using Python 3.10 - a well-supported recent version. Adjust if needed (e.g., 3.9, 3.11).\n",
    "#   - python=3.10\n",
    "\n",
    "#   # --- Core Data Handling & I/O ---\n",
    "#   - pandas      # For reading CSV files\n",
    "#   - numpy       # For loading numeric data (optional, but used in load_numeric_data)\n",
    "#   - requests    # For downloading the dataset files from URLs\n",
    "\n",
    "#   # --- Pip Tool ---\n",
    "#   - pip         # Required for installing pip packages listed below\n",
    "\n",
    "#   # --- Optional Development Tools ---\n",
    "#   # Uncomment these lines if you plan to use Jupyter notebooks within this environment\n",
    "#   # - jupyter\n",
    "#   # - ipykernel\n",
    "\n",
    "#   # --- Pip dependencies ---\n",
    "#   # Install specific libraries, often those frequently updated or primarily distributed via pip\n",
    "#   - pip:\n",
    "#     - transformers  # Hugging Face library (needed for PreTrainedTokenizerFast)\n",
    "#     - tokenizers    # Hugging Face library (needed for ByteLevelBPETokenizer and training)\n",
    "\n",
    "# # Notes:\n",
    "# # - This environment focuses solely on the requirements of the preprocessing script.\n",
    "# # - It does *not* include PyTorch or other deep learning frameworks. If you need those\n",
    "# #   for model training later, you can either:\n",
    "# #     a) Add them to this file (e.g., add 'pytorch::pytorch', 'pytorch::torchvision', etc.\n",
    "# #        under dependencies and potentially add the 'pytorch'/'nvidia' channels back).\n",
    "# #     b) Create a separate environment for training that includes these packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Task 1.1. Dataset preprocessing \n",
    "Dataset:\n",
    "\n",
    "https://space.mit.edu/home/tegmark/aifeynman.html \n",
    "Note: The authors of this dataset are not affiliated with ML4SCI\n",
    "\n",
    "Description:\n",
    "Download the Feynman_with_units.tar.gz features and corresponding FeynmanEquations.csv targets. Preprocess and tokenize the target data and document your rationale for choice of tokenization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 11:46:59,566 - INFO - --- Feynman Dataset Preprocessing Script (Using Local Data) ---\n",
      "2025-04-22 11:46:59,567 - INFO - [Step 1] Verifying local data paths...\n",
      "2025-04-22 11:46:59,567 - INFO - Found Equations CSV: /home/nikitas/Desktop/Miche/GOOGLE/FeynmanEquations.csv\n",
      "2025-04-22 11:46:59,567 - INFO - Found unpacked Features directory: /home/nikitas/Desktop/Miche/GOOGLE/Feynman_with_units\n",
      "2025-04-22 11:46:59,567 - INFO - [Step 2] Download Data (Skipped - Using local files)\n",
      "2025-04-22 11:46:59,567 - INFO - [Step 3] Unpack Features Archive (Skipped - Using local directory)\n",
      "2025-04-22 11:46:59,567 - INFO - [Step 4] Training Byte-Level BPE Tokenizer on 'Formula' column...\n",
      "2025-04-22 11:46:59,576 - INFO - Confirmed equation column 'Formula' exists.\n",
      "2025-04-22 11:46:59,576 - INFO - Tokenizer output directory: /home/nikitas/Desktop/Miche/GOOGLE/feynman_tokenizer\n",
      "2025-04-22 11:46:59,576 - INFO - Tokenizer Settings: vocab_size=10000, min_frequency=2\n",
      "2025-04-22 11:46:59,576 - INFO - Initializing equation iterator for column 'Formula' from FeynmanEquations.csv...\n",
      "2025-04-22 11:46:59,578 - INFO - Successfully retrieved first equation from iterator.\n",
      "2025-04-22 11:46:59,578 - INFO - Starting tokenizer training...\n",
      "2025-04-22 11:46:59,579 - INFO - Equation iterator finished yielding 100 equations.\n",
      "2025-04-22 11:46:59,583 - INFO - Tokenizer training complete. Final vocab size: 330\n",
      "2025-04-22 11:46:59,583 - INFO - Tokenizer vocabulary/merges saved to feynman_tokenizer.\n",
      "2025-04-22 11:46:59,584 - INFO - Full tokenizer config saved to: feynman_tokenizer/tokenizer.json\n",
      "2025-04-22 11:46:59,584 - INFO - [Step 5] Wrapping tokenizer with Hugging Face PreTrainedTokenizerFast...\n",
      "2025-04-22 11:46:59,585 - INFO - Hugging Face tokenizer wrapper created successfully.\n",
      "2025-04-22 11:46:59,585 - INFO - Testing tokenizer encoding/decoding...\n",
      "2025-04-22 11:46:59,585 - INFO - Initializing equation iterator for column 'Formula' from FeynmanEquations.csv...\n",
      "2025-04-22 11:46:59,587 - INFO - Test Equation Sample: exp(-theta**2/2)/sqrt(2*pi)\n",
      "2025-04-22 11:46:59,587 - INFO -  -> Tokens (14): ['exp', '(-', 'theta', '**', '2', '/', '2', ')/', 'sqrt', '(', '2', '*', 'pi', ')']\n",
      "2025-04-22 11:46:59,587 - INFO -  -> Encoded IDs (14): [290, 304, 283, 261, 22, 19, 22, 298, 281, 12, 22, 14, 264, 13]\n",
      "2025-04-22 11:46:59,587 - INFO -  -> Decoded (clean): exp(-theta**2/2)/sqrt(2*pi)\n",
      "2025-04-22 11:46:59,587 - INFO - [Step 6] Mapping CSV filenames to feature files in 'Feynman_with_units'...\n",
      "2025-04-22 11:46:59,588 - INFO - Loading filenames from 'Filename' column in FeynmanEquations.csv...\n",
      "2025-04-22 11:46:59,589 - INFO - Loaded 100 non-empty filenames.\n",
      "2025-04-22 11:46:59,589 - INFO - Checking for 100 potential feature files in Feynman_with_units...\n",
      "2025-04-22 11:46:59,591 - INFO - Finished checking 100 filenames.\n",
      "2025-04-22 11:46:59,591 - INFO - Found 100 corresponding feature files.\n",
      "2025-04-22 11:46:59,591 - INFO - Example mapping: 'I.6.2a' -> '/home/nikitas/Desktop/Miche/GOOGLE/Feynman_with_units/I.6.2a'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "          Rationale for Byte-Level BPE Tokenization\n",
      "======================================================================\n",
      "\n",
      "    Byte-Level Byte Pair Encoding (BBPE) was chosen for tokenizing the Feynman equations (targets) for several reasons:\n",
      "\n",
      "    1.  **Handles Diverse Characters:** Equations contain a wide mix of mathematical symbols (e.g., +, -, *, /), Greek letters (e.g., \\theta, \\omega), numbers, standard letters (variable names), and LaTeX-like commands (e.g., \\sqrt, \\frac, ^, _). BBPE operates at the byte level initially, meaning it can handle *any* character without needing a predefined vocabulary of all possible symbols.\n",
      "\n",
      "    2.  **Subword Information:** BBPE learns to merge frequent byte sequences into tokens. This allows it to represent common mathematical operators, function names (like 'sin', 'cos'), common variable fragments, and even parts of LaTeX commands as single tokens, while still being able to break down rare or unseen sequences into smaller, known units (subwords or individual bytes). This is beneficial for capturing structure within the equations.\n",
      "\n",
      "    3.  **Robustness to Variations:** Unlike word-level tokenization (which would struggle with defining \"words\" in equations), BBPE is less sensitive to minor variations in notation or typos. It can tokenize novel combinations of symbols or slightly different variable names by breaking them down.\n",
      "\n",
      "    4.  **Controlled Vocabulary Size:** While equations can be infinitely complex, the underlying set of characters and common mathematical constructs is limited. BBPE allows controlling the final vocabulary size (`VOCAB_SIZE`) by limiting the number of merge operations, preventing an excessively large vocabulary while capturing the most frequent and meaningful patterns.\n",
      "\n",
      "    5.  **No Unknown Tokens (Almost):** Since it works at the byte level, BBPE inherently avoids the \"unknown token\" (<unk>) problem for individual characters. Unknown *sequences* are simply represented by the tokens corresponding to their constituent bytes or learned subwords. We still include <unk> as a special token for robustness or potential future use, but it's less critical than in word-level tokenizers.\n",
      "    \n",
      "======================================================================\n",
      "                    Usage Summary\n",
      "======================================================================\n",
      "\n",
      "[Tokenizer Usage]\n",
      " - The Hugging Face tokenizer object is loaded and ready for use.\n",
      " - Tokenizer files are saved in: /home/nikitas/Desktop/Miche/GOOGLE/feynman_tokenizer\n",
      " - Example:\n",
      "   ```python\n",
      "   # Assuming 'hf_tokenizer' is the loaded PreTrainedTokenizerFast object\n",
      "   equation = 'F = G * m1 * m2 / r**2'\n",
      "   encoded = hf_tokenizer(equation)\n",
      "   print('Encoded IDs:', encoded['input_ids'])\n",
      "   print('Decoded:', hf_tokenizer.decode(encoded['input_ids']))\n",
      "   ```\n",
      "\n",
      "[Feature Data Usage]\n",
      " - The 'numeric_file_paths' dictionary maps filenames to feature file Paths.\n",
      " - Feature files are located in: /home/nikitas/Desktop/Miche/GOOGLE/Feynman_with_units\n",
      " - Total mapped files: 100\n",
      " - Example:\n",
      "   ```python\n",
      "   # Assuming 'numeric_file_paths' is the dictionary and 'load_numeric_data' is defined\n",
      "   filename = 'I.6.2a'\n",
      "   if filename in numeric_file_paths:\n",
      "       # data_array = load_numeric_data(filename, numeric_file_paths)\n",
      "       file_path = numeric_file_paths[filename]\n",
      "       print(f'Path to feature file: {file_path}')\n",
      "       # data = np.loadtxt(file_path)\n",
      "   ```\n",
      "\n",
      "   Loading one example feature file for demonstration:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 11:46:59,881 - INFO - --- Feynman Dataset Preprocessing Script (Using Local Data) FINISHED ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   > Successfully loaded 'I.6.2a' with shape (1000000, 2)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tarfile # Keep for potential type hints if needed, but logic removed\n",
    "import requests # Keep for potential type hints if needed, but logic removed\n",
    "import shutil # Keep for potential type hints if needed, but logic removed\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Optional, List, Dict, Any\n",
    "import logging\n",
    "import os # Added to check environment variable\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Make sure to install required libraries:\n",
    "# pip install pandas requests tokenizers transformers numpy markupsafe\n",
    "try:\n",
    "    import pandas as pd\n",
    "    from tokenizers import ByteLevelBPETokenizer # type: ignore\n",
    "    from transformers import PreTrainedTokenizerFast\n",
    "    import numpy as np\n",
    "    import markupsafe # Explicitly import to check if installed\n",
    "except ImportError as e:\n",
    "    logging.error(f\"Error importing libraries: {e}\")\n",
    "    logging.error(\"Please ensure required libraries are installed in the 'feynman_tokenizer_env' environment:\")\n",
    "    logging.error(\"  conda install pandas numpy requests markupsafe -c conda-forge\")\n",
    "    logging.error(\"  pip install transformers tokenizers\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Configuration --- MODIFIED TO USE LOCAL PATHS ---\n",
    "\n",
    "# Assume the script is run from the project root directory (e.g., ~/Desktop/Miche/GOOGLE)\n",
    "PROJECT_ROOT = Path(\".\")\n",
    "\n",
    "# Direct paths to existing data within the project root directory\n",
    "EQUATIONS_CSV_PATH = PROJECT_ROOT / \"FeynmanEquations.csv\"\n",
    "FEATURES_EXTRACTED_PATH = PROJECT_ROOT / \"Feynman_with_units\" # The existing unpacked dir\n",
    "\n",
    "# Output directory for the tokenizer (will be created in project root)\n",
    "TOKENIZER_OUTPUT_DIR = PROJECT_ROOT / \"feynman_tokenizer\"\n",
    "\n",
    "# --- Original URLs and Paths (Commented out - No longer used) ---\n",
    "# EQUATIONS_CSV_URL = \"https://space.mit.edu/home/tegmark/aifeynman/FeynmanEquations.csv\"\n",
    "# FEATURES_TAR_URL = \"https://space.mit.edu/home/tegmark/aifeynman/Feynman_with_units.tar.gz\"\n",
    "# BASE_DATA_DIR = Path(\"./feynman_data\") # No longer using this subdir for inputs\n",
    "# EQUATIONS_CSV_PATH_ORIG = BASE_DATA_DIR / \"FeynmanEquations.csv\"\n",
    "# FEATURES_TAR_PATH = BASE_DATA_DIR / \"Feynman_with_units.tar.gz\"\n",
    "# FEATURES_DIR_NAME = \"Feynman_with_units\"\n",
    "# FEATURES_EXTRACTED_PATH_ORIG = BASE_DATA_DIR / FEATURES_DIR_NAME\n",
    "\n",
    "# CSV Column Names\n",
    "FILENAME_COLUMN = 'Filename'\n",
    "EQUATION_COLUMN = 'Formula'\n",
    "\n",
    "# Tokenizer Settings\n",
    "VOCAB_SIZE = 10_000\n",
    "MIN_FREQUENCY = 2\n",
    "SPECIAL_TOKENS = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "\n",
    "# --- Helper Functions --- (Download and Unpack are no longer needed by main logic)\n",
    "\n",
    "def download_file(url: str, destination: Path) -> bool:\n",
    "    \"\"\"Downloads a file from a URL to a destination path if it doesn't exist. (NO LONGER CALLED BY main)\"\"\"\n",
    "    if destination.exists():\n",
    "        logging.info(f\"File already exists: {destination}\")\n",
    "        return True\n",
    "    # ... (rest of download logic remains here in case needed elsewhere, but won't be executed)\n",
    "    logging.info(f\"Attempting download (called unexpectedly): {url} to {destination}\")\n",
    "    # Added security context modification attempt for DH_KEY_TOO_SMALL - USE WITH CAUTION\n",
    "    # Requires specific OpenSSL version potentially. May not work.\n",
    "    session = requests.Session()\n",
    "    try:\n",
    "        # Try forcing lower security level ciphers - WARNING: REDUCES SECURITY\n",
    "        logging.warning(\"Attempting download with reduced SSL security settings (SECLEVEL=1) due to potential DH_KEY_TOO_SMALL error.\")\n",
    "        CIPHERS = ('DEFAULT@SECLEVEL=1')\n",
    "        from requests.adapters import HTTPAdapter\n",
    "        from urllib3.util.ssl_ import create_urllib3_context\n",
    "\n",
    "        class CustomHttpAdapter(HTTPAdapter):\n",
    "            def init_poolmanager(self, connections, maxsize, block=False):\n",
    "                context = create_urllib3_context(ciphers=CIPHERS)\n",
    "                self.poolmanager = requests.packages.urllib3.PoolManager(\n",
    "                    num_pools=connections, maxsize=maxsize, block=block, ssl_context=context\n",
    "                )\n",
    "        session.mount('https://', CustomHttpAdapter())\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not apply custom SSL Adapter: {e}. Proceeding with default settings.\")\n",
    "        # Fallback to default session if adapter setup fails\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, stream=True, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        destination.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(destination, 'wb') as f:\n",
    "            shutil.copyfileobj(response.raw, f)\n",
    "        logging.info(f\"Successfully downloaded {destination.name}\")\n",
    "        return True\n",
    "    except requests.exceptions.SSLError as e:\n",
    "        logging.error(f\"SSL Error during download {url}: {e}\")\n",
    "        logging.error(\"This often happens if the server uses outdated security (e.g., DH_KEY_TOO_SMALL).\")\n",
    "        logging.error(\"Consider downloading the file manually via browser and placing it at the destination path.\")\n",
    "        if destination.exists(): destination.unlink(missing_ok=True)\n",
    "        return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Failed to download {url}: {e}\")\n",
    "        if destination.exists(): destination.unlink(missing_ok=True)\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during download: {e}\")\n",
    "        if destination.exists(): destination.unlink(missing_ok=True)\n",
    "        return False\n",
    "\n",
    "\n",
    "def unpack_tar_gz(tar_path: Path, extract_to: Path) -> bool:\n",
    "    \"\"\"Unpacks a .tar.gz file to a specified directory. (NO LONGER CALLED BY main)\"\"\"\n",
    "    if not tar_path.is_file():\n",
    "        logging.error(f\"Archive file not found or is not a file: {tar_path}\")\n",
    "        return False\n",
    "    if extract_to.is_dir() and any(extract_to.iterdir()):\n",
    "         logging.info(f\"Target directory {extract_to} already exists and is not empty. Assuming unpacked.\")\n",
    "         return True\n",
    "    # ... (rest of unpack logic remains here but won't be executed)\n",
    "    logging.info(f\"Attempting unpack (called unexpectedly): {tar_path.name} to {extract_to.parent}...\")\n",
    "    try:\n",
    "        extract_to.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=extract_to.parent)\n",
    "            if extract_to.is_dir():\n",
    "                 logging.info(f\"Successfully unpacked. Target directory: {extract_to}\")\n",
    "                 return True\n",
    "            else:\n",
    "                 logging.warning(f\"Tar file unpacked, but expected directory '{extract_to.name}' not found directly in '{extract_to.parent}'.\")\n",
    "                 # Simplified check - assuming failure if exact name doesn't match\n",
    "                 return False\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during unpacking: {e}\")\n",
    "        if extract_to.exists(): shutil.rmtree(extract_to, ignore_errors=True)\n",
    "        return False\n",
    "\n",
    "\n",
    "def equation_iterator(csv_file_path: Path, target_column: str) -> Iterator[str]:\n",
    "    \"\"\"Creates a generator to efficiently yield non-empty equations from a CSV column.\"\"\"\n",
    "    # (This function remains unchanged)\n",
    "    chunk_size = 1000\n",
    "    logging.info(f\"Initializing equation iterator for column '{target_column}' from {csv_file_path}...\")\n",
    "    if not csv_file_path.is_file():\n",
    "        logging.error(f\"Equation CSV file not found: {csv_file_path}\")\n",
    "        return\n",
    "    processed_count = 0\n",
    "    try:\n",
    "        chunk_iterator = pd.read_csv(\n",
    "            csv_file_path, usecols=[target_column], chunksize=chunk_size,\n",
    "            skipinitialspace=True, low_memory=False, dtype={target_column: str},\n",
    "            on_bad_lines='warn'\n",
    "        )\n",
    "        for i, chunk in enumerate(chunk_iterator):\n",
    "            if target_column not in chunk.columns:\n",
    "                 logging.error(f\"Column '{target_column}' not found in CSV chunk {i+1}. Stopping.\")\n",
    "                 return\n",
    "            for equation in chunk[target_column]:\n",
    "                if pd.isna(equation): continue\n",
    "                eq_str = str(equation).strip()\n",
    "                if not eq_str or eq_str.lower() == 'nan': continue\n",
    "                yield eq_str\n",
    "                processed_count += 1\n",
    "        logging.info(f\"Equation iterator finished yielding {processed_count} equations.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed reading CSV chunks from {csv_file_path}: {e}\")\n",
    "        return\n",
    "\n",
    "def load_numeric_data(filename: str, paths_dict: Dict[str, Path]) -> Optional[np.ndarray]:\n",
    "    \"\"\"Loads numeric data from a file specified by its base filename using a path map.\"\"\"\n",
    "    # (This function remains unchanged)\n",
    "    file_path = paths_dict.get(filename)\n",
    "    if file_path is None: return None\n",
    "    if not file_path.is_file(): return None\n",
    "    try:\n",
    "        data_array: np.ndarray = np.loadtxt(file_path)\n",
    "        return data_array\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load numeric file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Execution Logic --- MODIFIED ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Runs the main script logic using locally available data.\"\"\"\n",
    "    logging.info(\"--- Feynman Dataset Preprocessing Script (Using Local Data) ---\")\n",
    "\n",
    "    # 1. Verify Local Data Paths\n",
    "    logging.info(\"[Step 1] Verifying local data paths...\")\n",
    "    csv_ok = False\n",
    "    features_dir_ok = False\n",
    "\n",
    "    if EQUATIONS_CSV_PATH.is_file():\n",
    "        logging.info(f\"Found Equations CSV: {EQUATIONS_CSV_PATH.resolve()}\")\n",
    "        csv_ok = True\n",
    "    else:\n",
    "        logging.error(f\"Equations CSV file not found at expected location: {EQUATIONS_CSV_PATH.resolve()}\")\n",
    "        logging.error(\"Please ensure 'FeynmanEquations.csv' is in the same directory as the script.\")\n",
    "\n",
    "    if FEATURES_EXTRACTED_PATH.is_dir():\n",
    "        logging.info(f\"Found unpacked Features directory: {FEATURES_EXTRACTED_PATH.resolve()}\")\n",
    "        features_dir_ok = True\n",
    "    else:\n",
    "        logging.error(f\"Unpacked features directory not found at expected location: {FEATURES_EXTRACTED_PATH.resolve()}\")\n",
    "        logging.error(\"Please ensure the 'Feynman_with_units' directory (containing data files like I.10.7 etc.) is in the same directory as the script.\")\n",
    "\n",
    "    if not csv_ok:\n",
    "        logging.error(\"Cannot proceed without the Equations CSV file.\")\n",
    "        sys.exit(1)\n",
    "    # We can proceed with tokenization even if features dir is missing, but mapping will fail.\n",
    "\n",
    "    # 2. Download Data (SKIPPED)\n",
    "    logging.info(\"[Step 2] Download Data (Skipped - Using local files)\")\n",
    "\n",
    "    # 3. Unpack Features Archive (SKIPPED)\n",
    "    logging.info(\"[Step 3] Unpack Features Archive (Skipped - Using local directory)\")\n",
    "\n",
    "    # 4. Train BPE Tokenizer (Uses verified EQUATIONS_CSV_PATH)\n",
    "    logging.info(f\"[Step 4] Training Byte-Level BPE Tokenizer on '{EQUATION_COLUMN}' column...\")\n",
    "    tokenizer_json_path: Optional[Path] = None\n",
    "    tokenizer_trained = False\n",
    "\n",
    "    # Check equation column exists\n",
    "    try:\n",
    "        df_head = pd.read_csv(EQUATIONS_CSV_PATH, nrows=0, skipinitialspace=True)\n",
    "        if EQUATION_COLUMN not in df_head.columns:\n",
    "            logging.error(f\"Equation column '{EQUATION_COLUMN}' not found in {EQUATIONS_CSV_PATH}. Cannot train tokenizer.\")\n",
    "            sys.exit(1)\n",
    "        logging.info(f\"Confirmed equation column '{EQUATION_COLUMN}' exists.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not read or verify columns in {EQUATIONS_CSV_PATH}: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Create tokenizer output dir\n",
    "    try:\n",
    "        TOKENIZER_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        logging.info(f\"Tokenizer output directory: {TOKENIZER_OUTPUT_DIR.resolve()}\")\n",
    "    except OSError as e:\n",
    "        logging.error(f\"Could not create tokenizer output directory {TOKENIZER_OUTPUT_DIR}: {e}. Tokenizer training will fail.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    logging.info(f\"Tokenizer Settings: vocab_size={VOCAB_SIZE}, min_frequency={MIN_FREQUENCY}\")\n",
    "    try:\n",
    "        bpe_tokenizer = ByteLevelBPETokenizer()\n",
    "        eq_iter = equation_iterator(EQUATIONS_CSV_PATH, EQUATION_COLUMN)\n",
    "        # Check iterator\n",
    "        try:\n",
    "            first_item = next(eq_iter)\n",
    "            logging.info(\"Successfully retrieved first equation from iterator.\")\n",
    "        except StopIteration:\n",
    "            logging.error(\"Equation iterator did not yield any data. Check CSV content.\")\n",
    "            first_item = None\n",
    "\n",
    "        if first_item is not None:\n",
    "            from itertools import chain\n",
    "            full_iterator = chain([first_item], eq_iter)\n",
    "            logging.info(\"Starting tokenizer training...\")\n",
    "            bpe_tokenizer.train_from_iterator(\n",
    "                full_iterator, vocab_size=VOCAB_SIZE, min_frequency=MIN_FREQUENCY, special_tokens=SPECIAL_TOKENS\n",
    "            )\n",
    "            logging.info(f\"Tokenizer training complete. Final vocab size: {bpe_tokenizer.get_vocab_size()}\")\n",
    "            bpe_tokenizer.save_model(str(TOKENIZER_OUTPUT_DIR))\n",
    "            logging.info(f\"Tokenizer vocabulary/merges saved to {TOKENIZER_OUTPUT_DIR}.\")\n",
    "            tokenizer_json_path = TOKENIZER_OUTPUT_DIR / \"tokenizer.json\"\n",
    "            bpe_tokenizer.save(str(tokenizer_json_path))\n",
    "            logging.info(f\"Full tokenizer config saved to: {tokenizer_json_path}\")\n",
    "            tokenizer_trained = True\n",
    "        else:\n",
    "            logging.error(\"Cannot train tokenizer because no valid equations were found.\")\n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Tokenizer training failed unexpectedly: {e}\")\n",
    "\n",
    "    # 5. Wrap with PreTrainedTokenizerFast\n",
    "    logging.info(\"[Step 5] Wrapping tokenizer with Hugging Face PreTrainedTokenizerFast...\")\n",
    "    hf_tokenizer: Optional[PreTrainedTokenizerFast] = None\n",
    "    if tokenizer_trained and tokenizer_json_path and tokenizer_json_path.is_file():\n",
    "        try:\n",
    "            # (Wrap logic remains the same)\n",
    "            hf_tokenizer = PreTrainedTokenizerFast(\n",
    "                tokenizer_file=str(tokenizer_json_path),\n",
    "                bos_token=SPECIAL_TOKENS[0], eos_token=SPECIAL_TOKENS[2],\n",
    "                unk_token=SPECIAL_TOKENS[3], pad_token=SPECIAL_TOKENS[1],\n",
    "                mask_token=SPECIAL_TOKENS[4]\n",
    "            )\n",
    "            logging.info(\"Hugging Face tokenizer wrapper created successfully.\")\n",
    "            # (Testing logic remains the same)\n",
    "            logging.info(\"Testing tokenizer encoding/decoding...\")\n",
    "            try:\n",
    "                test_eq_iter = equation_iterator(EQUATIONS_CSV_PATH, EQUATION_COLUMN)\n",
    "                first_equation = next(test_eq_iter, None)\n",
    "                del test_eq_iter\n",
    "                if first_equation:\n",
    "                     logging.info(f\"Test Equation Sample: {first_equation}\")\n",
    "                     tokens = hf_tokenizer.tokenize(first_equation)\n",
    "                     logging.info(f\" -> Tokens ({len(tokens)}): {tokens}\")\n",
    "                     encoded_ids = hf_tokenizer.encode(first_equation)\n",
    "                     logging.info(f\" -> Encoded IDs ({len(encoded_ids)}): {encoded_ids}\")\n",
    "                     decoded_clean = hf_tokenizer.decode(encoded_ids, skip_special_tokens=True)\n",
    "                     logging.info(f\" -> Decoded (clean): {decoded_clean}\")\n",
    "                else:\n",
    "                     logging.warning(\"Could not retrieve an equation for tokenizer testing.\")\n",
    "            except Exception as e:\n",
    "                logging.exception(f\"Error during tokenizer test encode/decode: {e}\")\n",
    "        except Exception as e:\n",
    "            logging.exception(f\"Failed to load tokenizer with PreTrainedTokenizerFast: {e}\")\n",
    "    elif not tokenizer_trained:\n",
    "        logging.warning(\"Skipping tokenizer wrapping: Tokenizer training failed or was skipped.\")\n",
    "    else:\n",
    "        logging.warning(f\"Skipping tokenizer wrapping: Tokenizer file '{tokenizer_json_path}' not found.\")\n",
    "\n",
    "    # 6. Map Filenames to Feature Files (Uses verified FEATURES_EXTRACTED_PATH)\n",
    "    logging.info(f\"[Step 6] Mapping CSV filenames to feature files in '{FEATURES_EXTRACTED_PATH}'...\")\n",
    "    numeric_file_paths: Dict[str, Path] = {}\n",
    "    filenames: List[str] = []\n",
    "    missing_files_count = 0\n",
    "\n",
    "    # Load filenames from CSV (uses verified EQUATIONS_CSV_PATH)\n",
    "    try:\n",
    "        logging.info(f\"Loading filenames from '{FILENAME_COLUMN}' column in {EQUATIONS_CSV_PATH}...\")\n",
    "        filenames_series = pd.read_csv(\n",
    "            EQUATIONS_CSV_PATH, usecols=[FILENAME_COLUMN], skipinitialspace=True, dtype={FILENAME_COLUMN: str}\n",
    "        )[FILENAME_COLUMN]\n",
    "        filenames = [fn.strip() for fn in filenames_series if pd.notna(fn) and str(fn).strip()]\n",
    "        logging.info(f\"Loaded {len(filenames)} non-empty filenames.\")\n",
    "        if not filenames: logging.warning(f\"No valid filenames found in CSV column '{FILENAME_COLUMN}'.\")\n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Failed to read filenames column '{FILENAME_COLUMN}' from CSV: {e}\")\n",
    "        logging.warning(\"Skipping feature file mapping due to error reading filenames.\")\n",
    "\n",
    "    # Perform mapping if filenames loaded and features directory exists\n",
    "    if filenames and features_dir_ok: # Use the features_dir_ok flag from Step 1\n",
    "        logging.info(f\"Checking for {len(filenames)} potential feature files in {FEATURES_EXTRACTED_PATH}...\")\n",
    "        for fn in filenames:\n",
    "            potential_path = FEATURES_EXTRACTED_PATH / fn\n",
    "            if potential_path.is_file():\n",
    "                numeric_file_paths[fn] = potential_path.resolve() # Store absolute path\n",
    "            else:\n",
    "                missing_files_count += 1\n",
    "\n",
    "        found_files_count = len(numeric_file_paths)\n",
    "        logging.info(f\"Finished checking {len(filenames)} filenames.\")\n",
    "        logging.info(f\"Found {found_files_count} corresponding feature files.\")\n",
    "        if missing_files_count > 0:\n",
    "            logging.warning(f\"Did not find {missing_files_count} expected feature files in {FEATURES_EXTRACTED_PATH}.\")\n",
    "\n",
    "        if numeric_file_paths:\n",
    "             example_fn = next(iter(numeric_file_paths))\n",
    "             logging.info(f\"Example mapping: '{example_fn}' -> '{numeric_file_paths[example_fn]}'\")\n",
    "        elif found_files_count == 0 and filenames:\n",
    "             logging.warning(f\"No feature files mapped despite having filenames. Check directory '{FEATURES_EXTRACTED_PATH}'.\")\n",
    "\n",
    "    elif not filenames:\n",
    "         logging.info(\"Skipping feature mapping: No filenames loaded.\")\n",
    "    elif not features_dir_ok:\n",
    "         logging.info(f\"Skipping feature mapping: Feature directory '{FEATURES_EXTRACTED_PATH}' not found or verified.\")\n",
    "\n",
    "    # 7. Rationale and Usage Summary (Printed to console - No changes needed here)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"          Rationale for Byte-Level BPE Tokenization\")\n",
    "    print(\"=\"*70)\n",
    "    # (Rationale text remains the same)\n",
    "    rationale = \"\"\"\n",
    "    Byte-Level Byte Pair Encoding (BBPE) was chosen for tokenizing the Feynman equations (targets) for several reasons:\n",
    "\n",
    "    1.  **Handles Diverse Characters:** Equations contain a wide mix of mathematical symbols (e.g., +, -, *, /), Greek letters (e.g., \\\\theta, \\\\omega), numbers, standard letters (variable names), and LaTeX-like commands (e.g., \\\\sqrt, \\\\frac, ^, _). BBPE operates at the byte level initially, meaning it can handle *any* character without needing a predefined vocabulary of all possible symbols.\n",
    "\n",
    "    2.  **Subword Information:** BBPE learns to merge frequent byte sequences into tokens. This allows it to represent common mathematical operators, function names (like 'sin', 'cos'), common variable fragments, and even parts of LaTeX commands as single tokens, while still being able to break down rare or unseen sequences into smaller, known units (subwords or individual bytes). This is beneficial for capturing structure within the equations.\n",
    "\n",
    "    3.  **Robustness to Variations:** Unlike word-level tokenization (which would struggle with defining \"words\" in equations), BBPE is less sensitive to minor variations in notation or typos. It can tokenize novel combinations of symbols or slightly different variable names by breaking them down.\n",
    "\n",
    "    4.  **Controlled Vocabulary Size:** While equations can be infinitely complex, the underlying set of characters and common mathematical constructs is limited. BBPE allows controlling the final vocabulary size (`VOCAB_SIZE`) by limiting the number of merge operations, preventing an excessively large vocabulary while capturing the most frequent and meaningful patterns.\n",
    "\n",
    "    5.  **No Unknown Tokens (Almost):** Since it works at the byte level, BBPE inherently avoids the \"unknown token\" (<unk>) problem for individual characters. Unknown *sequences* are simply represented by the tokens corresponding to their constituent bytes or learned subwords. We still include <unk> as a special token for robustness or potential future use, but it's less critical than in word-level tokenizers.\n",
    "    \"\"\"\n",
    "    print(rationale)\n",
    "    print(\"=\"*70)\n",
    "    print(\"                    Usage Summary\")\n",
    "    print(\"=\"*70)\n",
    "    # (Usage summary text remains largely the same, paths updated)\n",
    "    print(\"\\n[Tokenizer Usage]\")\n",
    "    if hf_tokenizer:\n",
    "        print(\" - The Hugging Face tokenizer object is loaded and ready for use.\")\n",
    "        print(f\" - Tokenizer files are saved in: {TOKENIZER_OUTPUT_DIR.resolve()}\")\n",
    "        print(\" - Example:\")\n",
    "        print(\"   ```python\")\n",
    "        print(\"   # Assuming 'hf_tokenizer' is the loaded PreTrainedTokenizerFast object\")\n",
    "        print(\"   equation = 'F = G * m1 * m2 / r**2'\")\n",
    "        print(\"   encoded = hf_tokenizer(equation)\")\n",
    "        print(\"   print('Encoded IDs:', encoded['input_ids'])\")\n",
    "        print(\"   print('Decoded:', hf_tokenizer.decode(encoded['input_ids']))\")\n",
    "        print(\"   ```\")\n",
    "    else:\n",
    "        print(\" - Tokenizer training or loading FAILED.\")\n",
    "        print(f\" - Check logs for errors. If training seemed to succeed, verify tokenizer files exist in: {TOKENIZER_OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "    print(\"\\n[Feature Data Usage]\")\n",
    "    if numeric_file_paths:\n",
    "        print(\" - The 'numeric_file_paths' dictionary maps filenames to feature file Paths.\")\n",
    "        print(f\" - Feature files are located in: {FEATURES_EXTRACTED_PATH.resolve()}\")\n",
    "        print(f\" - Total mapped files: {len(numeric_file_paths)}\")\n",
    "        print(\" - Example:\")\n",
    "        print(\"   ```python\")\n",
    "        print(\"   # Assuming 'numeric_file_paths' is the dictionary and 'load_numeric_data' is defined\")\n",
    "        example_fn_usage = next(iter(numeric_file_paths))\n",
    "        print(f\"   filename = '{example_fn_usage}'\")\n",
    "        print(f\"   if filename in numeric_file_paths:\")\n",
    "        print(f\"       # data_array = load_numeric_data(filename, numeric_file_paths)\")\n",
    "        print(f\"       file_path = numeric_file_paths[filename]\")\n",
    "        print(f\"       print(f'Path to feature file: {{file_path}}')\")\n",
    "        print(f\"       # data = np.loadtxt(file_path)\")\n",
    "        print(\"   ```\")\n",
    "        print(\"\\n   Loading one example feature file for demonstration:\")\n",
    "        example_data = load_numeric_data(example_fn_usage, numeric_file_paths)\n",
    "        if example_data is not None:\n",
    "             print(f\"   > Successfully loaded '{example_fn_usage}' with shape {example_data.shape}\")\n",
    "        else:\n",
    "             print(f\"   > Failed to load example file '{example_fn_usage}'. Check logs for errors.\")\n",
    "    else:\n",
    "        print(\" - Mapping filenames to feature files FAILED or resulted in zero mapped files.\")\n",
    "        print(\" - Potential reasons:\")\n",
    "        print(f\"   - CSV '{EQUATIONS_CSV_PATH.name}' missing filenames or '{FILENAME_COLUMN}' column.\")\n",
    "        print(f\"   - Feature directory '{FEATURES_EXTRACTED_PATH.name}' not found or verified.\")\n",
    "        print(f\"   - Feature files not found in the expected directory structure within: {FEATURES_EXTRACTED_PATH.resolve()}\")\n",
    "        print(\"   - Check script logs above for specific warnings or errors.\")\n",
    "        print(\" - The 'numeric_file_paths' dictionary is empty.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    logging.info(\"--- Feynman Dataset Preprocessing Script (Using Local Data) FINISHED ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Task 1.2 Dataset preprocessing\n",
    "Dataset:\n",
    "\n",
    "https://alabama.box.com/s/xhgr2onrn503jyse2fs5vxtapg0oifcs \n",
    "\n",
    "Dataset:\n",
    "Download the dataset (split across 10 files) and preprocess and tokenize the target data and document your rationale for choice of tokenization. Data file is formatted with rows like \n",
    "“event type : Feynman diagram : amplitude : squared amplitude”\n",
    "Here the amplitudes are the input sequences and squared amplitudes are the target sequences. Note that indices like _123456 grow over the course of the dataset and should be normalized for each amplitude and squared amplitude. Use an 80-10-10 split of train-val-test across all files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 12:02:31,969 - INFO - Starting QED amplitude expression preprocessing script...\n",
      "2025-04-22 12:02:31,970 - INFO - Random seed set to: 42\n",
      "2025-04-22 12:02:31,970 - INFO - Loading data from directory: /home/nikitas/Desktop/Miche/GOOGLE/qed_data_raw\n",
      "2025-04-22 12:02:31,971 - INFO - Found 10 '.txt' files in qed_data_raw. Processing...\n",
      "2025-04-22 12:02:31,971 - INFO -   Processing file: QED-2-to-2-diag-TreeLevel-0.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 12:02:32,048 - INFO -     Finished QED-2-to-2-diag-TreeLevel-0.txt. Added 1728 examples. Skipped 0 lines.\n",
      "2025-04-22 12:02:32,049 - INFO -   Processing file: QED-2-to-2-diag-TreeLevel-1.txt...\n",
      "2025-04-22 12:02:32,104 - INFO -     Finished QED-2-to-2-diag-TreeLevel-1.txt. Added 1664 examples. Skipped 0 lines.\n",
      "2025-04-22 12:02:32,105 - INFO -   Processing file: QED-2-to-2-diag-TreeLevel-2.txt...\n",
      "2025-04-22 12:02:32,161 - INFO -     Finished QED-2-to-2-diag-TreeLevel-2.txt. Added 1600 examples. Skipped 0 lines.\n",
      "2025-04-22 12:02:32,161 - INFO -   Processing file: QED-2-to-2-diag-TreeLevel-3.txt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "          Preprocessing and Tokenization Rationale (QED Amplitudes)\n",
      "======================================================================\n",
      "\n",
      "1.  **Index Normalization (`_123456` -> `_0`, `_1`, ...):**\n",
      "    * **Problem:** The raw data contains indices (e.g., `p_123`, `gamma_45`) where the numeric part can grow very large across the dataset. Treating each unique numbered index (like `p_123` vs `p_124`) as a distinct token would lead to an enormous, sparse vocabulary.\n",
      "    * **Solution:** We normalize these indices *within each individual expression* (amplitude and squared amplitude separately). The first unique numeric subscript encountered (like `_123`) becomes `_0`, the second unique one (`_45`) becomes `_1`, and so on. If the *same* original index appears multiple times in one expression (e.g., `p_123 * p_123`), it gets the *same* normalized index (`p_0 * p_0`).\n",
      "    * **Benefit:** This dramatically reduces vocabulary size and helps the model focus on the *structure* and *relationships* between indexed terms rather than memorizing absolute index values. It treats `p_0`, `p_1` etc., as generic indexed placeholders.\n",
      "\n",
      "2.  **Custom Regex Tokenization (`TOKEN_PATTERN`):**\n",
      "    * **Goal:** To break down the normalized mathematical expressions into meaningful units suitable for sequence modeling.\n",
      "    * **Method:** A regular expression `([A-Za-z_]\\w*|\\d+|\\*\\*|\\^|[+\\-*/=()_,:])` is used to explicitly define tokens as:\n",
      "        * Identifiers: `p_0`, `m_e`, `gamma_1`, `exp`, etc. (alphanumeric starting with letter/_).\n",
      "        * Numbers: `2`, `4`, `1`, etc. (integers).\n",
      "        * Operators/Special Characters: `+`, `-`, `*`, `/`, `=`, `**` (power), `^` (power), `(`, `)`, `,`, `:`.\n",
      "    * **Why Regex Here?**\n",
      "        * **Preserves Structure:** Unlike statistical methods like BPE (Byte Pair Encoding) used for natural language or the previous Feynman equations, this regex approach prevents merging parts of distinct mathematical significance. For example, it ensures `p`, `_`, `0` remain separate if desired (though here `p_0` is one identifier token), or that operators like `**` are treated as single units. It guarantees that fundamental symbols like `+`, `*`, `(` are always distinct tokens.\n",
      "        * **Interpretability:** The resulting tokens directly correspond to the symbolic components of the expressions, making the input/output sequences easier to understand.\n",
      "        * **Domain Specificity:** The structure of these QED expressions is relatively regular compared to free-form text. A targeted regex can capture the known relevant components effectively without needing to learn merges from data (like BPE).\n",
      "    * **Alternative Considered (BPE):** While BPE could be trained, it might learn merges that are not mathematically ideal (e.g., merging an operator with part of a variable name) and could result in a less interpretable token set for this specific symbolic domain. Given the structured nature of the input, regex provides more control.\n",
      "\n",
      "3.  **Input/Target:**\n",
      "    * The 'amplitude' expression (normalized and tokenized) serves as the input sequence.\n",
      "    * The 'squared amplitude' expression (normalized and tokenized) serves as the target sequence.\n",
      "\n",
      "4.  **Splitting (80/10/10 Train/Val/Test):**\n",
      "    * Standard practice for robust model development: training on the largest portion, tuning hyperparameters on the validation set, and final unbiased evaluation on the held-out test set. Shuffling *before* splitting ensures randomness across data from all source files.\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 12:02:32,214 - INFO -     Finished QED-2-to-2-diag-TreeLevel-3.txt. Added 1536 examples. Skipped 0 lines.\n",
      "2025-04-22 12:02:32,215 - INFO -   Processing file: QED-2-to-2-diag-TreeLevel-4.txt...\n",
      "2025-04-22 12:02:32,269 - INFO -     Finished QED-2-to-2-diag-TreeLevel-4.txt. Added 1472 examples. Skipped 0 lines.\n",
      "2025-04-22 12:02:32,269 - INFO -   Processing file: QED-2-to-2-diag-TreeLevel-5.txt...\n",
      "2025-04-22 12:02:32,319 - INFO -     Finished QED-2-to-2-diag-TreeLevel-5.txt. Added 1408 examples. Skipped 0 lines.\n",
      "2025-04-22 12:02:32,319 - INFO -   Processing file: QED-2-to-2-diag-TreeLevel-6.txt...\n",
      "2025-04-22 12:02:32,370 - INFO -     Finished QED-2-to-2-diag-TreeLevel-6.txt. Added 1344 examples. Skipped 0 lines.\n",
      "2025-04-22 12:02:32,371 - INFO -   Processing file: QED-2-to-2-diag-TreeLevel-7.txt...\n",
      "2025-04-22 12:02:32,419 - INFO -     Finished QED-2-to-2-diag-TreeLevel-7.txt. Added 1280 examples. Skipped 0 lines.\n",
      "2025-04-22 12:02:32,420 - INFO -   Processing file: QED-2-to-2-diag-TreeLevel-8.txt...\n",
      "2025-04-22 12:02:32,504 - INFO -     Finished QED-2-to-2-diag-TreeLevel-8.txt. Added 1216 examples. Skipped 0 lines.\n",
      "2025-04-22 12:02:32,505 - INFO -   Processing file: QED-2-to-2-diag-TreeLevel-9.txt...\n",
      "2025-04-22 12:02:32,641 - INFO -     Finished QED-2-to-2-diag-TreeLevel-9.txt. Added 2304 examples. Skipped 0 lines.\n",
      "2025-04-22 12:02:32,641 - INFO - Finished processing 10 files.\n",
      "2025-04-22 12:02:32,641 - INFO - Successfully loaded and preprocessed 15552 examples.\n",
      "2025-04-22 12:02:32,642 - INFO - Splitting and saving dataset with prefix 'qed_amplitudes' to /home/nikitas/Desktop/Miche/GOOGLE/qed_data_processed...\n",
      "2025-04-22 12:02:32,642 - INFO - Total examples loaded: 15552\n",
      "2025-04-22 12:02:32,642 - INFO - Shuffling dataset with random seed 42...\n",
      "2025-04-22 12:02:32,646 - INFO - Calculated split sizes: Train=12441, Validation=1555, Test=1556\n",
      "2025-04-22 12:02:32,646 - INFO - Saving splits to directory: /home/nikitas/Desktop/Miche/GOOGLE/qed_data_processed\n",
      "2025-04-22 12:02:32,646 - INFO -   Saving train set (12441 examples) to qed_data_processed/qed_amplitudes_train.jsonl...\n",
      "2025-04-22 12:02:32,783 - INFO -     Successfully saved 12441 lines.\n",
      "2025-04-22 12:02:32,784 - INFO -   Saving val set (1555 examples) to qed_data_processed/qed_amplitudes_val.jsonl...\n",
      "2025-04-22 12:02:32,802 - INFO -     Successfully saved 1555 lines.\n",
      "2025-04-22 12:02:32,802 - INFO -   Saving test set (1556 examples) to qed_data_processed/qed_amplitudes_test.jsonl...\n",
      "2025-04-22 12:02:32,820 - INFO -     Successfully saved 1556 lines.\n",
      "2025-04-22 12:02:32,820 - INFO - Script finished successfully.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "QED Amplitude Expression Preprocessing Pipeline (Task 1.2)\n",
    "\n",
    "Handles the preprocessing of QED amplitude data:\n",
    "1.  Assumes data files (expected format: event:diagram:amplitude:sq_amplitude)\n",
    "    have been downloaded manually into a specified directory.\n",
    "2.  Loads data from all '.txt' files within that directory.\n",
    "3.  Parses lines, extracting amplitude (input) and squared amplitude (target).\n",
    "4.  Applies index normalization (e.g., _123 -> _0) locally within each expression.\n",
    "5.  Tokenizes normalized expressions using a custom regex tokenizer.\n",
    "6.  Structures data into input/target token pairs.\n",
    "7.  Shuffles the combined dataset.\n",
    "8.  Splits into 80/10/10 train/validation/test sets.\n",
    "9.  Saves splits to JSON Lines (.jsonl) files.\n",
    "10. Documents the rationale for tokenization choices.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple # Added Tuple for type hints\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# **IMPORTANT**: Manual Download Required!\n",
    "# Download the dataset (10 files) from:\n",
    "# https://alabama.box.com/s/xhgr2onrn503jyse2fs5vxtapg0oifcs\n",
    "# And place ALL the '.txt' files into the directory specified below.\n",
    "# Example: Create a directory 'qed_data_raw' next to this script\n",
    "#          and put the 10 downloaded txt files inside it.\n",
    "INPUT_DATA_DIR = Path(\"./qed_data_raw\") # <--- UPDATE if you place data elsewhere\n",
    "\n",
    "# Output directory for processed JSONL files\n",
    "OUTPUT_DIR = Path(\"./qed_data_processed\")\n",
    "OUTPUT_PREFIX = 'qed_amplitudes' # Files will be qed_amplitudes_train.jsonl etc.\n",
    "\n",
    "# Dataset split ratios\n",
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC = 0.1\n",
    "TEST_FRAC = 1.0 - TRAIN_FRAC - VAL_FRAC # Calculated automatically\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Logging Setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# --- Tokenization and Normalization ---\n",
    "\n",
    "# Regex pattern to capture tokens (same as before, seems suitable):\n",
    "# Group 1: Identifiers ([A-Za-z_]\\w*)\n",
    "# Group 2: Numbers (\\d+)\n",
    "# Group 3: Operators/Special Chars (\\*\\*|\\^|[+\\-*/=()_,:])\n",
    "TOKEN_PATTERN = re.compile(r\"([A-Za-z_]\\w*|\\d+|\\*\\*|\\^|[+\\-*/=()_,:])\")\n",
    "\n",
    "# Regex pattern to find numeric subscripts (e.g., \"_123456\")\n",
    "INDEX_PATTERN = re.compile(r'_\\d+')\n",
    "\n",
    "def tokenize(expression_string: str) -> List[str]:\n",
    "    \"\"\"Tokenizes a mathematical expression string using TOKEN_PATTERN.\"\"\"\n",
    "    if not isinstance(expression_string, str): # Added type check\n",
    "        return []\n",
    "    return TOKEN_PATTERN.findall(expression_string)\n",
    "\n",
    "def normalize_indices(expression_string: str) -> str:\n",
    "    \"\"\"Normalizes numeric subscripts (_123 -> _0) within a single expression string.\"\"\"\n",
    "    if not isinstance(expression_string, str): # Added type check\n",
    "        return \"\"\n",
    "\n",
    "    index_mapping: Dict[str, str] = {}\n",
    "    counter = [0] # Mutable counter for closure\n",
    "\n",
    "    def _replace_match(match):\n",
    "        original_index = match.group(0)\n",
    "        if original_index not in index_mapping:\n",
    "            normalized_index = f\"_{counter[0]}\"\n",
    "            index_mapping[original_index] = normalized_index\n",
    "            counter[0] += 1\n",
    "        return index_mapping[original_index]\n",
    "\n",
    "    return INDEX_PATTERN.sub(_replace_match, expression_string)\n",
    "\n",
    "# --- Data Loading and Processing ---\n",
    "\n",
    "def load_and_preprocess_data(data_directory: Path) -> List[Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Loads data from all .txt files in the specified directory, preprocesses,\n",
    "    and tokenizes amplitude and squared amplitude pairs.\n",
    "\n",
    "    Args:\n",
    "        data_directory (Path): The directory containing the downloaded .txt files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, List[str]]]: A list of dictionaries, each with keys\n",
    "                                     'input_tokens' and 'target_tokens'.\n",
    "    \"\"\"\n",
    "    dataset: List[Dict[str, List[str]]] = []\n",
    "    skipped_lines = 0\n",
    "    processed_files = 0\n",
    "\n",
    "    if not data_directory.is_dir():\n",
    "        logging.error(f\"Input data directory not found: {data_directory}\")\n",
    "        logging.error(\"Please ensure you have downloaded the data and placed the .txt files there.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Use pathlib's glob for cleaner path handling\n",
    "    file_paths = sorted(list(data_directory.glob(\"*.txt\")))\n",
    "\n",
    "    if not file_paths:\n",
    "        logging.error(f\"No '.txt' files found in directory: {data_directory}\")\n",
    "        logging.error(\"Please check the directory content and INPUT_DATA_DIR setting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    logging.info(f\"Found {len(file_paths)} '.txt' files in {data_directory}. Processing...\")\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        processed_files += 1\n",
    "        logging.info(f\"  Processing file: {file_path.name}...\")\n",
    "        line_count_in_file = 0\n",
    "        skipped_in_file = 0\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                for line_num, line in enumerate(infile, 1):\n",
    "                    line = line.strip()\n",
    "                    if not line: continue # Skip empty lines\n",
    "\n",
    "                    parts = line.split(' : ', 3) # Split only 3 times max\n",
    "                    # Expecting 4 parts: event:diagram:amplitude:sq_amplitude\n",
    "                    if len(parts) != 4:\n",
    "                        # Reduce logging noise, maybe log first few warnings per file?\n",
    "                        # if skipped_in_file < 5:\n",
    "                        #     logging.warning(f\"Skipping malformed line {line_num} in {file_path.name}: Expected 4 parts separated by ' : ', found {len(parts)}. Content: '{line[:100]}...'\")\n",
    "                        skipped_lines += 1\n",
    "                        skipped_in_file += 1\n",
    "                        continue\n",
    "\n",
    "                    # Input is amplitude, Target is squared amplitude\n",
    "                    amplitude_expr = parts[2]\n",
    "                    sq_amplitude_expr = parts[3]\n",
    "\n",
    "                    # 1. Normalize indices (Applied independently to each expression)\n",
    "                    normalized_amplitude = normalize_indices(amplitude_expr)\n",
    "                    normalized_sq_amplitude = normalize_indices(sq_amplitude_expr)\n",
    "\n",
    "                    # 2. Tokenize\n",
    "                    amplitude_tokens = tokenize(normalized_amplitude)\n",
    "                    sq_amplitude_tokens = tokenize(normalized_sq_amplitude)\n",
    "\n",
    "                    # Append structured data if tokenization successful\n",
    "                    if amplitude_tokens and sq_amplitude_tokens:\n",
    "                        dataset.append({\n",
    "                            'input_tokens': amplitude_tokens,\n",
    "                            'target_tokens': sq_amplitude_tokens\n",
    "                        })\n",
    "                        line_count_in_file += 1\n",
    "                    else:\n",
    "                        # if skipped_in_file < 5: # Example condition to reduce noise\n",
    "                        #    logging.warning(f\"Skipping line {line_num} in {file_path.name} due to empty tokens after processing.\")\n",
    "                        skipped_lines += 1\n",
    "                        skipped_in_file += 1\n",
    "\n",
    "            logging.info(f\"    Finished {file_path.name}. Added {line_count_in_file} examples. Skipped {skipped_in_file} lines.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"File not found during processing: {file_path}. This shouldn't happen if glob worked.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            logging.exception(f\"Error processing file {file_path}: {e}\") # Log full traceback\n",
    "            continue # Skip faulty file\n",
    "\n",
    "    logging.info(f\"Finished processing {processed_files} files.\")\n",
    "    if skipped_lines > 0:\n",
    "        logging.warning(f\"Total skipped lines across all files: {skipped_lines}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def split_and_save_dataset(\n",
    "    data: List[Dict[str, List[str]]],\n",
    "    train_frac: float,\n",
    "    val_frac: float,\n",
    "    output_dir: Path,\n",
    "    output_prefix: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Shuffles, splits (train/val/test), and saves the dataset to JSON Lines files.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        logging.warning(\"No data provided to split and save.\")\n",
    "        return\n",
    "\n",
    "    n_total = len(data)\n",
    "    logging.info(f\"Total examples loaded: {n_total}\")\n",
    "\n",
    "    if n_total == 0:\n",
    "        logging.error(\"No valid examples were loaded from the data files. Cannot split.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Ensure fractions are valid\n",
    "    if not (0 < train_frac < 1 and 0 < val_frac < 1 and (train_frac + val_frac) < 1):\n",
    "        logging.error(f\"Invalid split fractions: train={train_frac}, val={val_frac}. Must be > 0 and sum < 1.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Shuffle the data\n",
    "    logging.info(f\"Shuffling dataset with random seed {RANDOM_SEED}...\")\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Calculate split indices\n",
    "    n_train = int(train_frac * n_total)\n",
    "    n_val = int(val_frac * n_total)\n",
    "    # Test gets the remainder\n",
    "    n_test = n_total - n_train - n_val\n",
    "\n",
    "    # Check if splits are sensible\n",
    "    if n_train == 0 or n_val == 0 or n_test == 0:\n",
    "         logging.warning(f\"Dataset size ({n_total}) is very small, resulting in potentially zero examples in splits: Train={n_train}, Val={n_val}, Test={n_test}. Check input data.\")\n",
    "\n",
    "    # Perform the splits\n",
    "    splits: Dict[str, List[Dict]] = {\n",
    "        'train': data[:n_train],\n",
    "        'val': data[n_train : n_train + n_val],\n",
    "        'test': data[n_train + n_val :]\n",
    "    }\n",
    "\n",
    "    logging.info(f\"Calculated split sizes: Train={len(splits['train'])}, Validation={len(splits['val'])}, Test={len(splits['test'])}\")\n",
    "\n",
    "    # Save each split\n",
    "    try:\n",
    "        output_dir.mkdir(parents=True, exist_ok=True) # Ensure output directory exists\n",
    "        logging.info(f\"Saving splits to directory: {output_dir.resolve()}\")\n",
    "    except OSError as e:\n",
    "        logging.error(f\"Could not create output directory {output_dir}: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    for split_name, subset in splits.items():\n",
    "        output_filename = output_dir / f\"{output_prefix}_{split_name}.jsonl\"\n",
    "        logging.info(f\"  Saving {split_name} set ({len(subset)} examples) to {output_filename}...\")\n",
    "        try:\n",
    "            with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "                count = 0\n",
    "                for entry in subset:\n",
    "                    # Write each dictionary as a JSON string on its own line\n",
    "                    outfile.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "                    count +=1\n",
    "                logging.info(f\"    Successfully saved {count} lines.\")\n",
    "        except IOError as e:\n",
    "            logging.error(f\"Failed to write {split_name} set to {output_filename}: {e}\")\n",
    "        except Exception as e:\n",
    "             logging.exception(f\"An unexpected error occurred while writing {split_name} set: {e}\")\n",
    "\n",
    "\n",
    "def print_rationale() -> None:\n",
    "    \"\"\"Prints the documented rationale for the chosen preprocessing steps.\"\"\"\n",
    "    rationale = \"\"\"\n",
    "======================================================================\n",
    "          Preprocessing and Tokenization Rationale (QED Amplitudes)\n",
    "======================================================================\n",
    "\n",
    "1.  **Index Normalization (`_123456` -> `_0`, `_1`, ...):**\n",
    "    * **Problem:** The raw data contains indices (e.g., `p_123`, `gamma_45`) where the numeric part can grow very large across the dataset. Treating each unique numbered index (like `p_123` vs `p_124`) as a distinct token would lead to an enormous, sparse vocabulary.\n",
    "    * **Solution:** We normalize these indices *within each individual expression* (amplitude and squared amplitude separately). The first unique numeric subscript encountered (like `_123`) becomes `_0`, the second unique one (`_45`) becomes `_1`, and so on. If the *same* original index appears multiple times in one expression (e.g., `p_123 * p_123`), it gets the *same* normalized index (`p_0 * p_0`).\n",
    "    * **Benefit:** This dramatically reduces vocabulary size and helps the model focus on the *structure* and *relationships* between indexed terms rather than memorizing absolute index values. It treats `p_0`, `p_1` etc., as generic indexed placeholders.\n",
    "\n",
    "2.  **Custom Regex Tokenization (`TOKEN_PATTERN`):**\n",
    "    * **Goal:** To break down the normalized mathematical expressions into meaningful units suitable for sequence modeling.\n",
    "    * **Method:** A regular expression `([A-Za-z_]\\w*|\\d+|\\*\\*|\\^|[+\\-*/=()_,:])` is used to explicitly define tokens as:\n",
    "        * Identifiers: `p_0`, `m_e`, `gamma_1`, `exp`, etc. (alphanumeric starting with letter/_).\n",
    "        * Numbers: `2`, `4`, `1`, etc. (integers).\n",
    "        * Operators/Special Characters: `+`, `-`, `*`, `/`, `=`, `**` (power), `^` (power), `(`, `)`, `,`, `:`.\n",
    "    * **Why Regex Here?**\n",
    "        * **Preserves Structure:** Unlike statistical methods like BPE (Byte Pair Encoding) used for natural language or the previous Feynman equations, this regex approach prevents merging parts of distinct mathematical significance. For example, it ensures `p`, `_`, `0` remain separate if desired (though here `p_0` is one identifier token), or that operators like `**` are treated as single units. It guarantees that fundamental symbols like `+`, `*`, `(` are always distinct tokens.\n",
    "        * **Interpretability:** The resulting tokens directly correspond to the symbolic components of the expressions, making the input/output sequences easier to understand.\n",
    "        * **Domain Specificity:** The structure of these QED expressions is relatively regular compared to free-form text. A targeted regex can capture the known relevant components effectively without needing to learn merges from data (like BPE).\n",
    "    * **Alternative Considered (BPE):** While BPE could be trained, it might learn merges that are not mathematically ideal (e.g., merging an operator with part of a variable name) and could result in a less interpretable token set for this specific symbolic domain. Given the structured nature of the input, regex provides more control.\n",
    "\n",
    "3.  **Input/Target:**\n",
    "    * The 'amplitude' expression (normalized and tokenized) serves as the input sequence.\n",
    "    * The 'squared amplitude' expression (normalized and tokenized) serves as the target sequence.\n",
    "\n",
    "4.  **Splitting (80/10/10 Train/Val/Test):**\n",
    "    * Standard practice for robust model development: training on the largest portion, tuning hyperparameters on the validation set, and final unbiased evaluation on the held-out test set. Shuffling *before* splitting ensures randomness across data from all source files.\n",
    "\n",
    "======================================================================\n",
    "\"\"\"\n",
    "    print(rationale)\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main function to orchestrate the preprocessing and splitting.\"\"\"\n",
    "    logging.info(\"Starting QED amplitude expression preprocessing script...\")\n",
    "\n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(RANDOM_SEED)\n",
    "    logging.info(f\"Random seed set to: {RANDOM_SEED}\")\n",
    "\n",
    "    # Print rationale upfront\n",
    "    print_rationale()\n",
    "\n",
    "    # 1. Load and preprocess data\n",
    "    logging.info(f\"Loading data from directory: {INPUT_DATA_DIR.resolve()}\")\n",
    "    preprocessed_data = load_and_preprocess_data(INPUT_DATA_DIR)\n",
    "\n",
    "    if not preprocessed_data:\n",
    "        logging.error(\"No data loaded after processing files. Exiting.\")\n",
    "        # load_and_preprocess_data already exits if dir/files not found\n",
    "        sys.exit(1)\n",
    "\n",
    "    logging.info(f\"Successfully loaded and preprocessed {len(preprocessed_data)} examples.\")\n",
    "\n",
    "    # 2. Split and save the dataset\n",
    "    logging.info(f\"Splitting and saving dataset with prefix '{OUTPUT_PREFIX}' to {OUTPUT_DIR.resolve()}...\")\n",
    "    split_and_save_dataset(\n",
    "        data=preprocessed_data,\n",
    "        train_frac=TRAIN_FRAC,\n",
    "        val_frac=VAL_FRAC,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        output_prefix=OUTPUT_PREFIX\n",
    "    )\n",
    "\n",
    "    logging.info(\"Script finished successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Task 2: Train/Evaluate Transformer model\n",
    "Train a generic next-token-prediction Transformer model to map the input data to the tokenized output sequences. Evaluate performance on the test set using sequence accuracy as a metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 13:09:28,773 - WARNING - Running in Jupyter/IPython! Using default arguments.\n",
      "2025-04-22 13:09:28,774 - INFO - Starting script with effective arguments: Namespace(data_dir=PosixPath('qed_data_processed'), output_dir=PosixPath('qed_model_output_v2'), tokenizer_file_name='qed_custom_tokenizer.json', encoder_model_id='bert-base-uncased', decoder_model_id='bert-base-uncased', max_seq_length=128, vocab_size=10000, num_train_epochs=5, per_device_train_batch_size=16, per_device_eval_batch_size=16, learning_rate=5e-05, weight_decay=0.01, warmup_steps=500, logging_steps=100, seed=42, tokenizer_file=PosixPath('qed_model_output_v2/qed_custom_tokenizer.json'))\n",
      "2025-04-22 13:09:28,775 - INFO - Random seed set to 42\n",
      "2025-04-22 13:09:28,776 - INFO - Found train data file: qed_data_processed/qed_amplitudes_train.jsonl\n",
      "2025-04-22 13:09:28,776 - INFO - Found validation data file: qed_data_processed/qed_amplitudes_val.jsonl\n",
      "2025-04-22 13:09:28,776 - INFO - Found test data file: qed_data_processed/qed_amplitudes_test.jsonl\n",
      "2025-04-22 13:09:28,777 - INFO - Tokenizer file already exists at qed_model_output_v2/qed_custom_tokenizer.json, skipping training.\n",
      "2025-04-22 13:09:28,778 - INFO - Successfully loaded custom tokenizer from qed_model_output_v2/qed_custom_tokenizer.json\n",
      "2025-04-22 13:09:28,778 - INFO - Tokenizer vocabulary size: 266\n",
      "2025-04-22 13:09:28,778 - INFO - Using PAD token: '<pad>' (ID: 1)\n",
      "2025-04-22 13:09:28,779 - INFO - Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 13:09:29,031 - INFO - Datasets loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_tokens', 'target_tokens'],\n",
      "        num_rows: 12441\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_tokens', 'target_tokens'],\n",
      "        num_rows: 1555\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_tokens', 'target_tokens'],\n",
      "        num_rows: 1556\n",
      "    })\n",
      "})\n",
      "2025-04-22 13:09:29,032 - INFO - Tokenizing datasets...\n",
      "Running tokenizer on dataset: 100%|██████████| 12441/12441 [00:01<00:00, 9203.72 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 1555/1555 [00:00<00:00, 10288.61 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 1556/1556 [00:00<00:00, 6132.66 examples/s]\n",
      "2025-04-22 13:09:30,799 - INFO - Tokenized datasets structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 12441\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1555\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1556\n",
      "    })\n",
      "})\n",
      "2025-04-22 13:09:30,799 - INFO - Initializing Encoder-Decoder model (bert-base-uncased -> bert-base-uncased)\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-04-22 13:09:31,261 - INFO - Resizing ENCODER token embeddings to 266\n",
      "2025-04-22 13:09:31,263 - INFO - Resizing DECODER token embeddings to 266\n",
      "2025-04-22 13:09:31,265 - INFO - Model configuration complete.\n",
      "2025-04-22 13:09:31,265 - INFO - Data collator initialized.\n",
      "2025-04-22 13:09:31,265 - INFO - Defining Training Arguments (Minimal for compatibility check)...\n",
      "/home/nikitas/anaconda3/envs/feynman_seq2seq_env/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647352509/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2025-04-22 13:09:31,283 - INFO - Training arguments defined (Minimal).\n",
      "/tmp/ipykernel_165099/515218467.py:290: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-04-22 13:09:31,290 - INFO - Seq2SeqTrainer initialized.\n",
      "2025-04-22 13:09:31,290 - INFO - Starting model training...\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "/home/nikitas/anaconda3/envs/feynman_seq2seq_env/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='3890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 750/3890 06:22 < 26:44, 1.96 it/s, Epoch 0.96/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.589700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikitas/anaconda3/envs/feynman_seq2seq_env/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x76322503cd00>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nikitas/anaconda3/envs/feynman_seq2seq_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "WORKS ON CPU\n",
    "\n",
    "Task 2: Transformer Training for QED Amplitude Mapping (Jupyter Compatible)\n",
    "\n",
    "Trains an Encoder-Decoder Transformer model on the preprocessed QED amplitude dataset.\n",
    "Uses a custom tokenizer trained on the specific tokens generated during preprocessing.\n",
    "Modified to run within a Jupyter Notebook by detecting the environment and using\n",
    "default arguments instead of parsing command-line args when needed.\n",
    "Uses MINIMAL TrainingArguments parameters for compatibility testing.\n",
    "\n",
    "Pipeline:\n",
    "1.  Define paths and configurations (using defaults when in Jupyter).\n",
    "2.  Train a custom WordLevel tokenizer on the training data tokens if not already present.\n",
    "3.  Load the custom tokenizer.\n",
    "4.  Load the train/val/test datasets (JSONL format from Task 1.2).\n",
    "5.  Define a tokenization function using the custom tokenizer.\n",
    "6.  Tokenize the datasets using `datasets.map`.\n",
    "7.  Initialize an Encoder-Decoder model (e.g., bert-base-uncased) and resize\n",
    "    its token embeddings INDIVIDUALLY to match the custom tokenizer's vocabulary.\n",
    "8.  Configure the model for sequence-to-sequence tasks.\n",
    "9.  Set up Data Collator, MINIMAL Training Arguments, and Seq2SeqTrainer.\n",
    "10. Define sequence accuracy metric function.\n",
    "11. Train the model.\n",
    "12. Evaluate the model on the test set.\n",
    "\"\"\"\n",
    "\n",
    "# --- Core Imports ---\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "import argparse\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# --- Hugging Face Imports ---\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, decoders, processors\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoConfig,\n",
    "    EncoderDecoderModel,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "# import accelerate\n",
    "\n",
    "# Logging Setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Default Configuration (Used when running in Jupyter) ---\n",
    "DEFAULT_ARGS = {\n",
    "    \"data_dir\": Path(\"./qed_data_processed\"),\n",
    "    \"output_dir\": Path(\"./qed_model_output_v2\"),\n",
    "    \"tokenizer_file_name\": \"qed_custom_tokenizer.json\",\n",
    "    \"encoder_model_id\": \"bert-base-uncased\",\n",
    "    \"decoder_model_id\": \"bert-base-uncased\",\n",
    "    \"max_seq_length\": 128,\n",
    "    \"vocab_size\": 10000,\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"logging_steps\": 100,\n",
    "    # Removed eval/save strategy args - use minimal set\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "\n",
    "# --- Custom Tokenizer Training ---\n",
    "# (Function remains the same)\n",
    "def train_custom_tokenizer(\n",
    "    data_files: Dict[str, str],\n",
    "    output_path: Path,\n",
    "    vocab_size: int,\n",
    "    min_frequency: int = 2\n",
    "    ) -> None:\n",
    "    \"\"\" Trains a WordLevel tokenizer. \"\"\"\n",
    "    if output_path.exists():\n",
    "        logging.info(f\"Tokenizer file already exists at {output_path}, skipping training.\")\n",
    "        return\n",
    "    logging.info(\"Training custom WordLevel tokenizer...\")\n",
    "    try:\n",
    "        train_file_path = data_files.get('train')\n",
    "        if not train_file_path or not Path(train_file_path).is_file():\n",
    "            logging.error(f\"Training data file path missing or invalid: {train_file_path}\")\n",
    "            sys.exit(1)\n",
    "        raw_train_dataset = load_dataset(\"json\", data_files={'train': train_file_path}, split=\"train\")\n",
    "        logging.info(f\"Loaded {len(raw_train_dataset)} examples for tokenizer training.\")\n",
    "    except Exception as e: logging.error(f\"Failed to load training data: {e}\"); sys.exit(1)\n",
    "    special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"]\n",
    "    def get_training_corpus():\n",
    "        sequences_yielded = 0\n",
    "        for example in raw_train_dataset:\n",
    "            in_tokens = example.get(\"input_tokens\"); tgt_tokens = example.get(\"target_tokens\")\n",
    "            if in_tokens and isinstance(in_tokens, list): yield in_tokens; sequences_yielded += 1\n",
    "            if tgt_tokens and isinstance(tgt_tokens, list): yield tgt_tokens; sequences_yielded += 1\n",
    "        if sequences_yielded == 0: logging.warning(\"Tokenizer training corpus yielded zero sequences.\")\n",
    "        else: logging.info(f\"Tokenizer training corpus yielded {sequences_yielded} sequences.\")\n",
    "    try:\n",
    "        custom_tokenizer = Tokenizer(models.WordLevel(unk_token=\"<unk>\"))\n",
    "        custom_tokenizer.pre_tokenizer = None\n",
    "        trainer = trainers.WordLevelTrainer(vocab_size=vocab_size, min_frequency=min_frequency, special_tokens=special_tokens)\n",
    "        logging.info(f\"Starting tokenizer training (vocab_size={vocab_size}, min_freq={min_frequency})...\")\n",
    "        custom_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "        logging.info(\"Tokenizer training finished.\")\n",
    "        current_vocab = custom_tokenizer.get_vocab()\n",
    "        tokens_to_add = [token for token in special_tokens if token not in current_vocab]\n",
    "        if tokens_to_add:\n",
    "            num_added = custom_tokenizer.add_special_tokens(tokens_to_add)\n",
    "            logging.warning(f\"Added {num_added} special tokens ({tokens_to_add}) missing after training.\")\n",
    "            if not all(token in custom_tokenizer.get_vocab() for token in special_tokens): raise ValueError(\"Failed to add all required special tokens!\")\n",
    "        bos_token_id = custom_tokenizer.token_to_id(\"<s>\"); eos_token_id = custom_tokenizer.token_to_id(\"</s>\")\n",
    "        if bos_token_id is None or eos_token_id is None: raise ValueError(\"BOS/EOS tokens missing from trained tokenizer vocab.\")\n",
    "        custom_tokenizer.post_processor = processors.TemplateProcessing(single=\"<s> $A </s>\", special_tokens=[(\"<s>\", bos_token_id), (\"</s>\", eos_token_id)])\n",
    "        logging.info(\"Set tokenizer post-processor for BOS/EOS.\")\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        custom_tokenizer.save(str(output_path))\n",
    "        logging.info(f\"Custom tokenizer trained ({custom_tokenizer.get_vocab_size()} vocab size) and saved to {output_path}\")\n",
    "    except Exception as e: logging.exception(f\"Error during tokenizer training or saving: {e}\"); sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Data Loading and Tokenization ---\n",
    "# (Function remains the same)\n",
    "def tokenize_function(examples, tokenizer, max_len):\n",
    "    \"\"\" Tokenizes input and target token lists using the custom tokenizer. \"\"\"\n",
    "    input_strings = [\" \".join(map(str, tokens)) if isinstance(tokens, list) else \"\" for tokens in examples['input_tokens']]\n",
    "    target_strings = [\" \".join(map(str, tokens)) if isinstance(tokens, list) else \"\" for tokens in examples['target_tokens']]\n",
    "    model_inputs = tokenizer(input_strings, max_length=max_len, padding=False, truncation=True)\n",
    "    labels = tokenizer(text_target=target_strings, max_length=max_len, padding=False, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# --- Metrics ---\n",
    "# (Functions remain the same)\n",
    "def compute_sequence_accuracy(predictions, labels, pad_token_id):\n",
    "    \"\"\" Calculates exact sequence match accuracy, ignoring padding. \"\"\"\n",
    "    pred_ids = np.argmax(predictions, axis=-1); labels = np.asarray(labels)\n",
    "    if pred_ids.shape != labels.shape:\n",
    "        logging.warning(f\"Shape mismatch in metrics: preds {pred_ids.shape}, labels {labels.shape}. Truncating.\")\n",
    "        min_len = min(pred_ids.shape[1], labels.shape[1]); pred_ids = pred_ids[:, :min_len]; labels = labels[:, :min_len]\n",
    "    non_padding_mask = (labels != pad_token_id); correct_tokens = (pred_ids == labels) & non_padding_mask\n",
    "    sum_non_padding = np.sum(non_padding_mask, axis=1); sum_correct_tokens = np.sum(correct_tokens, axis=1)\n",
    "    correct_sequences = np.where(sum_non_padding > 0, sum_correct_tokens == sum_non_padding, True)\n",
    "    accuracy = np.mean(correct_sequences); return float(accuracy)\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    \"\"\" Computes metrics for evaluation, specifically sequence accuracy. \"\"\"\n",
    "    predictions, labels = eval_pred; pad_token_id = tokenizer.pad_token_id\n",
    "    if pad_token_id is None:\n",
    "        pad_token_id = tokenizer.token_to_id(\"<pad>\")\n",
    "        if pad_token_id is None: raise ValueError(\"Cannot determine pad_token_id for metrics.\")\n",
    "    seq_acc = compute_sequence_accuracy(predictions, labels, pad_token_id); return {\"sequence_accuracy\": seq_acc}\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "def main():\n",
    "    \"\"\" Orchestrates the training/evaluation, compatible with Jupyter and terminal. \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Train/evaluate Seq2Seq model.\")\n",
    "    is_jupyter = any(['ipykernel' in arg for arg in sys.argv])\n",
    "\n",
    "    if is_jupyter:\n",
    "        logging.warning(\"Running in Jupyter/IPython! Using default arguments.\")\n",
    "        # Create a temporary dict excluding keys not needed by Namespace/older args\n",
    "        temp_defaults = {k: v for k, v in DEFAULT_ARGS.items() if k != 'eval_save_strategy'} # Example if strategy was in defaults\n",
    "        args = argparse.Namespace(**temp_defaults)\n",
    "        args.tokenizer_file = args.output_dir / args.tokenizer_file_name\n",
    "    else:\n",
    "        # Define arguments for command-line execution\n",
    "        parser.add_argument(\"--data_dir\", type=Path, default=DEFAULT_ARGS[\"data_dir\"])\n",
    "        parser.add_argument(\"--output_dir\", type=Path, default=DEFAULT_ARGS[\"output_dir\"])\n",
    "        parser.add_argument(\"--tokenizer_file_name\", type=str, default=DEFAULT_ARGS[\"tokenizer_file_name\"])\n",
    "        parser.add_argument(\"--encoder_model_id\", type=str, default=DEFAULT_ARGS[\"encoder_model_id\"])\n",
    "        parser.add_argument(\"--decoder_model_id\", type=str, default=DEFAULT_ARGS[\"decoder_model_id\"])\n",
    "        parser.add_argument(\"--max_seq_length\", type=int, default=DEFAULT_ARGS[\"max_seq_length\"])\n",
    "        parser.add_argument(\"--vocab_size\", type=int, default=DEFAULT_ARGS[\"vocab_size\"])\n",
    "        parser.add_argument(\"--num_train_epochs\", type=int, default=DEFAULT_ARGS[\"num_train_epochs\"])\n",
    "        parser.add_argument(\"--per_device_train_batch_size\", type=int, default=DEFAULT_ARGS[\"per_device_train_batch_size\"])\n",
    "        parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=DEFAULT_ARGS[\"per_device_eval_batch_size\"])\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=DEFAULT_ARGS[\"learning_rate\"])\n",
    "        parser.add_argument(\"--weight_decay\", type=float, default=DEFAULT_ARGS[\"weight_decay\"])\n",
    "        parser.add_argument(\"--warmup_steps\", type=int, default=DEFAULT_ARGS[\"warmup_steps\"])\n",
    "        parser.add_argument(\"--logging_steps\", type=int, default=DEFAULT_ARGS[\"logging_steps\"])\n",
    "        # Remove eval/save strategy args from parser definition\n",
    "        # parser.add_argument(\"--eval_save_strategy\", type=str, default=DEFAULT_ARGS[\"eval_save_strategy\"], choices=[\"epoch\", \"steps\"])\n",
    "        # parser.add_argument(\"--eval_save_steps\", type=int, default=DEFAULT_ARGS[\"eval_save_steps\"])\n",
    "        parser.add_argument(\"--seed\", type=int, default=DEFAULT_ARGS[\"seed\"])\n",
    "        args = parser.parse_args()\n",
    "        args.tokenizer_file = args.output_dir / args.tokenizer_file_name\n",
    "\n",
    "    args.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    logging.info(f\"Starting script with effective arguments: {args}\")\n",
    "\n",
    "    torch.manual_seed(args.seed); np.random.seed(args.seed); random.seed(args.seed)\n",
    "    logging.info(f\"Random seed set to {args.seed}\")\n",
    "    OUTPUT_PREFIX = 'qed_amplitudes'\n",
    "\n",
    "    data_files = {\n",
    "        \"train\": str(args.data_dir / f\"{OUTPUT_PREFIX}_train.jsonl\"),\n",
    "        \"validation\": str(args.data_dir / f\"{OUTPUT_PREFIX}_val.jsonl\"),\n",
    "        \"test\": str(args.data_dir / f\"{OUTPUT_PREFIX}_test.jsonl\"),\n",
    "    }\n",
    "    for split, path in data_files.items():\n",
    "        if not Path(path).is_file(): logging.error(f\"{split.capitalize()} data file not found: {path}\"); sys.exit(1)\n",
    "        logging.info(f\"Found {split} data file: {path}\")\n",
    "\n",
    "    train_custom_tokenizer(data_files, args.tokenizer_file, args.vocab_size)\n",
    "    try:\n",
    "        tokenizer = PreTrainedTokenizerFast(tokenizer_file=str(args.tokenizer_file),\n",
    "                                            bos_token=\"<s>\", eos_token=\"</s>\",\n",
    "                                            unk_token=\"<unk>\", pad_token=\"<pad>\")\n",
    "        if tokenizer.pad_token is None:\n",
    "             logging.warning(\"Manually adding PAD token '<pad>' to tokenizer.\")\n",
    "             tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "        logging.info(f\"Successfully loaded custom tokenizer from {args.tokenizer_file}\")\n",
    "        logging.info(f\"Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
    "        if tokenizer.pad_token_id is None: raise ValueError(\"Tokenizer pad_token_id is None.\")\n",
    "        logging.info(f\"Using PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "    except Exception as e: logging.exception(f\"Failed to load custom tokenizer: {e}\"); sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        logging.info(\"Loading datasets...\"); raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "        logging.info(f\"Datasets loaded: {raw_datasets}\")\n",
    "    except Exception as e: logging.exception(f\"Failed to load datasets: {e}\"); sys.exit(1)\n",
    "\n",
    "    logging.info(\"Tokenizing datasets...\")\n",
    "    try:\n",
    "        tokenize_partial = lambda examples: tokenize_function(examples, tokenizer, args.max_seq_length)\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_partial, batched=True, remove_columns=raw_datasets[\"train\"].column_names, desc=\"Running tokenizer on dataset\"\n",
    "        )\n",
    "        logging.info(f\"Tokenized datasets structure: {tokenized_datasets}\")\n",
    "    except Exception as e: logging.exception(f\"Failed during dataset tokenization: {e}\"); sys.exit(1)\n",
    "\n",
    "    logging.info(f\"Initializing Encoder-Decoder model ({args.encoder_model_id} -> {args.decoder_model_id})\")\n",
    "    try:\n",
    "        model = EncoderDecoderModel.from_encoder_decoder_pretrained(args.encoder_model_id, args.decoder_model_id)\n",
    "        logging.info(f\"Resizing ENCODER token embeddings to {tokenizer.vocab_size}\")\n",
    "        model.encoder.resize_token_embeddings(len(tokenizer))\n",
    "        logging.info(f\"Resizing DECODER token embeddings to {tokenizer.vocab_size}\")\n",
    "        model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "        model.config.decoder_start_token_id = tokenizer.bos_token_id; model.config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id; model.config.encoder.vocab_size = tokenizer.vocab_size\n",
    "        model.config.decoder.vocab_size = tokenizer.vocab_size; model.config.vocab_size = tokenizer.vocab_size\n",
    "        logging.info(\"Model configuration complete.\")\n",
    "    except Exception as e: logging.exception(f\"Failed to initialize or configure model: {e}\"); sys.exit(1)\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)\n",
    "    logging.info(\"Data collator initialized.\")\n",
    "\n",
    "    # --- MINIMAL Training Arguments Block (for compatibility testing) ---\n",
    "    logging.info(\"Defining Training Arguments (Minimal for compatibility check)...\")\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(args.output_dir),\n",
    "        # Required arguments\n",
    "        do_train=True,\n",
    "        do_eval=True, # Evaluation will happen at the end by default\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "        # Basic logging\n",
    "        logging_dir=str(args.output_dir / 'logs'),\n",
    "        logging_steps=args.logging_steps,\n",
    "        # Other standard args likely fine\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        seed=args.seed,\n",
    "        report_to=\"none\", # Disable external reporting like wandb\n",
    "        # --- Temporarily REMOVED all potentially problematic evaluation/saving args ---\n",
    "        # save_strategy=\"no\", # Removed even this\n",
    "        # evaluation_strategy=\"no\", # Removed even this\n",
    "        # save_total_limit=2, # Removed\n",
    "        load_best_model_at_end=False, # Cannot load best if not evaluating/saving during train\n",
    "        # metric_for_best_model=\"sequence_accuracy\", # Removed\n",
    "        # greater_is_better=True, # Removed\n",
    "        predict_with_generate=False, # Keep this\n",
    "    )\n",
    "    # --- End Minimal Block ---\n",
    "    logging.info(f\"Training arguments defined (Minimal).\")\n",
    "\n",
    "\n",
    "    compute_metrics_with_tokenizer = lambda eval_pred: compute_metrics(eval_pred, tokenizer)\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model, args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator, tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_with_tokenizer,\n",
    "    )\n",
    "    logging.info(\"Seq2SeqTrainer initialized.\")\n",
    "\n",
    "    logging.info(\"Starting model training...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        logging.info(\"Training finished successfully.\")\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        # Save the final model manually since automatic saving might be disabled\n",
    "        trainer.save_model(str(args.output_dir / \"final_model\"))\n",
    "        logging.info(f\"Final model saved to {args.output_dir / 'final_model'}\")\n",
    "    except Exception as e: logging.exception(f\"Training failed: {e}\"); sys.exit(1)\n",
    "\n",
    "    logging.info(\"Evaluating model on the test set...\")\n",
    "    try:\n",
    "        # Note: This evaluates the *final* model state, not necessarily the best validation one\n",
    "        test_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"], metric_key_prefix=\"test\")\n",
    "        trainer.log_metrics(\"test\", test_results)\n",
    "        trainer.save_metrics(\"test\", test_results)\n",
    "        if 'test_sequence_accuracy' in test_results:\n",
    "             print(f\"\\n--- Test Set Results ---\")\n",
    "             print(f\"  Test Sequence Accuracy: {test_results['test_sequence_accuracy']:.4f}\")\n",
    "             print(f\"------------------------\")\n",
    "        else:\n",
    "             logging.warning(\"'test_sequence_accuracy' not found in test evaluation results.\")\n",
    "             print(\"Test results:\", test_results)\n",
    "    except Exception as e: logging.exception(f\"Evaluation on test set failed: {e}\"); sys.exit(1)\n",
    "\n",
    "    logging.info(f\"Script finished. Model and results saved in {args.output_dir.resolve()}\")\n",
    "\n",
    "# --- Entry Point ---\n",
    "if __name__ == \"__main__\":\n",
    "     main()\n",
    "\n",
    "# --- Code to Run in Jupyter Cell ---\n",
    "# Copy all code above into a single cell, then run main() in the *next* cell:\n",
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Test 3: Train/Evaluate advanced model\n",
    "Repeat task two including checking sequence accuracy but with a model that leverages some slightly more advanced techniques. The model you use should relate to the project you’re applying for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 20:46:14 - INFO - [main] - ============================================================\n",
      "2025-04-22 20:46:14 - INFO - [main] -  Starting T5 Sequence-to-Sequence Model Training and Evaluation \n",
      "2025-04-22 20:46:14 - INFO - [main] - ============================================================\n",
      "2025-04-22 20:46:14 - INFO - [main] - Script execution started at: 2025-04-22 20:46:14\n",
      "2025-04-22 20:46:14 - INFO - [main] - CUDA available. Device: NVIDIA GeForce RTX 4090 Laptop GPU\n",
      "2025-04-22 20:46:14 - INFO - [main] - PyTorch CUDA version: 12.1\n",
      "2025-04-22 20:46:14 - INFO - [main] - Running with configuration:\n",
      "2025-04-22 20:46:14 - INFO - [main] -   train_file: qed_data_processed/qed_amplitudes_train.jsonl\n",
      "2025-04-22 20:46:14 - INFO - [main] -   val_file: qed_data_processed/qed_amplitudes_val.jsonl\n",
      "2025-04-22 20:46:14 - INFO - [main] -   test_file: qed_data_processed/qed_amplitudes_test.jsonl\n",
      "2025-04-22 20:46:14 - INFO - [main] -   output_dir: qed_t5_model_output_gpu_final\n",
      "2025-04-22 20:46:14 - INFO - [main] -   model_id: t5-base\n",
      "2025-04-22 20:46:14 - INFO - [main] -   task_prefix: \n",
      "2025-04-22 20:46:14 - INFO - [main] -   tokenizer_legacy: False\n",
      "2025-04-22 20:46:14 - INFO - [main] -   max_seq_length: 128\n",
      "2025-04-22 20:46:14 - INFO - [main] -   num_epochs: 5\n",
      "2025-04-22 20:46:14 - INFO - [main] -   train_batch_size: 16\n",
      "2025-04-22 20:46:14 - INFO - [main] -   eval_batch_size: 16\n",
      "2025-04-22 20:46:14 - INFO - [main] -   learning_rate: 0.0003\n",
      "2025-04-22 20:46:14 - INFO - [main] -   weight_decay: 0.01\n",
      "2025-04-22 20:46:14 - INFO - [main] -   warmup_steps: 200\n",
      "2025-04-22 20:46:14 - INFO - [main] -   logging_steps: 50\n",
      "2025-04-22 20:46:14 - INFO - [main] -   eval_strategy: epoch\n",
      "2025-04-22 20:46:14 - INFO - [main] -   eval_steps: 500\n",
      "2025-04-22 20:46:14 - INFO - [main] -   save_strategy: epoch\n",
      "2025-04-22 20:46:14 - INFO - [main] -   save_steps: 500\n",
      "2025-04-22 20:46:14 - INFO - [main] -   save_total_limit: 2\n",
      "2025-04-22 20:46:14 - INFO - [main] -   seed: 42\n",
      "2025-04-22 20:46:14 - INFO - [main] -   dataloader_num_workers: 0\n",
      "2025-04-22 20:46:14 - INFO - [main] -   fp16: True\n",
      "2025-04-22 20:46:14 - INFO - [main] -   gradient_checkpointing: True\n",
      "2025-04-22 20:46:14 - INFO - [main] -   label_smoothing: 0.1\n",
      "2025-04-22 20:46:14 - INFO - [main] -   num_beams: 4\n",
      "2025-04-22 20:46:14 - INFO - [main] -   report_to: tensorboard\n",
      "2025-04-22 20:46:14 - INFO - [main] - Output directory set to: qed_t5_model_output_gpu_final\n",
      "2025-04-22 20:46:14 - INFO - [main] - --- Stage 1: Loading Raw Data ---\n",
      "2025-04-22 20:46:14 - INFO - [load_jsonl] - Loading data from: qed_data_processed/qed_amplitudes_train.jsonl\n",
      "2025-04-22 20:46:14 - INFO - [load_jsonl] - Successfully loaded 12441 records from qed_data_processed/qed_amplitudes_train.jsonl.\n",
      "2025-04-22 20:46:14 - INFO - [load_jsonl] - Loading data from: qed_data_processed/qed_amplitudes_val.jsonl\n",
      "2025-04-22 20:46:15 - INFO - [load_jsonl] - Successfully loaded 1555 records from qed_data_processed/qed_amplitudes_val.jsonl.\n",
      "2025-04-22 20:46:15 - INFO - [load_jsonl] - Loading data from: qed_data_processed/qed_amplitudes_test.jsonl\n",
      "2025-04-22 20:46:15 - INFO - [load_jsonl] - Successfully loaded 1556 records from qed_data_processed/qed_amplitudes_test.jsonl.\n",
      "2025-04-22 20:46:15 - INFO - [main] - --- Raw Data Loading Complete ---\n",
      "2025-04-22 20:46:15 - INFO - [main] - --- Stage 2: Converting Tokens to Strings ---\n",
      "2025-04-22 20:46:15 - INFO - [convert_tokens_to_strings] - Attempting to convert 12441 items to strings...\n",
      "2025-04-22 20:46:15 - INFO - [convert_tokens_to_strings] - Successfully converted 12441 items to strings (skipped 0).\n",
      "2025-04-22 20:46:15 - INFO - [convert_tokens_to_strings] - Attempting to convert 1555 items to strings...\n",
      "2025-04-22 20:46:15 - INFO - [convert_tokens_to_strings] - Successfully converted 1555 items to strings (skipped 0).\n",
      "2025-04-22 20:46:15 - INFO - [convert_tokens_to_strings] - Attempting to convert 1556 items to strings...\n",
      "2025-04-22 20:46:15 - INFO - [convert_tokens_to_strings] - Successfully converted 1556 items to strings (skipped 0).\n",
      "2025-04-22 20:46:15 - INFO - [main] - --- Token-to-String Conversion Complete ---\n",
      "2025-04-22 20:46:15 - INFO - [main] - --- Stage 3: Initializing Tokenizer ---\n",
      "2025-04-22 20:46:15 - INFO - [main] - Tokenizer t5-base initialized (Fast: False, Vocab: 32000).\n",
      "2025-04-22 20:46:15 - INFO - [main] - --- Tokenizer Initialization Complete ---\n",
      "2025-04-22 20:46:15 - INFO - [main] - --- Stage 4: Encoding Data (Tokenization) ---\n",
      "2025-04-22 20:46:15 - INFO - [main] - Using Task Prefix: ''\n",
      "2025-04-22 20:46:15 - INFO - [main] - --> Encoding Training Data...\n",
      "2025-04-22 20:46:15 - INFO - [encode_sequences] - Starting encoding for 12441 pairs. MaxLen=128. Prefix=''. (Batch size: 500)\n",
      "2025-04-22 20:46:15 - INFO - [encode_sequences] -   Encoding batch 1-500/12441...\n",
      "2025-04-22 20:46:15 - INFO - [encode_sequences] -   Encoding batch 501-1000/12441...\n",
      "2025-04-22 20:46:15 - INFO - [encode_sequences] -   Encoding batch 1001-1500/12441...\n",
      "2025-04-22 20:46:16 - INFO - [encode_sequences] -   Encoding batch 1501-2000/12441...\n",
      "2025-04-22 20:46:16 - INFO - [encode_sequences] -   Encoding batch 2001-2500/12441...\n",
      "2025-04-22 20:46:16 - INFO - [encode_sequences] -   Encoding batch 2501-3000/12441...\n",
      "2025-04-22 20:46:16 - INFO - [encode_sequences] -   Encoding batch 3001-3500/12441...\n",
      "2025-04-22 20:46:17 - INFO - [encode_sequences] -   Encoding batch 3501-4000/12441...\n",
      "2025-04-22 20:46:17 - INFO - [encode_sequences] -   Encoding batch 4001-4500/12441...\n",
      "2025-04-22 20:46:17 - INFO - [encode_sequences] -   Encoding batch 4501-5000/12441...\n",
      "2025-04-22 20:46:17 - INFO - [encode_sequences] -   Encoding batch 5001-5500/12441...\n",
      "2025-04-22 20:46:18 - INFO - [encode_sequences] -   Encoding batch 5501-6000/12441...\n",
      "2025-04-22 20:46:18 - INFO - [encode_sequences] -   Encoding batch 6001-6500/12441...\n",
      "2025-04-22 20:46:18 - INFO - [encode_sequences] -   Encoding batch 6501-7000/12441...\n",
      "2025-04-22 20:46:18 - INFO - [encode_sequences] -   Encoding batch 7001-7500/12441...\n",
      "2025-04-22 20:46:18 - INFO - [encode_sequences] -   Encoding batch 7501-8000/12441...\n",
      "2025-04-22 20:46:19 - INFO - [encode_sequences] -   Encoding batch 8001-8500/12441...\n",
      "2025-04-22 20:46:19 - INFO - [encode_sequences] -   Encoding batch 8501-9000/12441...\n",
      "2025-04-22 20:46:19 - INFO - [encode_sequences] -   Encoding batch 9001-9500/12441...\n",
      "2025-04-22 20:46:19 - INFO - [encode_sequences] -   Encoding batch 9501-10000/12441...\n",
      "2025-04-22 20:46:20 - INFO - [encode_sequences] -   Encoding batch 10001-10500/12441...\n",
      "2025-04-22 20:46:20 - INFO - [encode_sequences] -   Encoding batch 10501-11000/12441...\n",
      "2025-04-22 20:46:20 - INFO - [encode_sequences] -   Encoding batch 11001-11500/12441...\n",
      "2025-04-22 20:46:20 - INFO - [encode_sequences] -   Encoding batch 11501-12000/12441...\n",
      "2025-04-22 20:46:21 - INFO - [encode_sequences] -   Encoding batch 12001-12441/12441...\n",
      "2025-04-22 20:46:21 - INFO - [encode_sequences] - Finished encoding all 12441 sequence pairs.\n",
      "2025-04-22 20:46:21 - INFO - [main] - --> Encoding Validation Data...\n",
      "2025-04-22 20:46:21 - INFO - [encode_sequences] - Starting encoding for 1555 pairs. MaxLen=128. Prefix=''. (Batch size: 500)\n",
      "2025-04-22 20:46:21 - INFO - [encode_sequences] -   Encoding batch 1-500/1555...\n",
      "2025-04-22 20:46:21 - INFO - [encode_sequences] -   Encoding batch 501-1000/1555...\n",
      "2025-04-22 20:46:21 - INFO - [encode_sequences] -   Encoding batch 1001-1500/1555...\n",
      "2025-04-22 20:46:22 - INFO - [encode_sequences] -   Encoding batch 1501-1555/1555...\n",
      "2025-04-22 20:46:22 - INFO - [encode_sequences] - Finished encoding all 1555 sequence pairs.\n",
      "2025-04-22 20:46:22 - INFO - [main] - --> Encoding Test Data...\n",
      "2025-04-22 20:46:22 - INFO - [encode_sequences] - Starting encoding for 1556 pairs. MaxLen=128. Prefix=''. (Batch size: 500)\n",
      "2025-04-22 20:46:22 - INFO - [encode_sequences] -   Encoding batch 1-500/1556...\n",
      "2025-04-22 20:46:22 - INFO - [encode_sequences] -   Encoding batch 501-1000/1556...\n",
      "2025-04-22 20:46:22 - INFO - [encode_sequences] -   Encoding batch 1001-1500/1556...\n",
      "2025-04-22 20:46:22 - INFO - [encode_sequences] -   Encoding batch 1501-1556/1556...\n",
      "2025-04-22 20:46:22 - INFO - [encode_sequences] - Finished encoding all 1556 sequence pairs.\n",
      "2025-04-22 20:46:22 - INFO - [main] - --- Data Encoding Phase Complete ---\n",
      "2025-04-22 20:46:22 - INFO - [main] - --- Stage 5: Creating PyTorch Datasets ---\n",
      "2025-04-22 20:46:22 - INFO - [__init__] - Created SequenceDataset with 12441 examples.\n",
      "2025-04-22 20:46:22 - INFO - [__init__] - Created SequenceDataset with 1555 examples.\n",
      "2025-04-22 20:46:22 - INFO - [__init__] - Created SequenceDataset with 1556 examples.\n",
      "2025-04-22 20:46:22 - INFO - [main] - --- PyTorch Datasets Created Successfully ---\n",
      "2025-04-22 20:46:22 - INFO - [main] - --- Stage 6: Initializing Model ---\n",
      "2025-04-22 20:46:23 - INFO - [main] - Model t5-base loaded.\n",
      "2025-04-22 20:46:23 - INFO - [main] - Est. Model Memory: 0.89 GB\n",
      "2025-04-22 20:46:23 - INFO - [main] - --- Model Initialization Complete ---\n",
      "2025-04-22 20:46:23 - INFO - [main] - --- Stage 7: Configuring Training Environment ---\n",
      "2025-04-22 20:46:23 - INFO - [main] - Attempting to enable Gradient Checkpointing...\n",
      "2025-04-22 20:46:23 - INFO - [main] - Gradient Checkpointing enabled via model method.\n",
      "2025-04-22 20:46:23 - INFO - [main] - Initializing Data Collator...\n",
      "2025-04-22 20:46:23 - INFO - [main] - Data Collator initialized.\n",
      "2025-04-22 20:46:23 - INFO - [main] - Defining Training Arguments...\n",
      "2025-04-22 20:46:23 - INFO - [main] - Reporting to tensorboard.\n",
      "2025-04-22 20:46:23 - INFO - [main] - Successfully initialized Seq2SeqTrainingArguments.\n",
      "2025-04-22 20:46:23 - INFO - [main] - Effective FP16: True, Grad Checkpointing: True\n",
      "2025-04-22 20:46:23 - INFO - [main] - --- Training Environment Configuration Complete ---\n",
      "2025-04-22 20:46:23 - INFO - [main] - --- Stage 8: Initializing Seq2SeqTrainer ---\n",
      "/tmp/ipykernel_99334/3935573170.py:512: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-04-22 20:46:23 - INFO - [main] - Seq2SeqTrainer Initialized.\n",
      "2025-04-22 20:46:23 - INFO - [main] - --- Stage 9: Starting Model Training ---\n",
      "2025-04-22 20:46:23 - INFO - [main] - Starting training for 5 epochs...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3890' max='3890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3890/3890 22:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Sequence Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.448400</td>\n",
       "      <td>1.422035</td>\n",
       "      <td>0.215434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.420400</td>\n",
       "      <td>1.405642</td>\n",
       "      <td>0.326688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.412800</td>\n",
       "      <td>1.401123</td>\n",
       "      <td>0.342122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.408100</td>\n",
       "      <td>1.397476</td>\n",
       "      <td>0.404502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.405500</td>\n",
       "      <td>1.395355</td>\n",
       "      <td>0.406431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "2025-04-22 20:50:58 - INFO - [compute_metrics_fn] - Computed sequence accuracy: 0.2154\n",
      "2025-04-22 20:55:34 - INFO - [compute_metrics_fn] - Computed sequence accuracy: 0.3267\n",
      "2025-04-22 21:00:11 - INFO - [compute_metrics_fn] - Computed sequence accuracy: 0.3421\n",
      "2025-04-22 21:04:44 - INFO - [compute_metrics_fn] - Computed sequence accuracy: 0.4045\n",
      "2025-04-22 21:09:16 - INFO - [compute_metrics_fn] - Computed sequence accuracy: 0.4064\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "2025-04-22 21:09:18 - INFO - [main] - --- Training Finished ---\n",
      "2025-04-22 21:09:18 - INFO - [main] - --- Training Metrics ---\n",
      "2025-04-22 21:09:18 - INFO - [main] - Trainer state saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  total_flos               =  8819677GF\n",
      "  train_loss               =     1.4702\n",
      "  train_runtime            = 0:22:54.71\n",
      "  train_samples_per_second =      45.25\n",
      "  train_steps_per_second   =       2.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 21:09:19 - INFO - [main] - Final best model saved to: qed_t5_model_output_gpu_final/best_model\n",
      "2025-04-22 21:09:19 - INFO - [main] - --- Stage 10: Evaluating Model on Test Set ---\n",
      "2025-04-22 21:09:19 - INFO - [main] - Running final evaluation on the test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='98' max='98' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [98/98 01:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 21:11:33 - INFO - [compute_metrics_fn] - Computed sequence accuracy: 0.4254\n",
      "2025-04-22 21:11:33 - INFO - [main] - --- Evaluation on Test Set Finished ---\n",
      "2025-04-22 21:11:33 - INFO - [main] - --- Test Set Results ---\n",
      "2025-04-22 21:11:33 - INFO - [main] - \n",
      "*** Test Sequence Accuracy: 0.4254 ***\n",
      "\n",
      "2025-04-22 21:11:33 - INFO - [main] - --- Final Evaluation Stage Complete ---\n",
      "2025-04-22 21:11:33 - INFO - [main] - ============================================================\n",
      "2025-04-22 21:11:33 - INFO - [main] -  Script execution finished at: 2025-04-22 21:11:33\n",
      "2025-04-22 21:11:33 - INFO - [main] -  Total execution time: 0:25:19.068186\n",
      "2025-04-22 21:11:33 - INFO - [main] - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** test metrics *****\n",
      "  epoch                   =        5.0\n",
      "  test_loss               =     1.3951\n",
      "  test_runtime            = 0:02:14.57\n",
      "  test_samples_per_second =     11.562\n",
      "  test_sequence_accuracy  =     0.4254\n",
      "  test_steps_per_second   =      0.728\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "End-to-end Training and Evaluation Script for a T5 Sequence-to-Sequence Model\n",
    "on the Preprocessed QED 2-to-2 Tree-Level Dataset.\n",
    "\n",
    "*** NOTE: This version is significantly modified for robust execution ***\n",
    "*** within a Jupyter Notebook (.ipynb) by REMOVING ARGPARSE.   ***\n",
    "*** ***\n",
    "*** Configuration is now handled by manually setting attributes on    ***\n",
    "*** the `args` object defined near the top of the script below.       ***\n",
    "*** Edit the `args` object directly to change parameters.             ***\n",
    "*** ***\n",
    "*** CORRECTED parameter name from evaluation_strategy to eval_strategy ***\n",
    "*** Removed diagnostic blocks for cleaner notebook execution.        ***\n",
    "*** ***\n",
    "*** This script is designed to automatically utilize GPUs via     ***\n",
    "*** Hugging Face Trainer/Accelerate when run in a correctly     ***\n",
    "*** configured environment (like the one from t5_gpu_env.yaml or t5_pip_test). ***\n",
    "\n",
    "Dependencies:\n",
    "- Working Python 3.10 environment (e.g., t5_pip_test) with necessary packages.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime # Use the standard datetime module\n",
    "import logging\n",
    "import argparse # Still used for the Namespace object, but not for parsing\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "import inspect # Keep inspect if needed by other parts, otherwise optional now\n",
    "# Import the specific classes needed\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainingArguments # Import base class for comparison if needed\n",
    ")\n",
    "from transformers.trainer_utils import set_seed # Use transformers' set_seed\n",
    "\n",
    "\n",
    "# Configure basic logging (Set level to DEBUG for detailed output, INFO for progress)\n",
    "# Check if logger already exists (useful in notebooks where cells might be re-run)\n",
    "logger = logging.getLogger(__name__)\n",
    "if not logger.hasHandlers():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, # INFO shows progress, DEBUG shows fine details\n",
    "        format=\"%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        # Use stream=sys.stdout to ensure output appears in notebook cell\n",
    "        stream=sys.stdout\n",
    "    )\n",
    "else:\n",
    "    # Ensure level is still INFO if logger was configured previously\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# <<< CONFIGURATION >>>\n",
    "# =============================================================================\n",
    "# --- EDIT THESE VALUES TO CONFIGURE THE RUN ---\n",
    "args = argparse.Namespace(\n",
    "    # --- File Paths ---\n",
    "    train_file='qed_data_processed/qed_amplitudes_train.jsonl',\n",
    "    val_file='qed_data_processed/qed_amplitudes_val.jsonl',\n",
    "    test_file='qed_data_processed/qed_amplitudes_test.jsonl',\n",
    "    output_dir='qed_t5_model_output_gpu_final', # Consider a new output dir\n",
    "\n",
    "    # --- Model Configuration ---\n",
    "    model_id=\"t5-base\",\n",
    "    task_prefix=\"\",\n",
    "    tokenizer_legacy=False,\n",
    "\n",
    "    # --- Tokenizer and Data Processing ---\n",
    "    max_seq_length=128,\n",
    "\n",
    "    # --- Training Hyperparameters ---\n",
    "    num_epochs=5,\n",
    "    # *** Adjust batch sizes based on GPU memory (RTX 4090 16GB can likely handle larger) ***\n",
    "    train_batch_size=16,    # Increased from 8, monitor GPU memory usage\n",
    "    eval_batch_size=16,     # Increased from 8\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=200,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",         # <<< CORRECTED NAME\n",
    "    eval_steps=500, # Note: eval_steps is ignored if eval_strategy is 'epoch'\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=500, # Note: save_steps is ignored if save_strategy is 'epoch'\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    dataloader_num_workers=0, # 0 is safest for notebooks, increase if needed\n",
    "\n",
    "    # --- Advanced Training Features ---\n",
    "    fp16=torch.cuda.is_available(), # Automatically use FP16 if GPU available\n",
    "    gradient_checkpointing=True, # Good for saving memory on large models\n",
    "    label_smoothing=0.1,\n",
    "\n",
    "    # --- Generation Configuration (for evaluation) ---\n",
    "    num_beams=4,\n",
    "\n",
    "    # --- Reporting ---\n",
    "    report_to=\"tensorboard\", # Or \"none\", \"wandb\", etc.\n",
    ")\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "# (Helper Functions: load_jsonl, convert_tokens_to_strings, encode_sequences, SequenceDataset, compute_metrics_fn)\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads a JSON Lines (.jsonl) file, skipping blank/whitespace-only lines.\"\"\"\n",
    "    data = []\n",
    "    file_path = Path(file_path)\n",
    "    logger.info(f\"Loading data from: {file_path}\")\n",
    "    if not file_path.is_file():\n",
    "        logger.error(f\"Data file not found: {file_path}\")\n",
    "        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        logger.warning(f\"Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "                        continue\n",
    "        logger.info(f\"Successfully loaded {len(data)} records from {file_path}.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load data from {file_path}: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "def convert_tokens_to_strings(raw_data_list):\n",
    "    \"\"\"Converts lists of tokens into single whitespace-joined strings.\"\"\"\n",
    "    input_strings = []\n",
    "    target_strings = []\n",
    "    if not raw_data_list:\n",
    "        logger.warning(\"Input data list is empty for token-to-string conversion.\")\n",
    "        return input_strings, target_strings\n",
    "\n",
    "    skipped_count = 0\n",
    "    logger.info(f\"Attempting to convert {len(raw_data_list)} items to strings...\")\n",
    "    for i, item in enumerate(raw_data_list):\n",
    "        input_toks = item.get('input_tokens')\n",
    "        target_toks = item.get('target_tokens')\n",
    "\n",
    "        if isinstance(input_toks, list) and isinstance(target_toks, list):\n",
    "            try:\n",
    "                # Ensure all tokens are strings before joining\n",
    "                str_input_toks = [str(tok) for tok in input_toks]\n",
    "                str_target_toks = [str(tok) for tok in target_toks]\n",
    "                joined_input = \" \".join(str_input_toks)\n",
    "                joined_target = \" \".join(str_target_toks)\n",
    "                input_strings.append(joined_input)\n",
    "                target_strings.append(joined_target)\n",
    "            except Exception as e:\n",
    "                 logger.warning(f\"Error joining tokens for item at index {i}: {e}. Item: {item}\", exc_info=True)\n",
    "                 skipped_count += 1\n",
    "        else:\n",
    "            missing_keys = []\n",
    "            if 'input_tokens' not in item: missing_keys.append('input_tokens')\n",
    "            if 'target_tokens' not in item: missing_keys.append('target_tokens')\n",
    "            invalid_types = []\n",
    "            if 'input_tokens' in item and not isinstance(input_toks, list): invalid_types.append(f\"input_tokens (type: {type(input_toks).__name__})\")\n",
    "            if 'target_tokens' in item and not isinstance(target_toks, list): invalid_types.append(f\"target_tokens (type: {type(target_toks).__name__})\")\n",
    "            log_msg = f\"Skipping item at index {i}.\"\n",
    "            if missing_keys: log_msg += f\" Missing keys: {missing_keys}.\"\n",
    "            if invalid_types: log_msg += f\" Keys with non-list values: {invalid_types}.\"\n",
    "            item_str = str(item)\n",
    "            if len(item_str) > 200: item_str = item_str[:200] + \"...\"\n",
    "            log_msg += f\" Item (truncated): {item_str}\"\n",
    "            logger.warning(log_msg)\n",
    "            skipped_count += 1\n",
    "\n",
    "    logger.info(f\"Successfully converted {len(input_strings)} items to strings (skipped {skipped_count}).\")\n",
    "    if not input_strings and skipped_count > 0 and skipped_count == len(raw_data_list):\n",
    "         logger.error(\"All items were skipped during token-to-string conversion.\")\n",
    "         raise ValueError(\"Failed to convert any items to strings.\")\n",
    "    elif not input_strings and len(raw_data_list) > 0 and skipped_count != len(raw_data_list):\n",
    "        logger.error(f\"Conversion resulted in an empty list of strings, but {len(raw_data_list)} items were processed ({skipped_count} skipped).\")\n",
    "        raise ValueError(\"String conversion unexpectedly produced no output despite valid input items existing.\")\n",
    "    elif not input_strings and not raw_data_list:\n",
    "        logger.warning(\"Input data list was empty, resulting in empty string lists.\")\n",
    "    return input_strings, target_strings\n",
    "\n",
    "\n",
    "def encode_sequences(tokenizer, input_strings, target_strings, max_len, task_prefix=\"\", batch_size=500):\n",
    "    \"\"\"Tokenizes input and target string pairs using the T5 tokenizer.\"\"\"\n",
    "    logger.info(f\"Starting encoding for {len(input_strings)} pairs. MaxLen={max_len}. Prefix='{task_prefix}'. (Batch size: {batch_size})\")\n",
    "\n",
    "    if not isinstance(input_strings, list) or not isinstance(target_strings, list):\n",
    "        raise TypeError(\"input_strings and target_strings must be lists.\")\n",
    "    if not input_strings or not target_strings:\n",
    "        logger.warning(\"Received empty list(s) for encoding. Returning empty.\")\n",
    "        return {'input_ids': [], 'attention_mask': [], 'labels': []}\n",
    "    if len(input_strings) != len(target_strings):\n",
    "        raise ValueError(f\"Input ({len(input_strings)}) and Target ({len(target_strings)}) string lists must have the same length.\")\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_labels = []\n",
    "    num_sequences = len(input_strings)\n",
    "\n",
    "    for i in range(0, num_sequences, batch_size):\n",
    "        batch_start = i\n",
    "        batch_end = min(i + batch_size, num_sequences)\n",
    "        logger.info(f\"  Encoding batch {batch_start+1}-{batch_end}/{num_sequences}...\")\n",
    "\n",
    "        input_batch = input_strings[batch_start:batch_end]\n",
    "        target_batch = target_strings[batch_start:batch_end]\n",
    "\n",
    "        # Encode Inputs\n",
    "        try:\n",
    "            processed_input_batch = [f\"{task_prefix}{s}\" if task_prefix else s for s in input_batch]\n",
    "            # Use batch encoding directly\n",
    "            encoder_outputs = tokenizer(\n",
    "                processed_input_batch, max_length=max_len, padding='max_length', truncation=True, return_tensors=None\n",
    "            )\n",
    "            batch_input_ids = encoder_outputs['input_ids']\n",
    "            batch_attention_mask = encoder_outputs['attention_mask']\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Tokenizer CRASHED on ENCODER batch: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "        # Encode Targets (Labels)\n",
    "        try:\n",
    "            # Use text_target argument for T5/Seq2Seq models\n",
    "            decoder_outputs = tokenizer(\n",
    "                text_target=target_batch, max_length=max_len, padding='max_length', truncation=True, return_tensors=None\n",
    "            )\n",
    "            batch_labels = decoder_outputs['input_ids']\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Tokenizer CRASHED on DECODER batch: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "        # Validation before extending\n",
    "        if len(batch_input_ids) != len(input_batch) or len(batch_attention_mask) != len(input_batch) or len(batch_labels) != len(target_batch):\n",
    "            raise ValueError(f\"Batch encoding length mismatch! Input={len(batch_input_ids)}, Attn={len(batch_attention_mask)}, Labels={len(batch_labels)}, Expected={len(input_batch)}\")\n",
    "\n",
    "        all_input_ids.extend(batch_input_ids)\n",
    "        all_attention_mask.extend(batch_attention_mask)\n",
    "        all_labels.extend(batch_labels)\n",
    "\n",
    "    logger.info(f\"Finished encoding all {len(all_input_ids)} sequence pairs.\")\n",
    "    if len(all_input_ids) != num_sequences:\n",
    "         raise ValueError(f\"Encoded sequence count ({len(all_input_ids)}) mismatch original ({num_sequences}).\")\n",
    "\n",
    "    return {'input_ids': all_input_ids, 'attention_mask': all_attention_mask, 'labels': all_labels}\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset wrapper for tokenized sequence data.\"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        required_keys = ['input_ids', 'attention_mask', 'labels']\n",
    "        if not isinstance(encodings, dict) or not all(key in encodings for key in required_keys):\n",
    "            logger.error(f\"Invalid encodings passed to SequenceDataset. Got Keys: {list(encodings.keys()) if isinstance(encodings, dict) else type(encodings)}\")\n",
    "            raise ValueError(f\"Encodings must be a dict with keys: {required_keys}.\")\n",
    "        try:\n",
    "            lengths = {key: len(val) for key, val in encodings.items() if key in required_keys and isinstance(val, list)}\n",
    "            if len(lengths) != len(required_keys):\n",
    "                 missing_or_wrong_type = [k for k in required_keys if k not in lengths]\n",
    "                 raise ValueError(f\"Missing or non-list required keys in encodings: {missing_or_wrong_type}\")\n",
    "            if len(set(lengths.values())) > 1:\n",
    "                 raise ValueError(f\"Inconsistent lengths in encodings: {lengths}\")\n",
    "            self.length = lengths.get('input_ids', 0)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed validating encoding structure/lengths: {e}\") from e\n",
    "\n",
    "        self.encodings = {k: v for k, v in encodings.items() if k in required_keys}\n",
    "        if self.length == 0: logger.warning(\"Initializing SequenceDataset with length 0.\")\n",
    "        logger.info(f\"Created SequenceDataset with {self.length} examples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not 0 <= idx < self.length: raise IndexError(f\"Index {idx} out of bounds ({self.length}).\")\n",
    "        try:\n",
    "            item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "            if 'labels' not in item: raise KeyError(f\"'labels' key missing for index {idx}\")\n",
    "            return item\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed creating item at index {idx}: {e}\", exc_info=True)\n",
    "            raise IndexError(f\"Error retrieving item at index {idx}: {e}\") from e\n",
    "\n",
    "\n",
    "tokenizer_for_metrics = None # Global tokenizer for compute_metrics\n",
    "\n",
    "def compute_metrics_fn(eval_pred):\n",
    "    \"\"\"Calculates exact match accuracy between decoded predictions and labels.\"\"\"\n",
    "    global tokenizer_for_metrics\n",
    "    if tokenizer_for_metrics is None:\n",
    "        logger.error(\"Tokenizer missing in compute_metrics!\")\n",
    "        return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple): predictions = predictions[0]\n",
    "    if predictions.ndim == 3 and predictions.shape[-1] > 1:\n",
    "         predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    if not isinstance(predictions, np.ndarray): predictions = np.array(predictions)\n",
    "    if not isinstance(labels, np.ndarray): labels = np.array(labels)\n",
    "\n",
    "    labels = labels.astype(np.int64)\n",
    "    pad_token_id = tokenizer_for_metrics.pad_token_id\n",
    "    labels = np.where(labels != -100, labels, pad_token_id)\n",
    "\n",
    "    try:\n",
    "        decoded_preds = tokenizer_for_metrics.batch_decode(predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        decoded_labels = tokenizer_for_metrics.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Decode failed in compute_metrics: {e}\", exc_info=True)\n",
    "        return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    if not decoded_preds or not decoded_labels or len(decoded_preds) != len(decoded_labels):\n",
    "        logger.warning(f\"Metrics decode issue: Preds empty={not decoded_preds}, Labels empty={not decoded_labels}, LenPred={len(decoded_preds)}, LenLabel={len(decoded_labels)}\")\n",
    "        return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "    matches = [pred == label for pred, label in zip(decoded_preds, decoded_labels)]\n",
    "    accuracy = np.mean(matches) if matches else 0.0\n",
    "\n",
    "    logger.info(f\"Computed sequence accuracy: {accuracy:.4f}\")\n",
    "    return {\"sequence_accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main Execution Logic\n",
    "# =============================================================================\n",
    "\n",
    "def main(config_args):\n",
    "    \"\"\"Orchestrates the T5 training and evaluation pipeline using config object.\"\"\"\n",
    "    args = config_args # Use the config object passed as argument\n",
    "\n",
    "    # --- Basic Setup ---\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\" Starting T5 Sequence-to-Sequence Model Training and Evaluation \")\n",
    "    logger.info(\"=\"*60)\n",
    "    start_time = datetime.datetime.now()\n",
    "    logger.info(f\"Script execution started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"CUDA available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "        logger.info(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        logger.warning(\"CUDA not available. Running on CPU.\")\n",
    "        if args.fp16:\n",
    "             logger.warning(\"fp16=True ignored because CUDA is not available.\")\n",
    "             args.fp16 = False # Override config if no GPU\n",
    "\n",
    "    logger.info(f\"Running with configuration:\")\n",
    "    for k, v in vars(args).items(): logger.info(f\"  {k}: {v}\")\n",
    "\n",
    "    set_seed(args.seed)\n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"Output directory set to: {output_dir}\")\n",
    "\n",
    "    # --- Data Loading and Preprocessing ---\n",
    "    logger.info(\"--- Stage 1: Loading Raw Data ---\")\n",
    "    train_raw = load_jsonl(args.train_file)\n",
    "    val_raw = load_jsonl(args.val_file)\n",
    "    test_raw = load_jsonl(args.test_file)\n",
    "    if not train_raw or not val_raw: # Test raw is optional for training\n",
    "        raise ValueError(\"Training and/or validation raw datasets empty.\")\n",
    "    logger.info(\"--- Raw Data Loading Complete ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 2: Converting Tokens to Strings ---\")\n",
    "    train_in, train_tgt = convert_tokens_to_strings(train_raw)\n",
    "    val_in, val_tgt = convert_tokens_to_strings(val_raw)\n",
    "    test_in, test_tgt = convert_tokens_to_strings(test_raw) # Convert test even if empty list results\n",
    "    if not train_in or not val_in:\n",
    "        raise ValueError(\"Training and/or validation datasets empty after string conversion.\")\n",
    "    logger.info(\"--- Token-to-String Conversion Complete ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 3: Initializing Tokenizer ---\")\n",
    "    try:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(args.model_id, legacy=args.tokenizer_legacy)\n",
    "        global tokenizer_for_metrics; tokenizer_for_metrics = tokenizer\n",
    "        logger.info(f\"Tokenizer {args.model_id} initialized (Fast: {tokenizer.is_fast}, Vocab: {tokenizer.vocab_size}).\")\n",
    "    except Exception as e: logger.error(f\"Failed initializing tokenizer: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- Tokenizer Initialization Complete ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 4: Encoding Data (Tokenization) ---\")\n",
    "    task_prefix = args.task_prefix or \"\"\n",
    "    logger.info(f\"Using Task Prefix: '{task_prefix}'\")\n",
    "    try:\n",
    "        logger.info(\"--> Encoding Training Data...\")\n",
    "        train_enc = encode_sequences(tokenizer, train_in, train_tgt, args.max_seq_length, task_prefix)\n",
    "        if not train_enc.get('input_ids'): raise ValueError(\"Training encoding failed (returned empty).\")\n",
    "\n",
    "        logger.info(\"--> Encoding Validation Data...\")\n",
    "        val_enc = encode_sequences(tokenizer, val_in, val_tgt, args.max_seq_length, task_prefix)\n",
    "        if not val_enc.get('input_ids'): raise ValueError(\"Validation encoding failed (returned empty).\")\n",
    "\n",
    "        # Encode test data if available\n",
    "        test_enc = None\n",
    "        if test_in and test_tgt:\n",
    "            logger.info(\"--> Encoding Test Data...\")\n",
    "            test_enc = encode_sequences(tokenizer, test_in, test_tgt, args.max_seq_length, task_prefix)\n",
    "            if not test_enc.get('input_ids'): logger.warning(\"Test encoding returned empty.\")\n",
    "        else:\n",
    "             logger.info(\"Test input/target strings empty, skipping test encoding.\")\n",
    "\n",
    "    except Exception as e: logger.error(f\"Data encoding failed: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- Data Encoding Phase Complete ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 5: Creating PyTorch Datasets ---\")\n",
    "    try:\n",
    "        train_ds = SequenceDataset(train_enc)\n",
    "        val_ds = SequenceDataset(val_enc)\n",
    "        test_ds = SequenceDataset(test_enc) if test_enc and test_enc.get('input_ids') else None # Only create if encoding successful\n",
    "    except Exception as e: logger.error(f\"Dataset creation failed: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- PyTorch Datasets Created Successfully ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 6: Initializing Model ---\")\n",
    "    try:\n",
    "        model = T5ForConditionalGeneration.from_pretrained(args.model_id)\n",
    "        logger.info(f\"Model {args.model_id} loaded.\")\n",
    "        if torch.cuda.is_available(): logger.info(f\"Est. Model Memory: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "    except Exception as e: logger.error(f\"Failed initializing model: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- Model Initialization Complete ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 7: Configuring Training Environment ---\")\n",
    "    use_gc = args.gradient_checkpointing\n",
    "    if use_gc: logger.info(\"Attempting to enable Gradient Checkpointing...\")\n",
    "    try:\n",
    "        if use_gc and hasattr(model, 'gradient_checkpointing_enable'):\n",
    "             model.gradient_checkpointing_enable(); logger.info(\"Gradient Checkpointing enabled via model method.\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed enabling Gradient Checkpointing on model: {e}. Relying on TrainingArguments setting.\", exc_info=True)\n",
    "        use_gc = args.gradient_checkpointing # Keep desired value\n",
    "\n",
    "    use_fp16 = args.fp16 # Already checked against cuda availability\n",
    "    logger.info(\"Initializing Data Collator...\")\n",
    "    try:\n",
    "        collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100, pad_to_multiple_of=8 if use_fp16 else None)\n",
    "        logger.info(\"Data Collator initialized.\")\n",
    "    except Exception as e: logger.error(f\"Collator init failed: {e}\", exc_info=True); raise\n",
    "\n",
    "\n",
    "    # <<<--- DIRECT INITIALIZATION (Corrected Parameter Name) --- >>>\n",
    "    logger.info(\"Defining Training Arguments...\")\n",
    "    report_to = args.report_to.lower() if isinstance(args.report_to, str) else args.report_to\n",
    "    if report_to == \"none\": logger.info(\"Reporting disabled.\")\n",
    "    elif report_to == \"tensorboard\": logger.info(\"Reporting to tensorboard.\")\n",
    "    else: logger.info(f\"Reporting to: {report_to}\")\n",
    "\n",
    "    train_args = None # Initialize\n",
    "    try:\n",
    "        # Note: No signature check needed now, using corrected param name\n",
    "        train_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=str(output_dir),\n",
    "            # Core Training Params\n",
    "            num_train_epochs=args.num_epochs,\n",
    "            per_device_train_batch_size=args.train_batch_size,\n",
    "            per_device_eval_batch_size=args.eval_batch_size,\n",
    "            learning_rate=args.learning_rate,\n",
    "            weight_decay=args.weight_decay,\n",
    "            warmup_steps=args.warmup_steps,\n",
    "            seed=args.seed,\n",
    "            # Evaluation and Saving Strategy (Corrected Name)\n",
    "            eval_strategy=args.eval_strategy,               # <<< CORRECTED\n",
    "            eval_steps=args.eval_steps if args.eval_strategy == \"steps\" else None, # <<< CORRECTED\n",
    "            save_strategy=args.save_strategy,\n",
    "            save_steps=args.save_steps if args.save_strategy == \"steps\" else None,\n",
    "            save_total_limit=args.save_total_limit,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"sequence_accuracy\",\n",
    "            greater_is_better=True,\n",
    "            # Logging\n",
    "            logging_dir=str(output_dir / 'logs'),\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=args.logging_steps,\n",
    "            report_to=report_to,\n",
    "            # Performance / Hardware\n",
    "            fp16=use_fp16,\n",
    "            gradient_checkpointing=use_gc,\n",
    "            dataloader_num_workers=args.dataloader_num_workers,\n",
    "            # Seq2Seq Specific\n",
    "            predict_with_generate=True,\n",
    "            generation_max_length=args.max_seq_length,\n",
    "            generation_num_beams=args.num_beams,\n",
    "            label_smoothing_factor=args.label_smoothing,\n",
    "        )\n",
    "        logger.info(\"Successfully initialized Seq2SeqTrainingArguments.\")\n",
    "        logger.info(f\"Effective FP16: {train_args.fp16}, Grad Checkpointing: {train_args.gradient_checkpointing}\")\n",
    "\n",
    "    except TypeError as te:\n",
    "        logger.error(f\"FAILED to initialize Seq2SeqTrainingArguments: {te}\", exc_info=True)\n",
    "        raise te # Re-raise to stop execution\n",
    "    except Exception as e:\n",
    "        logger.error(f\"FAILED to initialize Seq2SeqTrainingArguments with unexpected error: {e}\", exc_info=True)\n",
    "        raise e\n",
    "\n",
    "    if train_args is None:\n",
    "        raise RuntimeError(\"train_args was not successfully defined.\")\n",
    "\n",
    "    logger.info(\"--- Training Environment Configuration Complete ---\")\n",
    "    # <<<--- END DIRECT INITIALIZATION --- >>>\n",
    "\n",
    "\n",
    "    logger.info(\"--- Stage 8: Initializing Seq2SeqTrainer ---\")\n",
    "    try:\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=train_args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            data_collator=collator,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics_fn,\n",
    "        )\n",
    "        logger.info(\"Seq2SeqTrainer Initialized.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Trainer init failed: {e}\", exc_info=True)\n",
    "        raise e\n",
    "\n",
    "\n",
    "    logger.info(\"--- Stage 9: Starting Model Training ---\")\n",
    "    try:\n",
    "        logger.info(f\"Starting training for {args.num_epochs} epochs...\")\n",
    "        train_res = trainer.train()\n",
    "        metrics = train_res.metrics\n",
    "        logger.info(\"--- Training Finished ---\")\n",
    "        logger.info(\"--- Training Metrics ---\")\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        logger.info(\"Trainer state saved.\")\n",
    "        best_path = str(output_dir / \"best_model\")\n",
    "        trainer.save_model(best_path)\n",
    "        logger.info(f\"Final best model saved to: {best_path}\")\n",
    "    except Exception as e: logger.error(f\"Training failed: {e}\", exc_info=True); raise\n",
    "\n",
    "    logger.info(\"--- Stage 10: Evaluating Model on Test Set ---\")\n",
    "    if test_ds:\n",
    "        try:\n",
    "            logger.info(\"Running final evaluation on the test set...\")\n",
    "            test_res = trainer.evaluate(eval_dataset=test_ds, metric_key_prefix=\"test\")\n",
    "            logger.info(\"--- Evaluation on Test Set Finished ---\")\n",
    "            logger.info(\"--- Test Set Results ---\")\n",
    "            trainer.log_metrics(\"test\", test_res)\n",
    "            trainer.save_metrics(\"test\", test_res)\n",
    "            metric_key = \"test_sequence_accuracy\"\n",
    "            if metric_key in test_res:\n",
    "                logger.info(f\"\\n*** Test Sequence Accuracy: {test_res[metric_key]:.4f} ***\\n\")\n",
    "            else:\n",
    "                logger.warning(f\"Metric '{metric_key}' not found in test results.\")\n",
    "                logger.info(f\"Available test metrics: {list(test_res.keys())}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Test evaluation failed: {e}\", exc_info=True)\n",
    "    else:\n",
    "        logger.warning(\"Test dataset unavailable or not loaded. Skipping final evaluation.\")\n",
    "    logger.info(\"--- Final Evaluation Stage Complete ---\")\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    total_time = end_time - start_time\n",
    "    logger.info(\"=\"*60)\n",
    "    try:\n",
    "        end_time_str = end_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    except Exception:\n",
    "        end_time_str = \"Time Unavailable\"\n",
    "    logger.info(f\" Script execution finished at: {end_time_str}\")\n",
    "    logger.info(f\" Total execution time: {total_time}\")\n",
    "    logger.info(\"=\"*60)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Execute Main Function (for Notebook context)\n",
    "# =============================================================================\n",
    "# In a notebook, we call main directly instead of using if __name__ == \"__main__\":\n",
    "try:\n",
    "    # Ensure CUDA check happens right before main call if needed\n",
    "    if not torch.cuda.is_available() and args.fp16:\n",
    "        logger.warning(\"MAIN (Notebook): CUDA not detected before starting main function!\")\n",
    "        logger.warning(\"MAIN (Notebook): Disabling fp16 as CUDA is not available.\")\n",
    "        args.fp16 = False\n",
    "\n",
    "    main(args) # Call the main function with the configured args\n",
    "\n",
    "except Exception as e:\n",
    "     logger.critical(f\"Unhandled exception terminated script execution: {e}\", exc_info=True)\n",
    "     # In a notebook, just logging might be sufficient, or re-raise if preferred\n",
    "     # raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1: Next-Generation Transformer Models for Symbolic Calculations of Squared Amplitudes in HEP\n",
    "Model: Transformer model with a contemporary innovation added such as KAN layers, reinforcement learning, genetic algorithms, specialized long-sequence attention, etc. which improves the performance compared to a basic transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 21:31:15 - INFO - [main] - ============================================================\n",
      "2025-04-22 21:31:15 - INFO - [main] -  Starting LongT5 Training/Evaluation for Symbolic HEP Calculation \n",
      "2025-04-22 21:31:15 - INFO - [main] - ============================================================\n",
      "2025-04-22 21:31:15 - INFO - [main] - Script execution started at: 2025-04-22 21:31:15\n",
      "2025-04-22 21:31:15 - INFO - [main] - CUDA available. Device: NVIDIA GeForce RTX 4090 Laptop GPU\n",
      "2025-04-22 21:31:15 - INFO - [main] - PyTorch CUDA version: 12.1\n",
      "2025-04-22 21:31:15 - INFO - [main] - Running with configuration:\n",
      "2025-04-22 21:31:15 - INFO - [main] -   train_file: qed_data_processed/qed_amplitudes_train.jsonl\n",
      "2025-04-22 21:31:15 - INFO - [main] -   val_file: qed_data_processed/qed_amplitudes_val.jsonl\n",
      "2025-04-22 21:31:15 - INFO - [main] -   test_file: qed_data_processed/qed_amplitudes_test.jsonl\n",
      "2025-04-22 21:31:15 - INFO - [main] -   output_dir: longt5_symbolic_hep_output\n",
      "2025-04-22 21:31:15 - INFO - [main] -   model_id: google/long-t5-tglobal-base\n",
      "2025-04-22 21:31:15 - INFO - [main] -   task_prefix: \n",
      "2025-04-22 21:31:15 - INFO - [main] -   tokenizer_legacy: False\n",
      "2025-04-22 21:31:15 - INFO - [main] -   max_seq_length: 2048\n",
      "2025-04-22 21:31:15 - INFO - [main] -   num_epochs: 5\n",
      "2025-04-22 21:31:15 - INFO - [main] -   train_batch_size: 4\n",
      "2025-04-22 21:31:15 - INFO - [main] -   eval_batch_size: 4\n",
      "2025-04-22 21:31:15 - INFO - [main] -   learning_rate: 5e-05\n",
      "2025-04-22 21:31:15 - INFO - [main] -   weight_decay: 0.01\n",
      "2025-04-22 21:31:15 - INFO - [main] -   warmup_steps: 500\n",
      "2025-04-22 21:31:15 - INFO - [main] -   logging_steps: 100\n",
      "2025-04-22 21:31:15 - INFO - [main] -   eval_strategy: epoch\n",
      "2025-04-22 21:31:15 - INFO - [main] -   eval_steps: None\n",
      "2025-04-22 21:31:15 - INFO - [main] -   save_strategy: epoch\n",
      "2025-04-22 21:31:15 - INFO - [main] -   save_steps: None\n",
      "2025-04-22 21:31:15 - INFO - [main] -   save_total_limit: 2\n",
      "2025-04-22 21:31:15 - INFO - [main] -   seed: 42\n",
      "2025-04-22 21:31:15 - INFO - [main] -   dataloader_num_workers: 0\n",
      "2025-04-22 21:31:15 - INFO - [main] -   fp16: True\n",
      "2025-04-22 21:31:15 - INFO - [main] -   gradient_checkpointing: True\n",
      "2025-04-22 21:31:15 - INFO - [main] -   label_smoothing: 0.1\n",
      "2025-04-22 21:31:15 - INFO - [main] -   num_beams: 4\n",
      "2025-04-22 21:31:15 - INFO - [main] -   report_to: tensorboard\n",
      "2025-04-22 21:31:15 - INFO - [main] - Output directory set to: longt5_symbolic_hep_output\n",
      "2025-04-22 21:31:15 - INFO - [main] - --- Stage 1: Loading Raw Data ---\n",
      "2025-04-22 21:31:15 - INFO - [load_jsonl] - Loading data from: qed_data_processed/qed_amplitudes_train.jsonl\n",
      "2025-04-22 21:31:15 - INFO - [load_jsonl] - Successfully loaded 12441 records from qed_data_processed/qed_amplitudes_train.jsonl.\n",
      "2025-04-22 21:31:15 - INFO - [load_jsonl] - Loading data from: qed_data_processed/qed_amplitudes_val.jsonl\n",
      "2025-04-22 21:31:15 - INFO - [load_jsonl] - Successfully loaded 1555 records from qed_data_processed/qed_amplitudes_val.jsonl.\n",
      "2025-04-22 21:31:15 - INFO - [load_jsonl] - Loading data from: qed_data_processed/qed_amplitudes_test.jsonl\n",
      "2025-04-22 21:31:15 - INFO - [load_jsonl] - Successfully loaded 1556 records from qed_data_processed/qed_amplitudes_test.jsonl.\n",
      "2025-04-22 21:31:15 - INFO - [main] - --- Raw Data Loading Complete ---\n",
      "2025-04-22 21:31:15 - INFO - [main] - --- Stage 2: Converting Tokens to Strings ---\n",
      "2025-04-22 21:31:15 - INFO - [convert_tokens_to_strings] - Attempting to convert 12441 items to strings...\n",
      "2025-04-22 21:31:15 - INFO - [convert_tokens_to_strings] - Successfully converted 12441 items to strings (skipped 0).\n",
      "2025-04-22 21:31:15 - INFO - [convert_tokens_to_strings] - Attempting to convert 1555 items to strings...\n",
      "2025-04-22 21:31:15 - INFO - [convert_tokens_to_strings] - Successfully converted 1555 items to strings (skipped 0).\n",
      "2025-04-22 21:31:15 - INFO - [convert_tokens_to_strings] - Attempting to convert 1556 items to strings...\n",
      "2025-04-22 21:31:15 - INFO - [convert_tokens_to_strings] - Successfully converted 1556 items to strings (skipped 0).\n",
      "2025-04-22 21:31:15 - INFO - [main] - --- Token-to-String Conversion Complete ---\n",
      "2025-04-22 21:31:15 - INFO - [main] - --- Stage 3: Initializing Tokenizer ---\n",
      "2025-04-22 21:31:15 - INFO - [main] - Tokenizer for google/long-t5-tglobal-base initialized (Fast: False, Vocab: 32000).\n",
      "2025-04-22 21:31:15 - INFO - [main] - --- Tokenizer Initialization Complete ---\n",
      "2025-04-22 21:31:15 - INFO - [main] - --- Stage 4: Encoding Data (Tokenization) ---\n",
      "2025-04-22 21:31:15 - INFO - [main] - Using Task Prefix: ''\n",
      "2025-04-22 21:31:15 - INFO - [main] - --> Encoding Training Data...\n",
      "2025-04-22 21:31:15 - INFO - [encode_sequences] - Starting encoding for 12441 pairs. MaxLen=2048. Prefix=''. (Batch size: 500)\n",
      "2025-04-22 21:31:15 - INFO - [encode_sequences] -   Encoding batch 1-500/12441...\n",
      "2025-04-22 21:31:16 - INFO - [encode_sequences] -   Encoding batch 501-1000/12441...\n",
      "2025-04-22 21:31:16 - INFO - [encode_sequences] -   Encoding batch 1001-1500/12441...\n",
      "2025-04-22 21:31:16 - INFO - [encode_sequences] -   Encoding batch 1501-2000/12441...\n",
      "2025-04-22 21:31:16 - INFO - [encode_sequences] -   Encoding batch 2001-2500/12441...\n",
      "2025-04-22 21:31:17 - INFO - [encode_sequences] -   Encoding batch 2501-3000/12441...\n",
      "2025-04-22 21:31:17 - INFO - [encode_sequences] -   Encoding batch 3001-3500/12441...\n",
      "2025-04-22 21:31:17 - INFO - [encode_sequences] -   Encoding batch 3501-4000/12441...\n",
      "2025-04-22 21:31:18 - INFO - [encode_sequences] -   Encoding batch 4001-4500/12441...\n",
      "2025-04-22 21:31:18 - INFO - [encode_sequences] -   Encoding batch 4501-5000/12441...\n",
      "2025-04-22 21:31:18 - INFO - [encode_sequences] -   Encoding batch 5001-5500/12441...\n",
      "2025-04-22 21:31:19 - INFO - [encode_sequences] -   Encoding batch 5501-6000/12441...\n",
      "2025-04-22 21:31:19 - INFO - [encode_sequences] -   Encoding batch 6001-6500/12441...\n",
      "2025-04-22 21:31:19 - INFO - [encode_sequences] -   Encoding batch 6501-7000/12441...\n",
      "2025-04-22 21:31:19 - INFO - [encode_sequences] -   Encoding batch 7001-7500/12441...\n",
      "2025-04-22 21:31:20 - INFO - [encode_sequences] -   Encoding batch 7501-8000/12441...\n",
      "2025-04-22 21:31:20 - INFO - [encode_sequences] -   Encoding batch 8001-8500/12441...\n",
      "2025-04-22 21:31:20 - INFO - [encode_sequences] -   Encoding batch 8501-9000/12441...\n",
      "2025-04-22 21:31:21 - INFO - [encode_sequences] -   Encoding batch 9001-9500/12441...\n",
      "2025-04-22 21:31:21 - INFO - [encode_sequences] -   Encoding batch 9501-10000/12441...\n",
      "2025-04-22 21:31:21 - INFO - [encode_sequences] -   Encoding batch 10001-10500/12441...\n",
      "2025-04-22 21:31:21 - INFO - [encode_sequences] -   Encoding batch 10501-11000/12441...\n",
      "2025-04-22 21:31:22 - INFO - [encode_sequences] -   Encoding batch 11001-11500/12441...\n",
      "2025-04-22 21:31:22 - INFO - [encode_sequences] -   Encoding batch 11501-12000/12441...\n",
      "2025-04-22 21:31:22 - INFO - [encode_sequences] -   Encoding batch 12001-12441/12441...\n",
      "2025-04-22 21:31:22 - INFO - [encode_sequences] - Finished encoding all 12441 sequence pairs.\n",
      "2025-04-22 21:31:22 - INFO - [main] - --> Encoding Validation Data...\n",
      "2025-04-22 21:31:22 - INFO - [encode_sequences] - Starting encoding for 1555 pairs. MaxLen=2048. Prefix=''. (Batch size: 500)\n",
      "2025-04-22 21:31:22 - INFO - [encode_sequences] -   Encoding batch 1-500/1555...\n",
      "2025-04-22 21:31:23 - INFO - [encode_sequences] -   Encoding batch 501-1000/1555...\n",
      "2025-04-22 21:31:23 - INFO - [encode_sequences] -   Encoding batch 1001-1500/1555...\n",
      "2025-04-22 21:31:23 - INFO - [encode_sequences] -   Encoding batch 1501-1555/1555...\n",
      "2025-04-22 21:31:23 - INFO - [encode_sequences] - Finished encoding all 1555 sequence pairs.\n",
      "2025-04-22 21:31:23 - INFO - [main] - --> Encoding Test Data...\n",
      "2025-04-22 21:31:23 - INFO - [encode_sequences] - Starting encoding for 1556 pairs. MaxLen=2048. Prefix=''. (Batch size: 500)\n",
      "2025-04-22 21:31:23 - INFO - [encode_sequences] -   Encoding batch 1-500/1556...\n",
      "2025-04-22 21:31:24 - INFO - [encode_sequences] -   Encoding batch 501-1000/1556...\n",
      "2025-04-22 21:31:24 - INFO - [encode_sequences] -   Encoding batch 1001-1500/1556...\n",
      "2025-04-22 21:31:24 - INFO - [encode_sequences] -   Encoding batch 1501-1556/1556...\n",
      "2025-04-22 21:31:24 - INFO - [encode_sequences] - Finished encoding all 1556 sequence pairs.\n",
      "2025-04-22 21:31:24 - INFO - [main] - --- Data Encoding Phase Complete ---\n",
      "2025-04-22 21:31:24 - INFO - [main] - --- Stage 5: Creating PyTorch Datasets ---\n",
      "2025-04-22 21:31:24 - INFO - [__init__] - Created SequenceDataset with 12441 examples.\n",
      "2025-04-22 21:31:24 - INFO - [__init__] - Created SequenceDataset with 1555 examples.\n",
      "2025-04-22 21:31:24 - INFO - [__init__] - Created SequenceDataset with 1556 examples.\n",
      "2025-04-22 21:31:24 - INFO - [main] - --- PyTorch Datasets Created Successfully ---\n",
      "2025-04-22 21:31:24 - INFO - [main] - --- Stage 6: Initializing Model ---\n",
      "2025-04-22 21:31:25 - INFO - [main] - Model google/long-t5-tglobal-base loaded.\n",
      "2025-04-22 21:31:25 - INFO - [main] - Est. Model Memory: 1.19 GB\n",
      "2025-04-22 21:31:25 - INFO - [main] - --- Model Initialization Complete ---\n",
      "2025-04-22 21:31:25 - INFO - [main] - --- Stage 7: Configuring Training Environment ---\n",
      "2025-04-22 21:31:25 - INFO - [main] - Attempting to enable Gradient Checkpointing...\n",
      "2025-04-22 21:31:25 - INFO - [main] - Gradient Checkpointing enabled via model method.\n",
      "2025-04-22 21:31:25 - INFO - [main] - Model GC state: True\n",
      "2025-04-22 21:31:25 - INFO - [main] - Initializing Data Collator...\n",
      "2025-04-22 21:31:25 - INFO - [main] - Data Collator initialized.\n",
      "2025-04-22 21:31:25 - INFO - [main] - Defining Training Arguments...\n",
      "2025-04-22 21:31:25 - INFO - [main] - Reporting to: tensorboard\n",
      "2025-04-22 21:31:25 - INFO - [main] - Successfully initialized Seq2SeqTrainingArguments.\n",
      "2025-04-22 21:31:25 - INFO - [main] - Effective FP16: True, Grad Checkpointing: True\n",
      "2025-04-22 21:31:25 - INFO - [main] - --- Training Environment Configuration Complete ---\n",
      "2025-04-22 21:31:25 - INFO - [main] - --- Stage 8: Initializing Seq2SeqTrainer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_140970/1604048880.py:521: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-22 21:31:25 - INFO - [main] - Seq2SeqTrainer Initialized.\n",
      "2025-04-22 21:31:25 - INFO - [main] - --- Stage 9: Starting Model Training ---\n",
      "2025-04-22 21:31:25 - INFO - [main] - Starting training for 5 epochs...\n",
      "2025-04-22 21:31:27 - ERROR - [main] - Training failed: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 67.25 MiB is free. Process 99334 has 12.93 GiB memory in use. Including non-PyTorch memory, this process has 2.69 GiB memory in use. Of the allocated memory 2.14 GiB is allocated by PyTorch, and 268.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_140970/1604048880.py\", line 539, in main\n",
      "    train_res = trainer.train()\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/trainer.py\", line 3736, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 814, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 802, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py\", line 2173, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py\", line 1515, in forward\n",
      "    layer_outputs = self._gradient_checkpointing_func(\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/_compile.py\", line 32, in inner\n",
      "    return disable_fn(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 632, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 489, in checkpoint\n",
      "    return CheckpointFunction.apply(function, preserve, *args)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/autograd/function.py\", line 575, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 264, in forward\n",
      "    outputs = run_function(*args)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py\", line 1192, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py\", line 1101, in forward\n",
      "    attention_output = self.TransientGlobalSelfAttention(\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py\", line 993, in forward\n",
      "    attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/functional.py\", line 2140, in softmax\n",
      "    ret = input.softmax(dim)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 67.25 MiB is free. Process 99334 has 12.93 GiB memory in use. Including non-PyTorch memory, this process has 2.69 GiB memory in use. Of the allocated memory 2.14 GiB is allocated by PyTorch, and 268.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "2025-04-22 21:31:27 - CRITICAL - [<module>] - Unhandled exception terminated script execution: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 67.25 MiB is free. Process 99334 has 12.93 GiB memory in use. Including non-PyTorch memory, this process has 2.69 GiB memory in use. Of the allocated memory 2.14 GiB is allocated by PyTorch, and 268.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_140970/1604048880.py\", line 595, in <module>\n",
      "    main(args)\n",
      "  File \"/tmp/ipykernel_140970/1604048880.py\", line 539, in main\n",
      "    train_res = trainer.train()\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/trainer.py\", line 3736, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 814, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 802, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py\", line 2173, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py\", line 1515, in forward\n",
      "    layer_outputs = self._gradient_checkpointing_func(\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/_compile.py\", line 32, in inner\n",
      "    return disable_fn(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 632, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 489, in checkpoint\n",
      "    return CheckpointFunction.apply(function, preserve, *args)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/autograd/function.py\", line 575, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 264, in forward\n",
      "    outputs = run_function(*args)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py\", line 1192, in forward\n",
      "    self_attention_outputs = self.layer[0](\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py\", line 1101, in forward\n",
      "    attention_output = self.TransientGlobalSelfAttention(\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/transformers/models/longt5/modeling_longt5.py\", line 993, in forward\n",
      "    attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "  File \"/home/nikitas/anaconda3/envs/t5_pip_test/lib/python3.10/site-packages/torch/nn/functional.py\", line 2140, in softmax\n",
      "    ret = input.softmax(dim)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 67.25 MiB is free. Process 99334 has 12.93 GiB memory in use. Including non-PyTorch memory, this process has 2.69 GiB memory in use. Of the allocated memory 2.14 GiB is allocated by PyTorch, and 268.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Task 3.1: End-to-end Training and Evaluation Script for a LongT5 Model\n",
    "on Symbolic Calculations of Squared Amplitudes in HEP.\n",
    "\n",
    "This script adapts the previous T5 framework to use LongT5, a Transformer\n",
    "model with improved long-sequence handling capabilities (Transient Global Attention),\n",
    "as the \"contemporary innovation\".\n",
    "\n",
    "*** NOTE: This version is adapted for robust execution ***\n",
    "*** within a Jupyter Notebook (.ipynb). Configuration is handled    ***\n",
    "*** by manually setting attributes on the `args` object.            ***\n",
    "\n",
    "Key Changes from Basic T5 Script:\n",
    "- Model: Uses LongT5ForConditionalGeneration.\n",
    "- Tokenizer: Still uses T5Tokenizer (compatible with LongT5 checkpoints).\n",
    "- Checkpoint: Uses a pre-trained LongT5 checkpoint ('google/long-t5-tglobal-base').\n",
    "- Max Sequence Length: Increased significantly to leverage long-sequence capability.\n",
    "- Batch Size: Potentially reduced due to higher memory usage from longer sequences.\n",
    "- Output Directory: Changed to reflect the new model.\n",
    "- Comments/Logging: Updated for the new task and model.\n",
    "- Diagnostics: Removed path/signature checks for clarity.\n",
    "\n",
    "Dependencies:\n",
    "- Working Python 3.10 environment (e.g., t5_pip_test) with necessary packages\n",
    "  (transformers, accelerate, torch, datasets, sentencepiece, etc.).\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime # Use the standard datetime module\n",
    "import logging\n",
    "import argparse # Still used for the Namespace object, but not for parsing\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "import inspect # Keep inspect if needed by other parts, otherwise optional now\n",
    "\n",
    "# Import the specific classes needed\n",
    "# NOTE: LongT5 often uses the standard T5Tokenizer\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    LongT5ForConditionalGeneration, # <<< CHANGED Model Class\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainingArguments # Keep for reference if needed\n",
    ")\n",
    "from transformers.trainer_utils import set_seed # Use transformers' set_seed\n",
    "\n",
    "# Configure basic logging (Set level to DEBUG for detailed output, INFO for progress)\n",
    "# Check if logger already exists (useful in notebooks where cells might be re-run)\n",
    "logger = logging.getLogger(__name__)\n",
    "if not logger.hasHandlers():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, # INFO shows progress, DEBUG shows fine details\n",
    "        format=\"%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        # Use stream=sys.stdout to ensure output appears in notebook cell\n",
    "        stream=sys.stdout\n",
    "    )\n",
    "else:\n",
    "    # Ensure level is still INFO if logger was configured previously\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# <<< CONFIGURATION >>>\n",
    "# =============================================================================\n",
    "# --- EDIT THESE VALUES TO CONFIGURE THE RUN ---\n",
    "args = argparse.Namespace(\n",
    "    # --- File Paths ---\n",
    "    # !!! Update these if your HEP dataset has different names/locations !!!\n",
    "    train_file='qed_data_processed/qed_amplitudes_train.jsonl',\n",
    "    val_file='qed_data_processed/qed_amplitudes_val.jsonl',\n",
    "    test_file='qed_data_processed/qed_amplitudes_test.jsonl',\n",
    "    output_dir='longt5_symbolic_hep_output', # <<< CHANGED Output Dir\n",
    "\n",
    "    # --- Model Configuration ---\n",
    "    # <<< CHANGED Model Checkpoint to a LongT5 variant >>>\n",
    "    # (Other options: 'google/long-t5-local-base', 'google/long-t5-tglobal-large', etc.)\n",
    "    model_id=\"google/long-t5-tglobal-base\",\n",
    "    task_prefix=\"\", # Add task prefix if needed e.g. \"calculate squared amplitude: \"\n",
    "    tokenizer_legacy=False, # Usually False for newer models\n",
    "\n",
    "    # --- Tokenizer and Data Processing ---\n",
    "    # <<< INCREASED Max Sequence Length >>> (Adjust based on your data & GPU memory)\n",
    "    max_seq_length=2048, # e.g., 1024, 2048, 4096, up to 16k for some variants\n",
    "\n",
    "    # --- Training Hyperparameters ---\n",
    "    # !!! These likely need significant tuning for LongT5 and longer sequences !!!\n",
    "    num_epochs=5,           # May need more or fewer epochs\n",
    "    # <<< REDUCED Batch Size due to longer sequences / potentially larger model >>>\n",
    "    train_batch_size=4,     # START SMALL! Monitor GPU memory. Adjust as needed.\n",
    "    eval_batch_size=4,      # START SMALL!\n",
    "    learning_rate=5e-5,     # Common starting point, adjust based on results\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,       # May need adjustment based on dataset size/LR schedule\n",
    "    logging_steps=100,      # Log more often initially?\n",
    "    eval_strategy=\"epoch\",  # Correct parameter name\n",
    "    eval_steps=None,        # Not used when eval_strategy is 'epoch'\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=None,        # Not used when save_strategy is 'epoch'\n",
    "    save_total_limit=2,     # Keep latest 2 checkpoints\n",
    "    seed=42,\n",
    "    dataloader_num_workers=0, # Safest for notebooks\n",
    "\n",
    "    # --- Advanced Training Features ---\n",
    "    fp16=torch.cuda.is_available(), # Use mixed precision if possible\n",
    "    gradient_checkpointing=True,    # HIGHLY Recommended for long sequences/large models\n",
    "    label_smoothing=0.1,\n",
    "\n",
    "    # --- Generation Configuration (for evaluation) ---\n",
    "    num_beams=4,\n",
    "\n",
    "    # --- Reporting ---\n",
    "    report_to=\"tensorboard\", # Or \"none\", \"wandb\", etc.\n",
    ")\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "# (Helper Functions: load_jsonl, convert_tokens_to_strings, encode_sequences, SequenceDataset, compute_metrics_fn)\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads a JSON Lines (.jsonl) file, skipping blank/whitespace-only lines.\"\"\"\n",
    "    data = []\n",
    "    file_path = Path(file_path)\n",
    "    logger.info(f\"Loading data from: {file_path}\")\n",
    "    if not file_path.is_file():\n",
    "        logger.error(f\"Data file not found: {file_path}\")\n",
    "        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        logger.warning(f\"Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "                        continue\n",
    "        logger.info(f\"Successfully loaded {len(data)} records from {file_path}.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load data from {file_path}: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "def convert_tokens_to_strings(raw_data_list):\n",
    "    \"\"\"Converts lists of tokens into single whitespace-joined strings.\"\"\"\n",
    "    input_strings = []\n",
    "    target_strings = []\n",
    "    if not raw_data_list:\n",
    "        logger.warning(\"Input data list is empty for token-to-string conversion.\")\n",
    "        return input_strings, target_strings\n",
    "\n",
    "    skipped_count = 0\n",
    "    logger.info(f\"Attempting to convert {len(raw_data_list)} items to strings...\")\n",
    "    for i, item in enumerate(raw_data_list):\n",
    "        # !!! Adapt these keys if your HEP dataset uses different names !!!\n",
    "        input_toks = item.get('input_tokens')  # Or e.g., 'process_tokens'\n",
    "        target_toks = item.get('target_tokens') # Or e.g., 'amplitude_tokens'\n",
    "\n",
    "        if isinstance(input_toks, list) and isinstance(target_toks, list):\n",
    "            try:\n",
    "                str_input_toks = [str(tok) for tok in input_toks]\n",
    "                str_target_toks = [str(tok) for tok in target_toks]\n",
    "                joined_input = \" \".join(str_input_toks)\n",
    "                joined_target = \" \".join(str_target_toks)\n",
    "                input_strings.append(joined_input)\n",
    "                target_strings.append(joined_target)\n",
    "            except Exception as e:\n",
    "                 logger.warning(f\"Error joining tokens for item at index {i}: {e}. Item: {item}\", exc_info=True)\n",
    "                 skipped_count += 1\n",
    "        else:\n",
    "            missing_keys = []\n",
    "            # !!! Update keys checked if needed !!!\n",
    "            if 'input_tokens' not in item: missing_keys.append('input_tokens')\n",
    "            if 'target_tokens' not in item: missing_keys.append('target_tokens')\n",
    "            invalid_types = []\n",
    "            if 'input_tokens' in item and not isinstance(input_toks, list): invalid_types.append(f\"input_tokens (type: {type(input_toks).__name__})\")\n",
    "            if 'target_tokens' in item and not isinstance(target_toks, list): invalid_types.append(f\"target_tokens (type: {type(target_toks).__name__})\")\n",
    "            log_msg = f\"Skipping item at index {i}.\"\n",
    "            if missing_keys: log_msg += f\" Missing keys: {missing_keys}.\"\n",
    "            if invalid_types: log_msg += f\" Keys with non-list values: {invalid_types}.\"\n",
    "            item_str = str(item)\n",
    "            if len(item_str) > 200: item_str = item_str[:200] + \"...\"\n",
    "            log_msg += f\" Item (truncated): {item_str}\"\n",
    "            logger.warning(log_msg)\n",
    "            skipped_count += 1\n",
    "\n",
    "    logger.info(f\"Successfully converted {len(input_strings)} items to strings (skipped {skipped_count}).\")\n",
    "    if not input_strings and skipped_count > 0 and skipped_count == len(raw_data_list):\n",
    "         logger.error(\"All items were skipped during token-to-string conversion.\")\n",
    "         raise ValueError(\"Failed to convert any items to strings.\")\n",
    "    elif not input_strings and len(raw_data_list) > 0 and skipped_count != len(raw_data_list):\n",
    "        logger.error(f\"Conversion resulted in an empty list of strings, but {len(raw_data_list)} items were processed ({skipped_count} skipped).\")\n",
    "        raise ValueError(\"String conversion unexpectedly produced no output despite valid input items existing.\")\n",
    "    elif not input_strings and not raw_data_list:\n",
    "        logger.warning(\"Input data list was empty, resulting in empty string lists.\")\n",
    "    return input_strings, target_strings\n",
    "\n",
    "\n",
    "def encode_sequences(tokenizer, input_strings, target_strings, max_len, task_prefix=\"\", batch_size=500):\n",
    "    \"\"\"Tokenizes input and target string pairs.\"\"\"\n",
    "    logger.info(f\"Starting encoding for {len(input_strings)} pairs. MaxLen={max_len}. Prefix='{task_prefix}'. (Batch size: {batch_size})\")\n",
    "\n",
    "    if not isinstance(input_strings, list) or not isinstance(target_strings, list):\n",
    "        raise TypeError(\"input_strings and target_strings must be lists.\")\n",
    "    if not input_strings or not target_strings:\n",
    "        logger.warning(\"Received empty list(s) for encoding. Returning empty.\")\n",
    "        return {'input_ids': [], 'attention_mask': [], 'labels': []}\n",
    "    if len(input_strings) != len(target_strings):\n",
    "        raise ValueError(f\"Input ({len(input_strings)}) and Target ({len(target_strings)}) string lists must have the same length.\")\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_labels = []\n",
    "    num_sequences = len(input_strings)\n",
    "\n",
    "    for i in range(0, num_sequences, batch_size):\n",
    "        batch_start = i\n",
    "        batch_end = min(i + batch_size, num_sequences)\n",
    "        logger.info(f\"  Encoding batch {batch_start+1}-{batch_end}/{num_sequences}...\")\n",
    "\n",
    "        input_batch = input_strings[batch_start:batch_end]\n",
    "        target_batch = target_strings[batch_start:batch_end]\n",
    "\n",
    "        # Encode Inputs\n",
    "        try:\n",
    "            processed_input_batch = [f\"{task_prefix}{s}\" if task_prefix else s for s in input_batch]\n",
    "            encoder_outputs = tokenizer(\n",
    "                processed_input_batch, max_length=max_len, padding='max_length', truncation=True, return_tensors=None\n",
    "            )\n",
    "            batch_input_ids = encoder_outputs['input_ids']\n",
    "            batch_attention_mask = encoder_outputs['attention_mask']\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Tokenizer CRASHED on ENCODER batch: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "        # Encode Targets (Labels)\n",
    "        try:\n",
    "            decoder_outputs = tokenizer(\n",
    "                text_target=target_batch, max_length=max_len, padding='max_length', truncation=True, return_tensors=None\n",
    "            )\n",
    "            batch_labels = decoder_outputs['input_ids']\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Tokenizer CRASHED on DECODER batch: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "        if len(batch_input_ids) != len(input_batch) or len(batch_attention_mask) != len(input_batch) or len(batch_labels) != len(target_batch):\n",
    "            raise ValueError(f\"Batch encoding length mismatch! Input={len(batch_input_ids)}, Attn={len(batch_attention_mask)}, Labels={len(batch_labels)}, Expected={len(input_batch)}\")\n",
    "\n",
    "        all_input_ids.extend(batch_input_ids)\n",
    "        all_attention_mask.extend(batch_attention_mask)\n",
    "        all_labels.extend(batch_labels)\n",
    "\n",
    "    logger.info(f\"Finished encoding all {len(all_input_ids)} sequence pairs.\")\n",
    "    if len(all_input_ids) != num_sequences:\n",
    "         raise ValueError(f\"Encoded sequence count ({len(all_input_ids)}) mismatch original ({num_sequences}).\")\n",
    "\n",
    "    return {'input_ids': all_input_ids, 'attention_mask': all_attention_mask, 'labels': all_labels}\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset wrapper for tokenized sequence data.\"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        required_keys = ['input_ids', 'attention_mask', 'labels']\n",
    "        if not isinstance(encodings, dict) or not all(key in encodings for key in required_keys):\n",
    "            logger.error(f\"Invalid encodings passed to SequenceDataset. Got Keys: {list(encodings.keys()) if isinstance(encodings, dict) else type(encodings)}\")\n",
    "            raise ValueError(f\"Encodings must be a dict with keys: {required_keys}.\")\n",
    "        try:\n",
    "            lengths = {key: len(val) for key, val in encodings.items() if key in required_keys and isinstance(val, list)}\n",
    "            if len(lengths) != len(required_keys):\n",
    "                 missing_or_wrong_type = [k for k in required_keys if k not in lengths]\n",
    "                 raise ValueError(f\"Missing or non-list required keys in encodings: {missing_or_wrong_type}\")\n",
    "            if len(set(lengths.values())) > 1:\n",
    "                 raise ValueError(f\"Inconsistent lengths in encodings: {lengths}\")\n",
    "            self.length = lengths.get('input_ids', 0)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed validating encoding structure/lengths: {e}\") from e\n",
    "\n",
    "        self.encodings = {k: v for k, v in encodings.items() if k in required_keys}\n",
    "        if self.length == 0: logger.warning(\"Initializing SequenceDataset with length 0.\")\n",
    "        logger.info(f\"Created SequenceDataset with {self.length} examples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not 0 <= idx < self.length: raise IndexError(f\"Index {idx} out of bounds ({self.length}).\")\n",
    "        try:\n",
    "            item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "            if 'labels' not in item: raise KeyError(f\"'labels' key missing for index {idx}\")\n",
    "            return item\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed creating item at index {idx}: {e}\", exc_info=True)\n",
    "            raise IndexError(f\"Error retrieving item at index {idx}: {e}\") from e\n",
    "\n",
    "\n",
    "tokenizer_for_metrics = None # Global tokenizer for compute_metrics\n",
    "\n",
    "def compute_metrics_fn(eval_pred):\n",
    "    \"\"\"Calculates exact match accuracy between decoded predictions and labels.\"\"\"\n",
    "    # !!! Consider adding more sophisticated metrics for symbolic math if needed !!!\n",
    "    # E.g., SymPy based comparison, tree edit distance, etc. Accuracy might be too strict.\n",
    "    global tokenizer_for_metrics\n",
    "    if tokenizer_for_metrics is None:\n",
    "        logger.error(\"Tokenizer missing in compute_metrics!\")\n",
    "        return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple): predictions = predictions[0]\n",
    "    if predictions.ndim == 3 and predictions.shape[-1] > 1:\n",
    "         predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    if not isinstance(predictions, np.ndarray): predictions = np.array(predictions)\n",
    "    if not isinstance(labels, np.ndarray): labels = np.array(labels)\n",
    "\n",
    "    labels = labels.astype(np.int64)\n",
    "    pad_token_id = tokenizer_for_metrics.pad_token_id\n",
    "    labels = np.where(labels != -100, labels, pad_token_id)\n",
    "\n",
    "    try:\n",
    "        decoded_preds = tokenizer_for_metrics.batch_decode(predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        decoded_labels = tokenizer_for_metrics.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Decode failed in compute_metrics: {e}\", exc_info=True)\n",
    "        return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    if not decoded_preds or not decoded_labels or len(decoded_preds) != len(decoded_labels):\n",
    "        logger.warning(f\"Metrics decode issue: Preds empty={not decoded_preds}, Labels empty={not decoded_labels}, LenPred={len(decoded_preds)}, LenLabel={len(decoded_labels)}\")\n",
    "        return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "    matches = [pred == label for pred, label in zip(decoded_preds, decoded_labels)]\n",
    "    accuracy = np.mean(matches) if matches else 0.0\n",
    "\n",
    "    logger.info(f\"Computed sequence accuracy: {accuracy:.4f}\")\n",
    "    # Consider adding other metrics here later\n",
    "    return {\"sequence_accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main Execution Logic\n",
    "# =============================================================================\n",
    "\n",
    "def main(config_args):\n",
    "    \"\"\"Orchestrates the LongT5 training and evaluation for symbolic HEP.\"\"\"\n",
    "    args = config_args # Use the config object passed as argument\n",
    "\n",
    "    # --- Basic Setup ---\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\" Starting LongT5 Training/Evaluation for Symbolic HEP Calculation \")\n",
    "    logger.info(\"=\"*60)\n",
    "    start_time = datetime.datetime.now()\n",
    "    logger.info(f\"Script execution started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"CUDA available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "        logger.info(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        logger.warning(\"CUDA not available. Running on CPU.\")\n",
    "        if args.fp16:\n",
    "             logger.warning(\"fp16=True ignored because CUDA is not available.\")\n",
    "             args.fp16 = False # Override config if no GPU\n",
    "\n",
    "    logger.info(f\"Running with configuration:\")\n",
    "    for k, v in vars(args).items(): logger.info(f\"  {k}: {v}\")\n",
    "\n",
    "    set_seed(args.seed)\n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"Output directory set to: {output_dir}\")\n",
    "\n",
    "    # --- Data Loading and Preprocessing ---\n",
    "    logger.info(\"--- Stage 1: Loading Raw Data ---\")\n",
    "    train_raw = load_jsonl(args.train_file)\n",
    "    val_raw = load_jsonl(args.val_file)\n",
    "    test_raw = load_jsonl(args.test_file)\n",
    "    if not train_raw or not val_raw:\n",
    "        raise ValueError(\"Training and/or validation raw datasets empty.\")\n",
    "    logger.info(\"--- Raw Data Loading Complete ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 2: Converting Tokens to Strings ---\")\n",
    "    train_in, train_tgt = convert_tokens_to_strings(train_raw)\n",
    "    val_in, val_tgt = convert_tokens_to_strings(val_raw)\n",
    "    test_in, test_tgt = convert_tokens_to_strings(test_raw)\n",
    "    if not train_in or not val_in:\n",
    "        raise ValueError(\"Training and/or validation datasets empty after string conversion.\")\n",
    "    logger.info(\"--- Token-to-String Conversion Complete ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 3: Initializing Tokenizer ---\")\n",
    "    try:\n",
    "        # Use T5Tokenizer, often compatible with LongT5 checkpoints\n",
    "        tokenizer = T5Tokenizer.from_pretrained(args.model_id, legacy=args.tokenizer_legacy)\n",
    "        global tokenizer_for_metrics; tokenizer_for_metrics = tokenizer\n",
    "        logger.info(f\"Tokenizer for {args.model_id} initialized (Fast: {tokenizer.is_fast}, Vocab: {tokenizer.vocab_size}).\")\n",
    "    except Exception as e: logger.error(f\"Failed initializing tokenizer: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- Tokenizer Initialization Complete ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 4: Encoding Data (Tokenization) ---\")\n",
    "    task_prefix = args.task_prefix or \"\"\n",
    "    logger.info(f\"Using Task Prefix: '{task_prefix}'\")\n",
    "    try:\n",
    "        logger.info(\"--> Encoding Training Data...\")\n",
    "        train_enc = encode_sequences(tokenizer, train_in, train_tgt, args.max_seq_length, task_prefix)\n",
    "        if not train_enc.get('input_ids'): raise ValueError(\"Training encoding failed (returned empty).\")\n",
    "\n",
    "        logger.info(\"--> Encoding Validation Data...\")\n",
    "        val_enc = encode_sequences(tokenizer, val_in, val_tgt, args.max_seq_length, task_prefix)\n",
    "        if not val_enc.get('input_ids'): raise ValueError(\"Validation encoding failed (returned empty).\")\n",
    "\n",
    "        test_enc = None\n",
    "        if test_in and test_tgt:\n",
    "            logger.info(\"--> Encoding Test Data...\")\n",
    "            test_enc = encode_sequences(tokenizer, test_in, test_tgt, args.max_seq_length, task_prefix)\n",
    "            if not test_enc.get('input_ids'): logger.warning(\"Test encoding returned empty.\")\n",
    "        else:\n",
    "             logger.info(\"Test input/target strings empty, skipping test encoding.\")\n",
    "\n",
    "    except Exception as e: logger.error(f\"Data encoding failed: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- Data Encoding Phase Complete ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 5: Creating PyTorch Datasets ---\")\n",
    "    try:\n",
    "        train_ds = SequenceDataset(train_enc)\n",
    "        val_ds = SequenceDataset(val_enc)\n",
    "        test_ds = SequenceDataset(test_enc) if test_enc and test_enc.get('input_ids') else None\n",
    "    except Exception as e: logger.error(f\"Dataset creation failed: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- PyTorch Datasets Created Successfully ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 6: Initializing Model ---\")\n",
    "    try:\n",
    "        # <<< CHANGED to LongT5 Model >>>\n",
    "        model = LongT5ForConditionalGeneration.from_pretrained(args.model_id)\n",
    "        logger.info(f\"Model {args.model_id} loaded.\")\n",
    "        if torch.cuda.is_available(): logger.info(f\"Est. Model Memory: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "    except Exception as e: logger.error(f\"Failed initializing model: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- Model Initialization Complete ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 7: Configuring Training Environment ---\")\n",
    "    use_gc = args.gradient_checkpointing\n",
    "    if use_gc: logger.info(\"Attempting to enable Gradient Checkpointing...\")\n",
    "    try:\n",
    "        if use_gc and hasattr(model, 'gradient_checkpointing_enable'):\n",
    "             model.gradient_checkpointing_enable(); logger.info(\"Gradient Checkpointing enabled via model method.\")\n",
    "             if hasattr(model, 'is_gradient_checkpointing'): logger.info(f\"Model GC state: {model.is_gradient_checkpointing}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed enabling Gradient Checkpointing on model: {e}. Relying on TrainingArguments setting.\", exc_info=True)\n",
    "        use_gc = args.gradient_checkpointing # Keep desired value\n",
    "\n",
    "    use_fp16 = args.fp16 # Already checked against cuda availability\n",
    "    logger.info(\"Initializing Data Collator...\")\n",
    "    try:\n",
    "        collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100, pad_to_multiple_of=8 if use_fp16 else None)\n",
    "        logger.info(\"Data Collator initialized.\")\n",
    "    except Exception as e: logger.error(f\"Collator init failed: {e}\", exc_info=True); raise\n",
    "\n",
    "\n",
    "    # --- Initialize Training Arguments (Corrected Parameter Name) ---\n",
    "    logger.info(\"Defining Training Arguments...\")\n",
    "    report_to = args.report_to.lower() if isinstance(args.report_to, str) else args.report_to\n",
    "    if report_to == \"none\": logger.info(\"Reporting disabled.\")\n",
    "    else: logger.info(f\"Reporting to: {report_to}\")\n",
    "\n",
    "    train_args = None # Initialize\n",
    "    try:\n",
    "        train_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=str(output_dir),\n",
    "            # Core Training Params\n",
    "            num_train_epochs=args.num_epochs,\n",
    "            per_device_train_batch_size=args.train_batch_size,\n",
    "            per_device_eval_batch_size=args.eval_batch_size,\n",
    "            learning_rate=args.learning_rate,\n",
    "            weight_decay=args.weight_decay,\n",
    "            warmup_steps=args.warmup_steps,\n",
    "            seed=args.seed,\n",
    "            # Evaluation and Saving Strategy (Corrected Name)\n",
    "            eval_strategy=args.eval_strategy,\n",
    "            eval_steps=args.eval_steps if args.eval_strategy == \"steps\" else None,\n",
    "            save_strategy=args.save_strategy,\n",
    "            save_steps=args.save_steps if args.save_strategy == \"steps\" else None,\n",
    "            save_total_limit=args.save_total_limit,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"sequence_accuracy\", # Make sure this matches compute_metrics output\n",
    "            greater_is_better=True,\n",
    "            # Logging\n",
    "            logging_dir=str(output_dir / 'logs'),\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=args.logging_steps,\n",
    "            report_to=report_to,\n",
    "            # Performance / Hardware\n",
    "            fp16=use_fp16,\n",
    "            gradient_checkpointing=use_gc, # Use value potentially modified if model enable failed\n",
    "            dataloader_num_workers=args.dataloader_num_workers,\n",
    "            # Seq2Seq Specific\n",
    "            predict_with_generate=True,\n",
    "            # Generation params used during evaluation AND testing with predict_with_generate=True\n",
    "            generation_max_length=args.max_seq_length, # Control output length during eval/predict\n",
    "            generation_num_beams=args.num_beams,\n",
    "            label_smoothing_factor=args.label_smoothing,\n",
    "        )\n",
    "        logger.info(\"Successfully initialized Seq2SeqTrainingArguments.\")\n",
    "        logger.info(f\"Effective FP16: {train_args.fp16}, Grad Checkpointing: {train_args.gradient_checkpointing}\")\n",
    "\n",
    "    except Exception as e: # Catch any exception during init\n",
    "        logger.error(f\"FAILED to initialize Seq2SeqTrainingArguments: {e}\", exc_info=True)\n",
    "        raise e # Re-raise to stop execution\n",
    "\n",
    "    if train_args is None:\n",
    "        raise RuntimeError(\"train_args was not successfully defined.\")\n",
    "\n",
    "    logger.info(\"--- Training Environment Configuration Complete ---\")\n",
    "\n",
    "    # --- Initialize Trainer ---\n",
    "    logger.info(\"--- Stage 8: Initializing Seq2SeqTrainer ---\")\n",
    "    try:\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=train_args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            data_collator=collator,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics_fn,\n",
    "        )\n",
    "        logger.info(\"Seq2SeqTrainer Initialized.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Trainer init failed: {e}\", exc_info=True)\n",
    "        raise e\n",
    "\n",
    "    # --- Training ---\n",
    "    logger.info(\"--- Stage 9: Starting Model Training ---\")\n",
    "    try:\n",
    "        logger.info(f\"Starting training for {args.num_epochs} epochs...\")\n",
    "        train_res = trainer.train()\n",
    "        metrics = train_res.metrics\n",
    "        logger.info(\"--- Training Finished ---\")\n",
    "        logger.info(\"--- Training Metrics ---\")\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        logger.info(\"Trainer state saved.\")\n",
    "        best_path = str(output_dir / \"best_model\")\n",
    "        trainer.save_model(best_path)\n",
    "        logger.info(f\"Final best model saved to: {best_path}\")\n",
    "    except Exception as e: logger.error(f\"Training failed: {e}\", exc_info=True); raise\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    logger.info(\"--- Stage 10: Evaluating Model on Test Set ---\")\n",
    "    if test_ds:\n",
    "        try:\n",
    "            logger.info(\"Running final evaluation on the test set...\")\n",
    "            test_res = trainer.evaluate(eval_dataset=test_ds, metric_key_prefix=\"test\")\n",
    "            logger.info(\"--- Evaluation on Test Set Finished ---\")\n",
    "            logger.info(\"--- Test Set Results ---\")\n",
    "            trainer.log_metrics(\"test\", test_res)\n",
    "            trainer.save_metrics(\"test\", test_res)\n",
    "            metric_key = \"test_sequence_accuracy\"\n",
    "            if metric_key in test_res:\n",
    "                logger.info(f\"\\n*** Test Sequence Accuracy: {test_res[metric_key]:.4f} ***\\n\")\n",
    "            else:\n",
    "                logger.warning(f\"Metric '{metric_key}' not found in test results.\")\n",
    "                logger.info(f\"Available test metrics: {list(test_res.keys())}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Test evaluation failed: {e}\", exc_info=True)\n",
    "    else:\n",
    "        logger.warning(\"Test dataset unavailable or not loaded. Skipping final evaluation.\")\n",
    "    logger.info(\"--- Final Evaluation Stage Complete ---\")\n",
    "\n",
    "    # --- End Script ---\n",
    "    end_time = datetime.datetime.now()\n",
    "    total_time = end_time - start_time\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\" Script execution finished successfully at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    logger.info(f\" Total execution time: {total_time}\")\n",
    "    logger.info(\"=\"*60)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Execute Main Function (for Notebook context)\n",
    "# =============================================================================\n",
    "# In a notebook, we call main directly instead of using if __name__ == \"__main__\":\n",
    "try:\n",
    "    # Ensure CUDA check happens right before main call if needed\n",
    "    if not torch.cuda.is_available() and args.fp16:\n",
    "        logger.warning(\"MAIN (Notebook): CUDA not detected before starting main function!\")\n",
    "        logger.warning(\"MAIN (Notebook): Disabling fp16 as CUDA is not available.\")\n",
    "        args.fp16 = False\n",
    "\n",
    "    # Call the main function with the configured args object\n",
    "    main(args)\n",
    "\n",
    "except Exception as e:\n",
    "     logger.critical(f\"Unhandled exception terminated script execution: {e}\", exc_info=True)\n",
    "     # In a notebook, just logging might be sufficient, or re-raise if preferred\n",
    "     # raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2: State-space Models for Squared Amplitude Calculation in High-Energy Physics\n",
    "Model: State-space model such as mamba or other model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] 'mamba_ssm' library not found. Please install it (`pip install mamba_ssm causal-conv1d>=1.1.0`) or replace StateSpaceLayer with your implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting Custom SSM Sequence-to-Sequence Script...\n",
      "[INFO] Current date/time (UTC): 2025-04-08 17:34:48 UTC\n",
      "[INFO] Using CPU\n",
      "[INFO] Initializing Tokenizer: bert-base-uncased\n",
      "[INFO] Tokenizer Vocab Size: 30522, Pad Token ID: 0\n",
      "[INFO] Loading data from: qed_expressions_train.jsonl\n",
      "[INFO] Successfully loaded 12441 records.\n",
      "[INFO] Loading data from: qed_expressions_val.jsonl\n",
      "[INFO] Successfully loaded 1555 records.\n",
      "[INFO] Loading data from: qed_expressions_test.jsonl\n",
      "[INFO] Successfully loaded 1556 records.\n",
      "[INFO] Converted 12441 items to strings (skipped 0).\n",
      "[INFO] Converted 1555 items to strings (skipped 0).\n",
      "[INFO] Converted 1556 items to strings (skipped 0).\n",
      "[INFO] Pre-tokenizing dataset with max_length=256...\n",
      "[INFO] Pre-tokenization complete. Dataset size: 12441 examples.\n",
      "[INFO] Pre-tokenizing dataset with max_length=256...\n",
      "[INFO] Pre-tokenization complete. Dataset size: 1555 examples.\n",
      "[INFO] Pre-tokenizing dataset with max_length=256...\n",
      "[INFO] Pre-tokenization complete. Dataset size: 1556 examples.\n",
      "[INFO] DataLoaders created.\n",
      "[INFO] Initializing SSMEncoderDecoder model...\n",
      "[Warning] Using Dummy StateSpaceLayer as 'mamba_ssm' not found.\n",
      "[Warning] Using Dummy StateSpaceLayer as 'mamba_ssm' not found.\n",
      "[Warning] Using Dummy StateSpaceLayer as 'mamba_ssm' not found.\n",
      "[Warning] Using Dummy StateSpaceLayer as 'mamba_ssm' not found.\n",
      "[Warning] Using Dummy StateSpaceLayer as 'mamba_ssm' not found.\n",
      "[Warning] Using Dummy StateSpaceLayer as 'mamba_ssm' not found.\n",
      "[INFO] Initialized SSMEncoderDecoder model:\n",
      "  - Vocab Size: 30522\n",
      "  - Embedding Dim (d_model): 512\n",
      "  - Max Sequence Length: 256\n",
      "  - Encoder SSM Layers: 6\n",
      "  - SSM Kwargs: {'d_state': 16, 'd_conv': 4, 'expand': 2}\n",
      "  - Pad Token ID: 0\n",
      "[INFO] Optimizer and Learning Rate Scheduler initialized.\n",
      "[INFO] Starting training for 5 epochs...\n",
      "\n",
      "--- Epoch 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/778 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                         \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 596\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[INFO] Script finished successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;66;03m# Optional: Add argument parsing (argparse) here for flexibility\u001b[39;00m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;66;03m# Optional: Set random seeds for reproducibility\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;66;03m# import random; random.seed(SEED)\u001b[39;00m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 478\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    474\u001b[0m train_pbar \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Training\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_pbar):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# Move batch to device\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m    479\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# May be unused by model but good practice\u001b[39;00m\n\u001b[1;32m    480\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Task 3.2: End-to-end Training and Evaluation Script for a Mamba SSM\n",
    "for Symbolic Calculations of Squared Amplitudes in HEP.\n",
    "\n",
    "This script adapts the previous framework to use a Mamba model,\n",
    "a type of State-Space Model (SSM), integrated within the Hugging Face ecosystem.\n",
    "\n",
    "*** NOTE: This version is adapted for robust execution ***\n",
    "*** within a Jupyter Notebook (.ipynb). Configuration is handled    ***\n",
    "*** by manually setting attributes on the `args` object.            ***\n",
    "\n",
    "Key Changes from T5/LongT5 Script:\n",
    "- Model: Uses MambaForCausalLM.\n",
    "- Tokenizer: Uses a tokenizer compatible with the Mamba checkpoint (e.g., GPTNeoXTokenizer).\n",
    "- Checkpoint: Uses a pre-trained Mamba checkpoint (e.g., 'state-spaces/mamba-130m').\n",
    "- Data Formatting: Input and Target are concatenated into a single sequence for Causal LM training.\n",
    "- Data Collator: Uses DataCollatorForLanguageModeling.\n",
    "- Training Arguments: Uses base TrainingArguments, predict_with_generate=True.\n",
    "- Trainer: Uses base Trainer.\n",
    "- Evaluation: compute_metrics decodes generated sequences and compares to references.\n",
    "- Output Directory: Changed to reflect the Mamba model.\n",
    "- Hyperparameters: Adjusted defaults for Mamba (likely need tuning).\n",
    "\n",
    "Dependencies:\n",
    "- Working Python 3.10 environment (e.g., t5_pip_test) with necessary packages\n",
    "  (transformers>=4.38, accelerate, torch, datasets, sentencepiece, causal-conv1d, mamba-ssm).\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime # Use the standard datetime module\n",
    "import logging\n",
    "import argparse # Still used for the Namespace object, but not for parsing\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "import inspect # Keep inspect if needed by other parts, otherwise optional now\n",
    "\n",
    "# Import the specific classes needed\n",
    "from transformers import (\n",
    "    AutoTokenizer, # Use AutoTokenizer for flexibility\n",
    "    MambaForCausalLM, # <<< CHANGED Model Class\n",
    "    DataCollatorForLanguageModeling, # <<< CHANGED Data Collator\n",
    "    Trainer, # <<< CHANGED to base Trainer\n",
    "    TrainingArguments, # <<< CHANGED to base TrainingArguments\n",
    "    TrainerCallback # For potential custom logging/debugging\n",
    ")\n",
    "from transformers.trainer_utils import set_seed, EvalPrediction\n",
    "\n",
    "# Configure basic logging\n",
    "logger = logging.getLogger(__name__)\n",
    "if not logger.hasHandlers():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        stream=sys.stdout # Ensure output to notebook\n",
    "    )\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# <<< CONFIGURATION >>>\n",
    "# =============================================================================\n",
    "# --- EDIT THESE VALUES TO CONFIGURE THE RUN ---\n",
    "args = argparse.Namespace(\n",
    "    # --- File Paths ---\n",
    "    # !!! Update these if your HEP dataset has different names/locations !!!\n",
    "    train_file='qed_data_processed/qed_amplitudes_train.jsonl',\n",
    "    val_file='qed_data_processed/qed_amplitudes_val.jsonl',\n",
    "    test_file='qed_data_processed/qed_amplitudes_test.jsonl',\n",
    "    output_dir='mamba_symbolic_hep_output', # <<< CHANGED Output Dir\n",
    "\n",
    "    # --- Model Configuration ---\n",
    "    # <<< CHANGED Model Checkpoint to a Mamba variant >>>\n",
    "    # Examples: 'state-spaces/mamba-130m', 'state-spaces/mamba-370m', ...\n",
    "    model_id=\"state-spaces/mamba-130m\",\n",
    "    # <<< Define a separator for causal LM formatting >>>\n",
    "    # Using EOS token is common\n",
    "    # tokenizer_id will be set based on model_id later, but we anticipate needing eos\n",
    "    separator_token=\"<|endoftext|>\", # Will be replaced by actual tokenizer.eos_token\n",
    "\n",
    "    # --- Tokenizer and Data Processing ---\n",
    "    # Mamba can handle long sequences, adjust based on data & GPU memory\n",
    "    max_seq_length=1024, # Start moderate, increase if possible\n",
    "\n",
    "    # --- Training Hyperparameters ---\n",
    "    # !!! Mamba often requires different HPs than Transformers !!!\n",
    "    num_epochs=3,           # May need more/fewer\n",
    "    # <<< Batch Size likely needs to be small for Mamba/long sequences >>>\n",
    "    train_batch_size=4,     # START SMALL!\n",
    "    eval_batch_size=4,      # START SMALL!\n",
    "    learning_rate=3e-4,     # Mamba might need different LR, check model card\n",
    "    weight_decay=0.1,       # Check model card recommendations\n",
    "    warmup_steps=100,       # Adjust based on dataset size/LR schedule\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    dataloader_num_workers=0,\n",
    "\n",
    "    # --- Advanced Training Features ---\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True, # Recommended for Mamba\n",
    "    # label_smoothing=0.0, # Typically not used for Causal LM\n",
    "\n",
    "    # --- Generation Configuration (for evaluation) ---\n",
    "    # These are used by compute_metrics via model.generate\n",
    "    generation_max_length=512, # Max length of the *generated target* part\n",
    "    num_beams=1, # Use greedy decoding (beam search less common for Mamba eval)\n",
    "    do_sample=False, # Use greedy decoding\n",
    "\n",
    "    # --- Reporting ---\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "# --- Global Tokenizer ---\n",
    "# We initialize the tokenizer globally after loading the config\n",
    "# because encode_sequences needs it, including the separator token.\n",
    "tokenizer = None\n",
    "\n",
    "# (Helper Functions: load_jsonl, convert_tokens_to_strings - Adapt if needed)\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads a JSON Lines (.jsonl) file.\"\"\"\n",
    "    # (Implementation is the same as before)\n",
    "    data = []\n",
    "    file_path = Path(file_path)\n",
    "    logger.info(f\"Loading data from: {file_path}\")\n",
    "    if not file_path.is_file():\n",
    "        logger.error(f\"Data file not found: {file_path}\")\n",
    "        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        logger.warning(f\"Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "                        continue\n",
    "        logger.info(f\"Successfully loaded {len(data)} records from {file_path}.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load data from {file_path}: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def convert_tokens_to_strings(raw_data_list):\n",
    "    \"\"\"Converts lists of tokens into single whitespace-joined strings.\"\"\"\n",
    "    # (Implementation is the same as before, check keys)\n",
    "    input_strings = []\n",
    "    target_strings = []\n",
    "    if not raw_data_list: return input_strings, target_strings\n",
    "    logger.info(f\"Attempting to convert {len(raw_data_list)} items to strings...\")\n",
    "    skipped_count = 0\n",
    "    for i, item in enumerate(raw_data_list):\n",
    "        input_toks = item.get('input_tokens') # Adapt key if needed\n",
    "        target_toks = item.get('target_tokens') # Adapt key if needed\n",
    "        if isinstance(input_toks, list) and isinstance(target_toks, list):\n",
    "            try:\n",
    "                joined_input = \" \".join([str(tok) for tok in input_toks])\n",
    "                joined_target = \" \".join([str(tok) for tok in target_toks])\n",
    "                input_strings.append(joined_input)\n",
    "                target_strings.append(joined_target)\n",
    "            except Exception as e: logger.warning(f\"Error joining tokens idx {i}: {e}\", exc_info=True); skipped_count+=1\n",
    "        else: logger.warning(f\"Skipping item idx {i}: Invalid/missing keys/types.\"); skipped_count+=1\n",
    "    logger.info(f\"Successfully converted {len(input_strings)} items to strings (skipped {skipped_count}).\")\n",
    "    return input_strings, target_strings\n",
    "\n",
    "\n",
    "# <<< CHANGED Data Encoding for Causal LM >>>\n",
    "def encode_sequences_for_causal_lm(input_strings, target_strings, max_len):\n",
    "    \"\"\"\n",
    "    Encodes input and target strings into a single sequence for Causal LM training.\n",
    "    Formats as: input_string + eos_token + target_string + eos_token\n",
    "    Creates labels where input tokens and pad tokens are masked (-100).\n",
    "    \"\"\"\n",
    "    global tokenizer # Use the globally initialized tokenizer\n",
    "    if tokenizer is None:\n",
    "        raise RuntimeError(\"Tokenizer is not initialized. Call init_tokenizer first.\")\n",
    "    if tokenizer.eos_token is None:\n",
    "        raise ValueError(\"Tokenizer must have an EOS token defined for this encoding scheme.\")\n",
    "\n",
    "    logger.info(f\"Starting Causal LM encoding for {len(input_strings)} pairs. MaxLen={max_len}.\")\n",
    "\n",
    "    results = {'input_ids': [], 'attention_mask': [], 'labels': []}\n",
    "    skipped_count = 0\n",
    "\n",
    "    for i, (input_str, target_str) in enumerate(zip(input_strings, target_strings)):\n",
    "        # 1. Tokenize Input\n",
    "        input_encoding = tokenizer(input_str, add_special_tokens=False) # Don't add EOS here yet\n",
    "        input_ids = input_encoding['input_ids']\n",
    "\n",
    "        # 2. Tokenize Target\n",
    "        target_encoding = tokenizer(target_str, add_special_tokens=False)\n",
    "        target_ids = target_encoding['input_ids']\n",
    "\n",
    "        # 3. Combine with Separator (EOS) and add final EOS\n",
    "        eos_token_id = tokenizer.eos_token_id\n",
    "        combined_ids = input_ids + [eos_token_id] + target_ids + [eos_token_id]\n",
    "\n",
    "        # 4. Truncate if necessary\n",
    "        if len(combined_ids) > max_len:\n",
    "            combined_ids = combined_ids[:max_len]\n",
    "            # logger.warning(f\"Sequence {i} truncated (len {len(combined_ids)+len(input_ids)+2} > {max_len})\") # Be careful about logging frequency\n",
    "\n",
    "        # 5. Create Attention Mask\n",
    "        attention_mask = [1] * len(combined_ids)\n",
    "\n",
    "        # 6. Create Labels (Mask input tokens and separator)\n",
    "        # Input length including the separator EOS token\n",
    "        input_len_plus_sep = len(input_ids) + 1\n",
    "        labels = [-100] * input_len_plus_sep + combined_ids[input_len_plus_sep:]\n",
    "        # Ensure labels length matches combined_ids length after potential truncation\n",
    "        labels = labels[:len(combined_ids)]\n",
    "        # Mask any remaining positions if truncation happened within the input part\n",
    "        labels[:min(input_len_plus_sep, len(labels))] = [-100] * min(input_len_plus_sep, len(labels))\n",
    "\n",
    "\n",
    "        # 7. Padding (Will be handled by DataCollator, but store results)\n",
    "        # Note: DataCollatorForLanguageModeling pads input_ids, attention_mask, and labels\n",
    "        results['input_ids'].append(combined_ids)\n",
    "        results['attention_mask'].append(attention_mask)\n",
    "        results['labels'].append(labels)\n",
    "\n",
    "        if i % 5000 == 0 and i > 0: # Log progress occasionally\n",
    "             logger.info(f\"  Encoded {i} sequences...\")\n",
    "\n",
    "    logger.info(f\"Finished Causal LM encoding {len(results['input_ids'])} sequences (skipped {skipped_count}).\")\n",
    "    return results\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset wrapper for Causal LM formatted sequence data.\"\"\"\n",
    "    # (Implementation is the same, just ensure keys match encode_sequences output)\n",
    "    def __init__(self, encodings):\n",
    "        required_keys = ['input_ids', 'attention_mask', 'labels']\n",
    "        if not isinstance(encodings, dict) or not all(key in encodings for key in required_keys):\n",
    "            logger.error(f\"Invalid encodings passed to SequenceDataset. Got Keys: {list(encodings.keys()) if isinstance(encodings, dict) else type(encodings)}\")\n",
    "            raise ValueError(f\"Encodings must be a dict with keys: {required_keys}.\")\n",
    "        try:\n",
    "            lengths = {key: len(val) for key, val in encodings.items() if key in required_keys and isinstance(val, list)}\n",
    "            if len(lengths) != len(required_keys):\n",
    "                 missing_or_wrong_type = [k for k in required_keys if k not in lengths]\n",
    "                 raise ValueError(f\"Missing or non-list required keys in encodings: {missing_or_wrong_type}\")\n",
    "            if len(set(lengths.values())) > 1:\n",
    "                 raise ValueError(f\"Inconsistent lengths in encodings: {lengths}\")\n",
    "            self.length = lengths.get('input_ids', 0)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed validating encoding structure/lengths: {e}\") from e\n",
    "\n",
    "        self.encodings = {k: v for k, v in encodings.items() if k in required_keys}\n",
    "        if self.length == 0: logger.warning(\"Initializing SequenceDataset with length 0.\")\n",
    "        logger.info(f\"Created SequenceDataset with {self.length} examples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not 0 <= idx < self.length: raise IndexError(f\"Index {idx} out of bounds ({self.length}).\")\n",
    "        try:\n",
    "            item = {key: self.encodings[key][idx] for key in self.encodings}\n",
    "            return item\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed creating item at index {idx}: {e}\", exc_info=True)\n",
    "            raise IndexError(f\"Error retrieving item at index {idx}: {e}\") from e\n",
    "\n",
    "\n",
    "# <<< CHANGED Evaluation Metrics for Causal LM Generation >>>\n",
    "def compute_metrics_causal_lm(eval_pred: EvalPrediction):\n",
    "    \"\"\"\n",
    "    Calculates exact match accuracy for Causal LM by comparing generated\n",
    "    sequences to reference sequences.\n",
    "    Assumes eval_pred.predictions contains generated token IDs and\n",
    "    eval_pred.label_ids contains the reference IDs (with -100 masking).\n",
    "    \"\"\"\n",
    "    global tokenizer # Use the globally initialized tokenizer\n",
    "    if tokenizer is None:\n",
    "        logger.error(\"Tokenizer missing in compute_metrics!\")\n",
    "        return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "    # Predictions are generated token IDs (potentially padded)\n",
    "    # Label IDs are reference token IDs (with -100 masking for input/padding)\n",
    "    predictions = eval_pred.predictions\n",
    "    label_ids = eval_pred.label_ids\n",
    "\n",
    "    if predictions is None or label_ids is None:\n",
    "         logger.error(\"compute_metrics received None for predictions or label_ids.\")\n",
    "         return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "    # Ensure numpy arrays\n",
    "    if not isinstance(predictions, np.ndarray): predictions = np.array(predictions)\n",
    "    if not isinstance(label_ids, np.ndarray): label_ids = np.array(label_ids)\n",
    "\n",
    "    # Replace -100 in labels with pad_token_id for decoding comparison\n",
    "    label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
    "\n",
    "    try:\n",
    "        # Decode generated predictions\n",
    "        # skip_special_tokens=True removes EOS, PAD etc.\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "        # Decode reference labels\n",
    "        decoded_labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Decode failed in compute_metrics: {e}\", exc_info=True)\n",
    "        # Log shapes for debugging\n",
    "        logger.error(f\"Pred shape: {predictions.shape}, Labels shape: {label_ids.shape}\")\n",
    "        return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "    # Post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    # Basic validation\n",
    "    if not decoded_preds or not decoded_labels or len(decoded_preds) != len(decoded_labels):\n",
    "        logger.warning(f\"Metrics decode issue: Preds empty={not decoded_preds}, Labels empty={not decoded_labels}, LenPred={len(decoded_preds)}, LenLabel={len(decoded_labels)}\")\n",
    "        return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "    # Calculate exact matches\n",
    "    matches = [pred == label for pred, label in zip(decoded_preds, decoded_labels)]\n",
    "    accuracy = np.mean(matches) if matches else 0.0\n",
    "\n",
    "    logger.info(f\"Computed sequence accuracy (generated vs reference): {accuracy:.4f}\")\n",
    "    # Return metric (can add others like BLEU, ROUGE if needed, though maybe less relevant for symbolic)\n",
    "    return {\"sequence_accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main Execution Logic\n",
    "# =============================================================================\n",
    "\n",
    "def main(config_args):\n",
    "    \"\"\"Orchestrates the Mamba training and evaluation for symbolic HEP.\"\"\"\n",
    "    global tokenizer # Allow main to modify the global tokenizer\n",
    "    args = config_args\n",
    "\n",
    "    # --- Basic Setup ---\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\" Starting Mamba Training/Evaluation for Symbolic HEP Calculation \")\n",
    "    logger.info(\"=\"*60)\n",
    "    start_time = datetime.datetime.now()\n",
    "    logger.info(f\"Script execution started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"CUDA available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "        logger.info(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        logger.warning(\"CUDA not available. Running on CPU.\")\n",
    "        if args.fp16:\n",
    "             logger.warning(\"fp16=True ignored because CUDA is not available.\")\n",
    "             args.fp16 = False\n",
    "\n",
    "    logger.info(f\"Running with configuration:\")\n",
    "    for k, v in vars(args).items(): logger.info(f\"  {k}: {v}\")\n",
    "\n",
    "    set_seed(args.seed)\n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"Output directory set to: {output_dir}\")\n",
    "\n",
    "    # --- Initialize Tokenizer ---\n",
    "    # Needs to happen before data encoding\n",
    "    logger.info(\"--- Stage 1: Initializing Tokenizer ---\")\n",
    "    try:\n",
    "        # Use AutoTokenizer based on the Mamba model ID\n",
    "        # Mamba models often use tokenizers like GPTNeoX\n",
    "        tokenizer_id = args.model_id # Or specify explicitly e.g., \"EleutherAI/gpt-neox-20b\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "        # <<< IMPORTANT: Set PAD token if missing (GPTNeoX doesn't always have one) >>>\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            logger.warning(f\"Tokenizer missing PAD token. Setting pad_token to eos_token ({tokenizer.eos_token}).\")\n",
    "            # Update model config potentially? Usually handled by Trainer/Collator if PAD=EOS\n",
    "            # model.config.pad_token_id = tokenizer.eos_token_id # Do this after model load\n",
    "\n",
    "        # Set the separator token based on the actual loaded tokenizer\n",
    "        args.separator_token = tokenizer.eos_token\n",
    "        logger.info(f\"Using separator token: {args.separator_token}\")\n",
    "\n",
    "        logger.info(f\"Tokenizer '{tokenizer_id}' initialized (Pad: {tokenizer.pad_token}, EOS: {tokenizer.eos_token}).\")\n",
    "    except Exception as e: logger.error(f\"Failed initializing tokenizer: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- Tokenizer Initialization Complete ---\")\n",
    "\n",
    "\n",
    "    # --- Data Loading and Preprocessing ---\n",
    "    logger.info(\"--- Stage 2: Loading Raw Data ---\")\n",
    "    train_raw = load_jsonl(args.train_file)\n",
    "    val_raw = load_jsonl(args.val_file)\n",
    "    test_raw = load_jsonl(args.test_file)\n",
    "    if not train_raw or not val_raw:\n",
    "        raise ValueError(\"Training and/or validation raw datasets empty.\")\n",
    "    logger.info(\"--- Raw Data Loading Complete ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 3: Converting Tokens to Strings ---\")\n",
    "    train_in, train_tgt = convert_tokens_to_strings(train_raw)\n",
    "    val_in, val_tgt = convert_tokens_to_strings(val_raw)\n",
    "    test_in, test_tgt = convert_tokens_to_strings(test_raw)\n",
    "    if not train_in or not val_in:\n",
    "        raise ValueError(\"Training and/or validation datasets empty after string conversion.\")\n",
    "    logger.info(\"--- Token-to-String Conversion Complete ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 4: Encoding Data for Causal LM ---\")\n",
    "    try:\n",
    "        logger.info(\"--> Encoding Training Data...\")\n",
    "        train_enc = encode_sequences_for_causal_lm(train_in, train_tgt, args.max_seq_length)\n",
    "        if not train_enc.get('input_ids'): raise ValueError(\"Training encoding failed.\")\n",
    "\n",
    "        logger.info(\"--> Encoding Validation Data...\")\n",
    "        val_enc = encode_sequences_for_causal_lm(val_in, val_tgt, args.max_seq_length)\n",
    "        if not val_enc.get('input_ids'): raise ValueError(\"Validation encoding failed.\")\n",
    "\n",
    "        test_enc = None\n",
    "        if test_in and test_tgt:\n",
    "            logger.info(\"--> Encoding Test Data...\")\n",
    "            test_enc = encode_sequences_for_causal_lm(test_in, test_tgt, args.max_seq_length)\n",
    "            if not test_enc.get('input_ids'): logger.warning(\"Test encoding returned empty.\")\n",
    "        else:\n",
    "             logger.info(\"Test input/target strings empty, skipping test encoding.\")\n",
    "\n",
    "    except Exception as e: logger.error(f\"Data encoding failed: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- Data Encoding Phase Complete ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 5: Creating PyTorch Datasets ---\")\n",
    "    try:\n",
    "        train_ds = SequenceDataset(train_enc)\n",
    "        val_ds = SequenceDataset(val_enc)\n",
    "        test_ds = SequenceDataset(test_enc) if test_enc and test_enc.get('input_ids') else None\n",
    "    except Exception as e: logger.error(f\"Dataset creation failed: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- PyTorch Datasets Created Successfully ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 6: Initializing Model ---\")\n",
    "    try:\n",
    "        # <<< CHANGED to Mamba Model >>>\n",
    "        model = MambaForCausalLM.from_pretrained(\n",
    "            args.model_id,\n",
    "            # torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16 # Use BF16 if available\n",
    "            # Use FP32 for stability initially, Trainer handles FP16/BF16 later\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "        # <<< Set pad_token_id in model config if tokenizer needed it >>>\n",
    "        if tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
    "             logger.info(\"Setting model.config.pad_token_id = tokenizer.eos_token_id\")\n",
    "             model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        logger.info(f\"Model {args.model_id} loaded.\")\n",
    "        if torch.cuda.is_available(): logger.info(f\"Est. Model Memory (FP32): {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "    except Exception as e: logger.error(f\"Failed initializing model: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- Model Initialization Complete ---\")\n",
    "\n",
    "    logger.info(\"--- Stage 7: Configuring Training Environment ---\")\n",
    "    use_gc = args.gradient_checkpointing\n",
    "    if use_gc: logger.info(\"Attempting to enable Gradient Checkpointing...\")\n",
    "    # Mamba models might have different GC enabling methods or rely solely on Trainer arg\n",
    "    try:\n",
    "        if use_gc and hasattr(model, 'gradient_checkpointing_enable'):\n",
    "             model.gradient_checkpointing_enable(); logger.info(\"Gradient Checkpointing enabled via model method.\")\n",
    "        elif use_gc:\n",
    "             logger.info(\"Model has no 'gradient_checkpointing_enable' method. Relying on TrainingArguments.\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed enabling Gradient Checkpointing on model: {e}. Relying on TrainingArguments setting.\", exc_info=True)\n",
    "        use_gc = args.gradient_checkpointing\n",
    "\n",
    "    use_fp16 = args.fp16\n",
    "    logger.info(\"Initializing Data Collator...\")\n",
    "    try:\n",
    "        # <<< Use DataCollatorForLanguageModeling >>>\n",
    "        # mlm=False for Causal LM (not Masked LM)\n",
    "        collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "        logger.info(\"Data Collator for Language Modeling initialized.\")\n",
    "    except Exception as e: logger.error(f\"Collator init failed: {e}\", exc_info=True); raise\n",
    "\n",
    "\n",
    "    # --- Initialize Training Arguments ---\n",
    "    logger.info(\"Defining Training Arguments...\")\n",
    "    report_to = args.report_to.lower() if isinstance(args.report_to, str) else args.report_to\n",
    "    if report_to == \"none\": logger.info(\"Reporting disabled.\")\n",
    "    else: logger.info(f\"Reporting to: {report_to}\")\n",
    "\n",
    "    train_args = None\n",
    "    try:\n",
    "        # <<< Use base TrainingArguments >>>\n",
    "        train_args = TrainingArguments(\n",
    "            output_dir=str(output_dir),\n",
    "            # Core Training Params\n",
    "            num_train_epochs=args.num_epochs,\n",
    "            per_device_train_batch_size=args.train_batch_size,\n",
    "            per_device_eval_batch_size=args.eval_batch_size,\n",
    "            learning_rate=args.learning_rate,\n",
    "            weight_decay=args.weight_decay,\n",
    "            warmup_steps=args.warmup_steps,\n",
    "            seed=args.seed,\n",
    "            # Evaluation and Saving Strategy\n",
    "            eval_strategy=args.eval_strategy,\n",
    "            eval_steps=args.eval_steps if args.eval_strategy == \"steps\" else None,\n",
    "            save_strategy=args.save_strategy,\n",
    "            save_steps=args.save_steps if args.save_strategy == \"steps\" else None,\n",
    "            save_total_limit=args.save_total_limit,\n",
    "            load_best_model_at_end=True, # Load best model based on eval metric\n",
    "            metric_for_best_model=\"sequence_accuracy\", # Needs to match compute_metrics output\n",
    "            greater_is_better=True, # Accuracy should be maximized\n",
    "            # Logging\n",
    "            logging_dir=str(output_dir / 'logs'),\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=args.logging_steps,\n",
    "            report_to=report_to,\n",
    "            # Performance / Hardware\n",
    "            fp16=use_fp16,\n",
    "            # bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(), # Optionally enable BF16\n",
    "            gradient_checkpointing=use_gc,\n",
    "            dataloader_num_workers=args.dataloader_num_workers,\n",
    "            # Other Args\n",
    "            remove_unused_columns=False, # Important for custom datasets/collators sometimes\n",
    "            # <<< Add args relevant for generation during evaluation >>>\n",
    "            predict_with_generate=True, # Tell Trainer to use generate() for predictions\n",
    "            # label_names=[\"labels\"], # Usually inferred correctly\n",
    "        )\n",
    "        logger.info(\"Successfully initialized TrainingArguments.\")\n",
    "        logger.info(f\"Effective FP16: {train_args.fp16}, Grad Checkpointing: {train_args.gradient_checkpointing}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"FAILED to initialize TrainingArguments: {e}\", exc_info=True)\n",
    "        raise e\n",
    "\n",
    "    if train_args is None:\n",
    "        raise RuntimeError(\"train_args was not successfully defined.\")\n",
    "\n",
    "    logger.info(\"--- Training Environment Configuration Complete ---\")\n",
    "\n",
    "    # --- Initialize Trainer ---\n",
    "    logger.info(\"--- Stage 8: Initializing Trainer ---\")\n",
    "    try:\n",
    "        # <<< Use base Trainer >>>\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=train_args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            data_collator=collator,\n",
    "            tokenizer=tokenizer, # Pass tokenizer for generation\n",
    "            compute_metrics=compute_metrics_causal_lm, # Use the Causal LM metrics function\n",
    "        )\n",
    "        logger.info(\"Trainer Initialized.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Trainer init failed: {e}\", exc_info=True)\n",
    "        raise e\n",
    "\n",
    "    # --- Training ---\n",
    "    logger.info(\"--- Stage 9: Starting Model Training ---\")\n",
    "    try:\n",
    "        logger.info(f\"Starting training for {args.num_epochs} epochs...\")\n",
    "        train_res = trainer.train()\n",
    "        metrics = train_res.metrics\n",
    "        logger.info(\"--- Training Finished ---\")\n",
    "        logger.info(\"--- Training Metrics ---\")\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        logger.info(\"Trainer state saved.\")\n",
    "        best_path = str(output_dir / \"best_model\")\n",
    "        trainer.save_model(best_path)\n",
    "        logger.info(f\"Final best model saved to: {best_path}\")\n",
    "    except Exception as e: logger.error(f\"Training failed: {e}\", exc_info=True); raise\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    logger.info(\"--- Stage 10: Evaluating Model on Test Set ---\")\n",
    "    if test_ds:\n",
    "        try:\n",
    "            logger.info(\"Running final evaluation on the test set...\")\n",
    "            # Use predict() which respects predict_with_generate=True\n",
    "            # Or use evaluate() which should also trigger generation if configured\n",
    "            test_res = trainer.evaluate(eval_dataset=test_ds, metric_key_prefix=\"test\")\n",
    "            logger.info(\"--- Evaluation on Test Set Finished ---\")\n",
    "            logger.info(\"--- Test Set Results ---\")\n",
    "            trainer.log_metrics(\"test\", test_res)\n",
    "            trainer.save_metrics(\"test\", test_res)\n",
    "            metric_key = \"test_sequence_accuracy\"\n",
    "            if metric_key in test_res:\n",
    "                logger.info(f\"\\n*** Test Sequence Accuracy: {test_res[metric_key]:.4f} ***\\n\")\n",
    "            else:\n",
    "                logger.warning(f\"Metric '{metric_key}' not found in test results.\")\n",
    "                logger.info(f\"Available test metrics: {list(test_res.keys())}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Test evaluation failed: {e}\", exc_info=True)\n",
    "    else:\n",
    "        logger.warning(\"Test dataset unavailable or not loaded. Skipping final evaluation.\")\n",
    "    logger.info(\"--- Final Evaluation Stage Complete ---\")\n",
    "\n",
    "    # --- End Script ---\n",
    "    end_time = datetime.datetime.now()\n",
    "    total_time = end_time - start_time\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\" Script execution finished successfully at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    logger.info(f\" Total execution time: {total_time}\")\n",
    "    logger.info(\"=\"*60)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Execute Main Function (for Notebook context)\n",
    "# =============================================================================\n",
    "try:\n",
    "    if not torch.cuda.is_available() and args.fp16:\n",
    "        logger.warning(\"MAIN (Notebook): CUDA not detected before starting main function!\")\n",
    "        logger.warning(\"MAIN (Notebook): Disabling fp16 as CUDA is not available.\")\n",
    "        args.fp16 = False\n",
    "\n",
    "    main(args) # Call the main function\n",
    "\n",
    "except Exception as e:\n",
    "     logger.critical(f\"Unhandled exception terminated script execution: {e}\", exc_info=True)\n",
    "     # raise e # Optionally re-raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3: Transformer Models for Symbolic Regression\n",
    "Model: Transformer model with a contemporary innovation added such as KAN layers, reinforcement learning, genetic algorithms, specialized long-sequence attention, etc. which improves the performance compared to a basic transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting Reformer Fine-tuning Script for Symbolic Regression...\n",
      "[INFO] Current date/time (UTC): 2025-04-23 03:45:11 UTC\n",
      "[INFO] Using model: google/reformer-enwik8 with Tokenizer: google/reformer-enwik8\n",
      "[INFO] Loading data from: qed_expressions_train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Error] Data file not found: qed_expressions_train.jsonl\n",
      "[FATAL] Critical error during data loading: File not found: qed_expressions_train.jsonl\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikitas/anaconda3/envs/t5_gpu_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Task 3.3: End-to-end Training and Evaluation Script for a Reformer Model\n",
    "for Symbolic Regression.\n",
    "\n",
    "This script fine-tunes a Reformer model configured as an Encoder-Decoder\n",
    "for symbolic regression tasks (sequence-to-sequence). Reformer is chosen\n",
    "for its efficient attention mechanism (LSH), suitable for potentially long sequences.\n",
    "\n",
    "*** NOTE: This version is adapted for robust execution ***\n",
    "*** within a Jupyter Notebook (.ipynb). Configuration is handled    ***\n",
    "*** by manually setting attributes on the `args` object.            ***\n",
    "\n",
    "Key Changes:\n",
    "- Model: Uses EncoderDecoderModel with Reformer weights.\n",
    "- Tokenizer: Uses ReformerTokenizer (character-level for 'google/reformer-enwik8').\n",
    "- Checkpoint: Uses 'google/reformer-enwik8' (with caveats about tokenizer suitability).\n",
    "- Data Loading: Assumes input/target strings are directly in the JSONL data.\n",
    "- Output Directory: Changed to reflect the model and task.\n",
    "- Dependencies: Requires 'transformers', 'torch', 'datasets', 'sentencepiece'.\n",
    "\n",
    "!!! WARNING !!!\n",
    "The 'google/reformer-enwik8' checkpoint uses a character-level tokenizer.\n",
    "This may be suboptimal for symbolic regression compared to a tokenizer trained\n",
    "on mathematical symbols or subwords. Consider using a different base model or\n",
    "tokenizer fine-tuning if needed.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime # Use the standard datetime module\n",
    "import logging\n",
    "import argparse # Still used for the Namespace object, but not for parsing\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "import inspect # Keep inspect if needed by other parts, otherwise optional now\n",
    "\n",
    "# Import the specific classes needed\n",
    "from transformers import (\n",
    "    ReformerTokenizer,         # <<< Reformer Tokenizer\n",
    "    ReformerModel,             # <<< Base Reformer model\n",
    "    EncoderDecoderModel,       # <<< Wrapper for Enc-Dec setup\n",
    "    DataCollatorForSeq2Seq,    # <<< Standard Seq2Seq Collator\n",
    "    Seq2SeqTrainer,            # <<< Standard Seq2Seq Trainer\n",
    "    Seq2SeqTrainingArguments,  # <<< Standard Seq2Seq Arguments\n",
    ")\n",
    "from transformers.trainer_utils import set_seed, EvalPrediction\n",
    "\n",
    "# Configure basic logging\n",
    "logger = logging.getLogger(__name__)\n",
    "if not logger.hasHandlers():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        stream=sys.stdout # Ensure output to notebook\n",
    "    )\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# =============================================================================\n",
    "# <<< CONFIGURATION >>>\n",
    "# =============================================================================\n",
    "# --- EDIT THESE VALUES TO CONFIGURE THE RUN ---\n",
    "args = argparse.Namespace(\n",
    "    # --- File Paths ---\n",
    "    # !!! UPDATE these to your symbolic regression data files !!!\n",
    "    # Expects keys like 'input_str' and 'target_str' in the JSONL\n",
    "    train_file='symbolic_regression_train.jsonl', # Placeholder name\n",
    "    val_file='symbolic_regression_val.jsonl',     # Placeholder name\n",
    "    test_file='symbolic_regression_test.jsonl',      # Placeholder name\n",
    "    output_dir='reformer_symbolic_regression_output', # <<< CHANGED Output Dir\n",
    "\n",
    "    # --- Model Configuration ---\n",
    "    # Using Reformer - WARNING about character-level tokenizer for enwik8!\n",
    "    model_id=\"google/reformer-enwik8\",\n",
    "    tokenizer_id=\"google/reformer-enwik8\", # Usually same as model_id\n",
    "    task_prefix=\"regress expression: \",     # Optional task prefix\n",
    "\n",
    "    # --- Tokenizer and Data Processing ---\n",
    "    max_seq_length=512, # Reformer can handle longer sequences, adjust based on data/memory\n",
    "\n",
    "    # --- Training Hyperparameters ---\n",
    "    # !!! Tune these for Reformer and your specific dataset !!!\n",
    "    num_epochs=5,\n",
    "    train_batch_size=8,     # Adjust based on sequence length and GPU memory\n",
    "    eval_batch_size=8,      # Adjust based on sequence length and GPU memory\n",
    "    learning_rate=5e-5,     # Starting point, may need adjustment\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=300,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",  # Correct parameter name\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    dataloader_num_workers=0,\n",
    "\n",
    "    # --- Advanced Training Features ---\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,    # Recommended for Reformer\n",
    "    label_smoothing_factor=0.1,\n",
    "\n",
    "    # --- Generation Configuration (for evaluation) ---\n",
    "    generation_num_beams=4,\n",
    "\n",
    "    # --- Reporting ---\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "# --- Global Tokenizer ---\n",
    "tokenizer = None\n",
    "tokenizer_for_metrics = None\n",
    "\n",
    "# (Helper Functions)\n",
    "def load_jsonl_direct_strings(file_path, input_key='input_str', target_key='target_str'):\n",
    "    \"\"\"\n",
    "    Loads data from a JSON Lines file, extracting specified input/target strings.\n",
    "    \"\"\"\n",
    "    input_strings = []\n",
    "    target_strings = []\n",
    "    file_path = Path(file_path)\n",
    "    logger.info(f\"Loading data from: {file_path} (expecting keys '{input_key}', '{target_key}')\")\n",
    "    if not file_path.is_file():\n",
    "        logger.error(f\"Data file not found: {file_path}\")\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    skipped_count = 0\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        item = json.loads(line)\n",
    "                        input_str = item.get(input_key)\n",
    "                        target_str = item.get(target_key)\n",
    "                        if isinstance(input_str, str) and isinstance(target_str, str):\n",
    "                            input_strings.append(input_str)\n",
    "                            target_strings.append(target_str)\n",
    "                        else:\n",
    "                            logger.warning(f\"Skipping line {i+1}: Missing/invalid types for keys '{input_key}' or '{target_key}'.\")\n",
    "                            skipped_count += 1\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        logger.warning(f\"Skipping invalid JSON on line {i+1}: {e}\")\n",
    "                        skipped_count += 1\n",
    "                    except Exception as item_e:\n",
    "                        logger.warning(f\"Error processing line {i+1}: {item_e}\")\n",
    "                        skipped_count += 1\n",
    "\n",
    "        logger.info(f\"Successfully loaded {len(input_strings)} string pairs (skipped {skipped_count} lines).\")\n",
    "        if not input_strings:\n",
    "             logger.warning(f\"No valid records loaded from {file_path}.\")\n",
    "        return input_strings, target_strings\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load data from {file_path}: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "def encode_sequences(source_texts, target_texts, max_len):\n",
    "    \"\"\"\n",
    "    Tokenizes source and target text sequences for Encoder-Decoder models.\n",
    "    \"\"\"\n",
    "    global tokenizer # Use global tokenizer\n",
    "    if tokenizer is None: raise RuntimeError(\"Tokenizer not initialized.\")\n",
    "\n",
    "    logger.info(f\"Encoding sequence pairs with max_length={max_len}...\")\n",
    "\n",
    "    # Tokenize source texts (encoder input)\n",
    "    encoder_inputs = tokenizer(\n",
    "        source_texts,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',   # Pad to max_len\n",
    "        truncation=True,        # Truncate sequences longer than max_len\n",
    "        return_tensors=None     # Return lists\n",
    "    )\n",
    "\n",
    "    # Tokenize target texts to create labels\n",
    "    # Decoder input_ids are usually created internally by shifting labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        decoder_labels = tokenizer(\n",
    "            target_texts,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "    # Assign the tokenized target IDs as 'labels'\n",
    "    encoder_inputs['labels'] = decoder_labels['input_ids']\n",
    "\n",
    "    logger.info(f\"Encoding complete for {len(encoder_inputs['input_ids'])} sequences.\")\n",
    "    return encoder_inputs\n",
    "\n",
    "\n",
    "class SequencePairDataset(Dataset):\n",
    "    \"\"\"Simple PyTorch Dataset for holding tokenized sequence pair data.\"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        if not isinstance(encodings, dict) or 'input_ids' not in encodings or 'labels' not in encodings:\n",
    "            raise ValueError(\"Encodings must be a dictionary containing 'input_ids' and 'labels'.\")\n",
    "        self.encodings = encodings\n",
    "        try:\n",
    "            self.length = len(encodings['input_ids'])\n",
    "            # Basic validation\n",
    "            if not all(isinstance(encodings[key], list) and len(encodings[key]) == self.length for key in encodings):\n",
    "                 raise ValueError(f\"All encoding keys must be lists of the same length ({self.length}).\")\n",
    "        except Exception as e:\n",
    "             raise ValueError(f\"Failed to validate encodings: {e}\")\n",
    "        if self.length == 0: logger.warning(\"Initializing SequencePairDataset with length 0.\")\n",
    "        logger.info(f\"Created Dataset with {self.length} examples.\")\n",
    "\n",
    "    def __len__(self): return self.length\n",
    "    def __getitem__(self, idx):\n",
    "        if not 0 <= idx < self.length: raise IndexError(f\"Index {idx} out of bounds.\")\n",
    "        try:\n",
    "            return {key: self.encodings[key][idx] for key in self.encodings}\n",
    "        except Exception as e: logger.error(f\"Failed retrieving item idx {idx}: {e}\"); raise\n",
    "\n",
    "\n",
    "def compute_metrics_fn(eval_pred):\n",
    "    \"\"\"Calculates exact sequence match accuracy after decoding.\"\"\"\n",
    "    # !!! Consider SymPy or numerical evaluation for more robust symbolic regression metrics !!!\n",
    "    global tokenizer_for_metrics\n",
    "    if tokenizer_for_metrics is None: logger.error(\"Tokenizer missing!\"); return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    if not isinstance(predictions, np.ndarray): predictions = np.array(predictions)\n",
    "    if not isinstance(labels, np.ndarray): labels = np.array(labels)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer_for_metrics.pad_token_id)\n",
    "\n",
    "    try:\n",
    "        # Predictions might already be IDs if predict_with_generate=True\n",
    "        # Check shape, if it looks like logits, argmax it. Reformer might output logits.\n",
    "        if predictions.ndim == 3: # Shape (batch_size, seq_len, vocab_size) -> Logits\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "        decoded_preds = tokenizer_for_metrics.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer_for_metrics.batch_decode(labels, skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "         logger.error(f\"Decoding failed in compute_metrics: {e}\", exc_info=True)\n",
    "         return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    if len(decoded_preds) != len(decoded_labels):\n",
    "        logger.warning(f\"Metrics: Mismatch pred/label count: {len(decoded_preds)} vs {len(decoded_labels)}\")\n",
    "        return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "    matches = [pred == label for pred, label in zip(decoded_preds, decoded_labels)]\n",
    "    accuracy = np.mean(matches) if matches else 0.0\n",
    "    logger.info(f\"Computed sequence accuracy: {accuracy:.4f}\")\n",
    "    return {\"sequence_accuracy\": float(accuracy)}\n",
    "\n",
    "# =============================================================================\n",
    "# Main Execution Logic\n",
    "# =============================================================================\n",
    "\n",
    "def main(config_args):\n",
    "    \"\"\"Orchestrates the Reformer fine-tuning for symbolic regression.\"\"\"\n",
    "    global tokenizer, tokenizer_for_metrics # Allow main to modify globals\n",
    "    args = config_args\n",
    "\n",
    "    # --- Basic Setup ---\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\" Starting Reformer Training/Evaluation for Symbolic Regression \")\n",
    "    logger.info(\"=\"*60)\n",
    "    start_time = datetime.datetime.now()\n",
    "    logger.info(f\"Script execution started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    if torch.cuda.is_available(): logger.info(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    else: logger.warning(\"CUDA not available. Running on CPU.\"); args.fp16 = False\n",
    "\n",
    "    logger.info(f\"Running with configuration:\")\n",
    "    for k, v in vars(args).items(): logger.info(f\"  {k}: {v}\")\n",
    "\n",
    "    set_seed(args.seed)\n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"Output directory set to: {output_dir}\")\n",
    "\n",
    "    # --- Initialize Tokenizer ---\n",
    "    logger.info(\"--- Stage 1: Initializing Tokenizer ---\")\n",
    "    try:\n",
    "        tokenizer = ReformerTokenizer.from_pretrained(args.tokenizer_id)\n",
    "        # Add special tokens if they don't exist - important for Reformer structure\n",
    "        special_tokens_to_add = {}\n",
    "        if tokenizer.pad_token is None: special_tokens_to_add['pad_token'] = '<pad>'\n",
    "        if tokenizer.bos_token is None: special_tokens_to_add['bos_token'] = '<s>' # Use BOS for start\n",
    "        if tokenizer.eos_token is None: special_tokens_to_add['eos_token'] = '</s>' # Use EOS for end\n",
    "        if special_tokens_to_add:\n",
    "             logger.warning(f\"Adding special tokens: {special_tokens_to_add}\")\n",
    "             tokenizer.add_special_tokens(special_tokens_to_add)\n",
    "        tokenizer_for_metrics = tokenizer # Set global for metrics\n",
    "        logger.warning(\"Using ReformerTokenizer (google/reformer-enwik8) which is CHARACTER-LEVEL. \"\n",
    "                       \"Ensure this matches your data or consider a different tokenizer/model.\")\n",
    "        logger.info(f\"Tokenizer '{args.tokenizer_id}' initialized (Vocab size: {len(tokenizer)}).\")\n",
    "        logger.info(f\"PAD={tokenizer.pad_token_id}, BOS={tokenizer.bos_token_id}, EOS={tokenizer.eos_token_id}\")\n",
    "    except Exception as e: logger.error(f\"Failed initializing tokenizer: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- Tokenizer Initialization Complete ---\")\n",
    "\n",
    "\n",
    "    # --- Data Loading (Direct Strings) ---\n",
    "    logger.info(\"--- Stage 2: Loading Raw Data (Direct Strings) ---\")\n",
    "    try:\n",
    "        # Assumes keys 'input_str', 'target_str' in the jsonl files\n",
    "        train_sources, train_targets = load_jsonl_direct_strings(args.train_file)\n",
    "        val_sources, val_targets = load_jsonl_direct_strings(args.val_file)\n",
    "        test_sources, test_targets = load_jsonl_direct_strings(args.test_file)\n",
    "    except Exception as e: logger.error(f\"Critical error during data loading: {e}\", exc_info=True); raise\n",
    "    if not train_sources or not val_sources: raise ValueError(\"Training and/or validation source/target strings empty.\")\n",
    "    logger.info(\"--- Raw String Data Loading Complete ---\")\n",
    "\n",
    "\n",
    "    # --- Tokenize Data ---\n",
    "    logger.info(\"--- Stage 3: Encoding Data (Tokenization) ---\")\n",
    "    try:\n",
    "        train_encodings = encode_sequences(train_sources, train_targets, args.max_seq_length)\n",
    "        val_encodings = encode_sequences(val_sources, val_targets, args.max_seq_length)\n",
    "        test_encodings = encode_sequences(test_sources, test_targets, args.max_seq_length) if test_sources else None\n",
    "    except Exception as e: logger.error(f\"Data encoding failed: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- Data Encoding Phase Complete ---\")\n",
    "\n",
    "\n",
    "    # --- Create Datasets ---\n",
    "    logger.info(\"--- Stage 4: Creating PyTorch Datasets ---\")\n",
    "    try:\n",
    "        train_dataset = SequencePairDataset(train_encodings)\n",
    "        val_dataset = SequencePairDataset(val_encodings)\n",
    "        test_dataset = SequencePairDataset(test_encodings) if test_encodings else None\n",
    "    except Exception as e: logger.error(f\"Dataset creation failed: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- PyTorch Datasets Created Successfully ---\")\n",
    "\n",
    "    # --- Initialize Model ---\n",
    "    logger.info(\"--- Stage 5: Initializing Model ---\")\n",
    "    try:\n",
    "        logger.info(f\"Loading Reformer weights from {args.model_id} for EncoderDecoderModel\")\n",
    "        model = EncoderDecoderModel.from_encoder_decoder_pretrained(args.model_id, args.model_id)\n",
    "\n",
    "        # Resize embeddings if tokenizer vocab changed\n",
    "        if model.config.encoder.vocab_size != len(tokenizer):\n",
    "             logger.info(f\"Resizing model token embeddings from {model.config.encoder.vocab_size} to {len(tokenizer)}\")\n",
    "             model.resize_token_embeddings(len(tokenizer))\n",
    "             # Update config values after resizing\n",
    "             model.config.encoder.vocab_size = len(tokenizer)\n",
    "             model.config.decoder.vocab_size = len(tokenizer)\n",
    "\n",
    "        # Configure for Seq2Seq Generation\n",
    "        model.config.decoder_start_token_id = tokenizer.bos_token_id # Use BOS as decoder start\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        # Critical: Ensure decoder is configured as a decoder with cross-attention\n",
    "        model.config.decoder.is_decoder = True\n",
    "        model.config.decoder.add_cross_attention = True\n",
    "\n",
    "        # Set generation parameters on the model config\n",
    "        model.config.max_length = args.max_seq_length # Max output length during generation\n",
    "        model.config.num_beams = args.generation_num_beams\n",
    "\n",
    "        # Tie weights\n",
    "        logger.info(\"Tying encoder and decoder weights.\")\n",
    "        model.tie_weights()\n",
    "\n",
    "        logger.info(f\"Model {args.model_id} (EncoderDecoder) loaded and configured.\")\n",
    "        if torch.cuda.is_available(): logger.info(f\"Est. Model Memory: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "        # Enable gradient checkpointing *on the model* if desired\n",
    "        if args.gradient_checkpointing:\n",
    "            model.gradient_checkpointing_enable()\n",
    "            logger.info(\"Gradient Checkpointing enabled via model method.\")\n",
    "\n",
    "    except Exception as e: logger.error(f\"Failed initializing model: {e}\", exc_info=True); raise\n",
    "    logger.info(\"--- Model Initialization Complete ---\")\n",
    "\n",
    "    # --- Initialize Collator ---\n",
    "    logger.info(\"--- Stage 6: Initializing Data Collator ---\")\n",
    "    try:\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            label_pad_token_id=-100, # Important: Ignore padding in loss\n",
    "            pad_to_multiple_of=8 if args.fp16 else None\n",
    "        )\n",
    "        logger.info(\"Data Collator Initialized.\")\n",
    "    except Exception as e: logger.error(f\"Collator init failed: {e}\", exc_info=True); raise\n",
    "\n",
    "    # --- Initialize Training Arguments ---\n",
    "    logger.info(\"--- Stage 7: Defining Training Arguments ---\")\n",
    "    report_to = args.report_to.lower() if isinstance(args.report_to, str) else args.report_to\n",
    "    logger.info(f\"Reporting to: {report_to}\")\n",
    "    train_args = None\n",
    "    try:\n",
    "        # Use Seq2SeqTrainingArguments\n",
    "        train_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=str(output_dir),\n",
    "            # Core Training Params\n",
    "            num_train_epochs=args.num_epochs,\n",
    "            per_device_train_batch_size=args.train_batch_size,\n",
    "            per_device_eval_batch_size=args.eval_batch_size,\n",
    "            learning_rate=args.learning_rate,\n",
    "            weight_decay=args.weight_decay,\n",
    "            warmup_steps=args.warmup_steps,\n",
    "            seed=args.seed,\n",
    "            # Evaluation and Saving Strategy\n",
    "            eval_strategy=args.eval_strategy, # Use correct name\n",
    "            save_strategy=args.save_strategy,\n",
    "            save_total_limit=args.save_total_limit,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"sequence_accuracy\",\n",
    "            greater_is_better=True,\n",
    "            # Logging\n",
    "            logging_dir=str(output_dir / 'logs'),\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=args.logging_steps,\n",
    "            report_to=report_to,\n",
    "            # Performance / Hardware\n",
    "            fp16=args.fp16,\n",
    "            # Gradient checkpointing - enable in args if enabled successfully on model\n",
    "            gradient_checkpointing=(args.gradient_checkpointing and hasattr(model, 'is_gradient_checkpointing') and model.is_gradient_checkpointing),\n",
    "            dataloader_num_workers=args.dataloader_num_workers,\n",
    "            # Seq2Seq Specific\n",
    "            predict_with_generate=True,\n",
    "            generation_max_length=args.max_seq_length, # Use config seq len for consistency\n",
    "            generation_num_beams=args.generation_num_beams,\n",
    "            label_smoothing_factor=args.label_smoothing_factor,\n",
    "        )\n",
    "        logger.info(\"Successfully initialized Seq2SeqTrainingArguments.\")\n",
    "        logger.info(f\"Effective FP16: {train_args.fp16}, Grad Checkpointing: {train_args.gradient_checkpointing}\")\n",
    "\n",
    "    except Exception as e: logger.error(f\"FAILED to initialize Training Arguments: {e}\", exc_info=True); raise\n",
    "    if train_args is None: raise RuntimeError(\"train_args failed initialization.\")\n",
    "    logger.info(\"--- Training Environment Configuration Complete ---\")\n",
    "\n",
    "    # --- Initialize Trainer ---\n",
    "    logger.info(\"--- Stage 8: Initializing Seq2SeqTrainer ---\")\n",
    "    try:\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=train_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics_fn,\n",
    "        )\n",
    "        logger.info(\"Seq2SeqTrainer Initialized.\")\n",
    "    except Exception as e: logger.error(f\"Trainer init failed: {e}\", exc_info=True); raise\n",
    "\n",
    "    # --- Training ---\n",
    "    logger.info(\"--- Stage 9: Starting Model Training ---\")\n",
    "    try:\n",
    "        logger.info(f\"Starting training for {args.num_epochs} epochs...\")\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        logger.info(\"--- Training Finished ---\")\n",
    "        logger.info(\"--- Training Metrics ---\")\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        logger.info(\"Trainer state saved.\")\n",
    "        best_path = str(output_dir / \"best_model\")\n",
    "        trainer.save_model(best_path) # Saves the best model if load_best_model_at_end=True\n",
    "        logger.info(f\"Final best model saved to: {best_path}\")\n",
    "    except Exception as e: logger.error(f\"Training failed: {e}\", exc_info=True); raise\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    logger.info(\"--- Stage 10: Evaluating Model on Test Set ---\")\n",
    "    if test_dataset:\n",
    "        try:\n",
    "            logger.info(\"Running final evaluation on the test set...\")\n",
    "            test_results = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")\n",
    "            logger.info(\"--- Evaluation on Test Set Finished ---\")\n",
    "            logger.info(\"--- Test Set Results ---\")\n",
    "            trainer.log_metrics(\"test\", test_results)\n",
    "            trainer.save_metrics(\"test\", test_results)\n",
    "            metric_key = \"test_sequence_accuracy\"\n",
    "            if metric_key in test_results:\n",
    "                logger.info(f\"\\n*** Test Sequence Accuracy: {test_results[metric_key]:.4f} ***\\n\")\n",
    "            else:\n",
    "                logger.warning(f\"Metric '{metric_key}' not found in test results.\")\n",
    "                logger.info(f\"Available test metrics: {list(test_results.keys())}\")\n",
    "        except Exception as e: logger.error(f\"Test evaluation failed: {e}\", exc_info=True)\n",
    "    else: logger.warning(\"Test dataset unavailable or not loaded. Skipping final evaluation.\")\n",
    "    logger.info(\"--- Final Evaluation Stage Complete ---\")\n",
    "\n",
    "    # --- End Script ---\n",
    "    end_time = datetime.datetime.now()\n",
    "    total_time = end_time - start_time\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\" Script execution finished successfully at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    logger.info(f\" Total execution time: {total_time}\")\n",
    "    logger.info(\"=\"*60)\n",
    "\n",
    "# =============================================================================\n",
    "# Execute Main Function (for Notebook context)\n",
    "# =============================================================================\n",
    "try:\n",
    "    if not torch.cuda.is_available() and args.fp16:\n",
    "        logger.warning(\"MAIN (Notebook): CUDA not detected!\")\n",
    "        args.fp16 = False\n",
    "    # Call the main function with the configured args object\n",
    "    main(args)\n",
    "except Exception as e:\n",
    "     logger.critical(f\"Unhandled exception terminated script execution: {e}\", exc_info=True)\n",
    "     # raise e # Optionally re-raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4: Titans for squared amplitude calculation\n",
    "Model: One of the core architectures from Google’s paper introducing Titans concept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed to import TitanTokenizer or TitanForConditionalGeneration.\n",
      "Please ensure these classes are defined in your transformers library or environment.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "End-to-end Fine-tuning Script for a Hypothetical \"Titan\" Sequence-to-Sequence Model\n",
    "on the Preprocessed QED Squared Amplitude Calculation Dataset.\n",
    "\n",
    "This script demonstrates fine-tuning a sequence-to-sequence model, referred to\n",
    "here as \"Titan\" (e.g., `google/titan-small`), presumably incorporating unique\n",
    "architectural features like time-mixing and recursion modules as specified in\n",
    "the configuration. It utilizes the Hugging Face Transformers library, including\n",
    "the `Seq2SeqTrainer` API.\n",
    "\n",
    "Key features demonstrated:\n",
    "- Hypothetical Titan Model: Assumes existence of `TitanTokenizer` and\n",
    "  `TitanForConditionalGeneration` with specific config flags (`use_time_mixing`,\n",
    "  `use_recursion`).\n",
    "- Advanced Training Techniques: Includes Gradient Checkpointing, Mixed Precision\n",
    "  Training (FP16), Label Smoothing, and Beam Search during evaluation.\n",
    "- Standard Workflow: Follows a common pattern of data loading, preprocessing,\n",
    "  tokenization, model configuration, training, and evaluation.\n",
    "- Exact Match Accuracy: Uses sequence-level exact match as the evaluation metric.\n",
    "\n",
    "Workflow:\n",
    "1. Load pre-split data (train, validation, test) from JSONL files.\n",
    "2. Reconstruct source (input amplitude) and target (squared amplitude) strings.\n",
    "3. Initialize the Titan tokenizer and model.\n",
    "4. Configure Titan-specific features (time-mixing, recursion) and enable\n",
    "   gradient checkpointing on the model instance.\n",
    "5. Define a function to tokenize source/target string pairs.\n",
    "6. Create PyTorch Dataset objects from the tokenized data.\n",
    "7. Instantiate `DataCollatorForSeq2Seq` for dynamic batching.\n",
    "8. Define the `compute_metrics` function for evaluation using decoded sequences.\n",
    "9. Configure `Seq2SeqTrainingArguments` with hyperparameters and features.\n",
    "10. Initialize and run the `Seq2SeqTrainer`.\n",
    "11. Evaluate the final model on the test set.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "# --- Hypothetical Titan Model Imports ---\n",
    "# Ensure these classes exist in your transformers installation or define placeholders\n",
    "try:\n",
    "    from transformers import (\n",
    "        TitanTokenizer,                # Hypothetical Tokenizer\n",
    "        TitanForConditionalGeneration, # Hypothetical Model\n",
    "        DataCollatorForSeq2Seq,\n",
    "        Seq2SeqTrainer,\n",
    "        Seq2SeqTrainingArguments,\n",
    "        TrainingArguments           # Explicit import\n",
    "    )\n",
    "    print(\"[INFO] Successfully imported hypothetical Titan classes.\")\n",
    "except ImportError:\n",
    "    print(\"[ERROR] Failed to import TitanTokenizer or TitanForConditionalGeneration.\", file=sys.stderr)\n",
    "    print(\"Please ensure these classes are defined in your transformers library or environment.\", file=sys.stderr)\n",
    "    # Define dummy classes if needed for script structure analysis:\n",
    "    # class TitanTokenizer: @staticmethod def from_pretrained(s): raise NotImplementedError\n",
    "    # class TitanForConditionalGeneration: @staticmethod def from_pretrained(s): raise NotImplementedError\n",
    "    # from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments, TrainingArguments\n",
    "    sys.exit(1) # Exit if the core classes are missing\n",
    "\n",
    "from tqdm.auto import tqdm # Optional progress bars\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# File Paths\n",
    "TRAIN_FILE = Path('qed_expressions_train.jsonl') # Use consistent naming if applicable\n",
    "VAL_FILE   = Path('qed_expressions_val.jsonl')\n",
    "TEST_FILE  = Path('qed_expressions_test.jsonl')\n",
    "OUTPUT_DIR = Path('titan_qed_squared_amplitude_output')\n",
    "\n",
    "# Model Configuration\n",
    "# Replace with actual model ID if 'google/titan-small' is a placeholder\n",
    "MODEL_ID = \"google/titan-small\" # Hypothetical Titan model identifier\n",
    "TOKENIZER_ID = \"google/titan-small\" # Usually same as model ID\n",
    "\n",
    "# Titan-Specific Configuration (Hypothetical)\n",
    "USE_TIME_MIXING = True\n",
    "USE_RECURSION   = True\n",
    "\n",
    "# Tokenizer and Data Processing Configuration\n",
    "MAX_SEQ_LENGTH = 512 # Max sequence length\n",
    "\n",
    "# Training Hyperparameters\n",
    "NUM_TRAIN_EPOCHS = 4 # Adjust based on convergence\n",
    "# Adjust batch size based on Titan model size and GPU memory\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 4\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_STEPS = 200\n",
    "LOGGING_STEPS = 50\n",
    "EVALUATION_STRATEGY = \"epoch\"\n",
    "SAVE_STRATEGY = \"epoch\"\n",
    "\n",
    "# Advanced Training Feature Flags/Values\n",
    "USE_FP16 = torch.cuda.is_available()\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "LABEL_SMOOTHING_FACTOR = 0.1\n",
    "\n",
    "# Generation Configuration (for evaluation)\n",
    "GENERATION_NUM_BEAMS = 4\n",
    "\n",
    "# --- Helper Functions and Classes ---\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads data from a JSON Lines file, handling basic errors.\"\"\"\n",
    "    data = []\n",
    "    file_path = Path(file_path)\n",
    "    print(f\"[INFO] Loading data from: {file_path}\")\n",
    "    if not file_path.is_file():\n",
    "        print(f\"[Error] Data file not found: {file_path}\", file=sys.stderr)\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"[Warning] Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "                        continue\n",
    "        print(f\"[INFO] Successfully loaded {len(data)} records.\")\n",
    "        if not data:\n",
    "             print(f\"[Warning] No valid records loaded from {file_path}.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load data from {file_path}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "\n",
    "def convert_tokens_to_strings(raw_data_list):\n",
    "    \"\"\"Converts lists of tokens from loaded data into source and target strings.\"\"\"\n",
    "    source_strings = []\n",
    "    target_strings = []\n",
    "    if not raw_data_list:\n",
    "        return source_strings, target_strings\n",
    "\n",
    "    skipped_count = 0\n",
    "    for i, item in enumerate(raw_data_list):\n",
    "        input_toks = item.get('input_tokens')\n",
    "        target_toks = item.get('target_tokens')\n",
    "\n",
    "        if isinstance(input_toks, list) and isinstance(target_toks, list):\n",
    "            source_strings.append(\" \".join(map(str, input_toks)))\n",
    "            target_strings.append(\" \".join(map(str, target_toks)))\n",
    "        else:\n",
    "            print(f\"[Warning] Skipping item at index {i} due to missing/invalid keys or non-list values: {item}\")\n",
    "            skipped_count += 1\n",
    "\n",
    "    print(f\"[INFO] Converted {len(source_strings)} items to strings (skipped {skipped_count}).\")\n",
    "    if not source_strings:\n",
    "        print(\"[Warning] No items successfully converted to strings.\")\n",
    "    return source_strings, target_strings\n",
    "\n",
    "\n",
    "def encode_sequences(tokenizer, source_texts, target_texts, max_len):\n",
    "    \"\"\"\n",
    "    Tokenizes source and target text sequences using the provided tokenizer.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Initialized Hugging Face tokenizer instance (e.g., TitanTokenizer).\n",
    "        source_texts (list[str]): List of source sequences.\n",
    "        target_texts (list[str]): List of target sequences for labels.\n",
    "        max_len (int): Maximum sequence length for padding and truncation.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing lists of 'input_ids', 'attention_mask', and 'labels'.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Encoding sequence pairs with max_length={max_len}...\")\n",
    "\n",
    "    # Tokenize source texts\n",
    "    encoder_inputs = tokenizer(\n",
    "        source_texts,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=None # Return lists\n",
    "    )\n",
    "\n",
    "    # Tokenize target texts to create labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        decoder_labels = tokenizer(\n",
    "            target_texts,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "    # Assign target input_ids as 'labels'\n",
    "    encoder_inputs['labels'] = decoder_labels['input_ids']\n",
    "\n",
    "    print(f\"[INFO] Encoding complete.\")\n",
    "    # Basic validation\n",
    "    if not encoder_inputs.get('input_ids') or not encoder_inputs.get('labels'):\n",
    "         print(\"[Warning] Encoding resulted in empty input_ids or labels.\")\n",
    "    elif len(encoder_inputs['input_ids']) != len(encoder_inputs['labels']):\n",
    "         print(\"[Warning] Mismatch in length between encoded inputs and labels.\")\n",
    "\n",
    "    return encoder_inputs\n",
    "\n",
    "\n",
    "class SequencePairDataset(Dataset):\n",
    "    \"\"\"Simple PyTorch Dataset for holding tokenized sequence pair data (as lists).\"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encodings (dict): Dictionary from tokenizer {'input_ids': [...], ...}.\n",
    "                              Values should be lists of token IDs or masks.\n",
    "        \"\"\"\n",
    "        if not isinstance(encodings, dict) or 'input_ids' not in encodings:\n",
    "            raise ValueError(\"Encodings must be a dictionary containing at least 'input_ids'.\")\n",
    "        self.encodings = encodings\n",
    "        try:\n",
    "            self.length = len(encodings['input_ids'])\n",
    "            for key in encodings:\n",
    "                if not isinstance(encodings[key], list) or len(encodings[key]) != self.length:\n",
    "                    raise ValueError(f\"Encoding key '{key}' is not a list or has inconsistent length.\")\n",
    "        except Exception as e:\n",
    "             raise ValueError(f\"Failed to validate encodings: {e}\")\n",
    "\n",
    "        if self.length == 0:\n",
    "            raise ValueError(\"Input encodings are empty.\")\n",
    "        print(f\"[INFO] Created Dataset with {self.length} examples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieves the tokenized data (as lists) for a single sample index.\"\"\"\n",
    "        if not 0 <= idx < self.length:\n",
    "             raise IndexError(f\"Index {idx} out of bounds for dataset of length {self.length}.\")\n",
    "        try:\n",
    "            return {key: self.encodings[key][idx] for key in self.encodings}\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to retrieve item at index {idx}: {e}\", file=sys.stderr)\n",
    "            raise\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Orchestrates the Titan model fine-tuning and evaluation pipeline.\"\"\"\n",
    "    print(\"[INFO] Starting Hypothetical Titan Model Fine-tuning Script...\")\n",
    "    try:\n",
    "        current_time_str = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
    "    except Exception:\n",
    "        current_time_str = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC (naive)')\n",
    "    print(f\"[INFO] Current time: {current_time_str} (local: San Diego, CA)\") # Include location context\n",
    "    print(f\"[INFO] Using model: {MODEL_ID} with Tokenizer: {TOKENIZER_ID}\")\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    try:\n",
    "        train_raw_data = load_jsonl(TRAIN_FILE)\n",
    "        val_raw_data = load_jsonl(VAL_FILE)\n",
    "        test_raw_data = load_jsonl(TEST_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Critical error during data loading: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not train_raw_data or not val_raw_data or not test_raw_data:\n",
    "        print(\"[FATAL] One or more required datasets are empty or failed to load. Exiting.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 2. Prepare Data (Tokens to Strings) ---\n",
    "    train_sources, train_targets = convert_tokens_to_strings(train_raw_data)\n",
    "    val_sources,   val_targets   = convert_tokens_to_strings(val_raw_data)\n",
    "    test_sources,  test_targets  = convert_tokens_to_strings(test_raw_data)\n",
    "\n",
    "    if not train_sources or not val_sources or not test_sources:\n",
    "        print(\"[FATAL] Data conversion resulted in empty lists. Check input data format. Exiting.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 3. Initialize Tokenizer ---\n",
    "    print(f\"[INFO] Initializing Tokenizer: {TOKENIZER_ID}\")\n",
    "    try:\n",
    "        # Assuming TitanTokenizer exists and works like standard tokenizers\n",
    "        tokenizer = TitanTokenizer.from_pretrained(TOKENIZER_ID)\n",
    "        # Add checks/additions for special tokens if necessary for Titan\n",
    "        if tokenizer.pad_token is None: tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "        # Determine start/end tokens based on tokenizer properties or model requirements\n",
    "        if tokenizer.bos_token is None: tokenizer.add_special_tokens({'bos_token': '<s>'})\n",
    "        if tokenizer.eos_token is None: tokenizer.add_special_tokens({'eos_token': '</s>'})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed to initialize tokenizer '{TOKENIZER_ID}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Store globally for metric computation\n",
    "    global tokenizer_for_metrics\n",
    "    tokenizer_for_metrics = tokenizer\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    print(f\"[INFO] Tokenizer Pad Token ID: {pad_token_id}\")\n",
    "    print(f\"[INFO] Tokenizer Vocab Size (initial): {tokenizer.vocab_size}\")\n",
    "\n",
    "\n",
    "    # --- 4. Initialize Model ---\n",
    "    print(f\"[INFO] Initializing Model: {MODEL_ID}\")\n",
    "    try:\n",
    "        model = TitanForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "\n",
    "        # Resize embeddings if vocab size changed\n",
    "        if model.config.vocab_size != len(tokenizer):\n",
    "             print(f\"[INFO] Resizing model embeddings from {model.config.vocab_size} to {len(tokenizer)}\")\n",
    "             model.resize_token_embeddings(len(tokenizer))\n",
    "             model.config.vocab_size = len(tokenizer) # Ensure config is updated\n",
    "\n",
    "        # Configure hypothetical Titan features - use try/except for robustness\n",
    "        config_updated = False\n",
    "        try:\n",
    "            if USE_TIME_MIXING and hasattr(model.config, 'use_time_mixing'):\n",
    "                model.config.use_time_mixing = True\n",
    "                print(\"[INFO] Enabled model.config.use_time_mixing\")\n",
    "                config_updated = True\n",
    "            elif USE_TIME_MIXING:\n",
    "                 print(\"[Warning] Configured USE_TIME_MIXING=True, but 'use_time_mixing' not found in model config.\")\n",
    "\n",
    "            if USE_RECURSION and hasattr(model.config, 'use_recursion'):\n",
    "                model.config.use_recursion = True\n",
    "                print(\"[INFO] Enabled model.config.use_recursion\")\n",
    "                config_updated = True\n",
    "            elif USE_RECURSION:\n",
    "                 print(\"[Warning] Configured USE_RECURSION=True, but 'use_recursion' not found in model config.\")\n",
    "\n",
    "        except Exception as config_e:\n",
    "             print(f\"[Warning] Error applying hypothetical Titan config options: {config_e}\")\n",
    "\n",
    "        # Configure standard seq2seq settings\n",
    "        model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        # Set generation defaults\n",
    "        model.config.max_length = MAX_SEQ_LENGTH\n",
    "        model.config.num_beams = GENERATION_NUM_BEAMS\n",
    "\n",
    "        # Enable Gradient Checkpointing if configured\n",
    "        if USE_GRADIENT_CHECKPOINTING:\n",
    "            try:\n",
    "                 model.gradient_checkpointing_enable()\n",
    "                 print(\"[INFO] Gradient Checkpointing enabled on the model.\")\n",
    "            except Exception as gc_e:\n",
    "                 print(f\"[Warning] Failed to enable gradient checkpointing on model: {gc_e}\")\n",
    "                 # Ensure training arg reflects this failure\n",
    "                 global USE_GRADIENT_CHECKPOINTING_EFFECTIVE\n",
    "                 USE_GRADIENT_CHECKPOINTING_EFFECTIVE = False\n",
    "        else:\n",
    "            USE_GRADIENT_CHECKPOINTING_EFFECTIVE = False\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed to initialize or configure the Titan model '{MODEL_ID}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 5. Tokenize Data ---\n",
    "    try:\n",
    "        train_encodings = encode_sequences(tokenizer, train_sources, train_targets, MAX_SEQ_LENGTH)\n",
    "        val_encodings   = encode_sequences(tokenizer, val_sources,   val_targets,   MAX_SEQ_LENGTH)\n",
    "        test_encodings  = encode_sequences(tokenizer, test_sources,  test_targets,  MAX_SEQ_LENGTH)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed during data tokenization: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not train_encodings.get('input_ids') or not val_encodings.get('input_ids') or not test_encodings.get('input_ids'):\n",
    "         print(\"[FATAL] Tokenization resulted in empty encodings for one or more splits. Exiting.\", file=sys.stderr)\n",
    "         sys.exit(1)\n",
    "\n",
    "    # --- 6. Create Datasets ---\n",
    "    try:\n",
    "        train_dataset = SequencePairDataset(train_encodings)\n",
    "        val_dataset   = SequencePairDataset(val_encodings)\n",
    "        test_dataset  = SequencePairDataset(test_encodings)\n",
    "    except ValueError as e:\n",
    "        print(f\"[FATAL] Failed to create Dataset objects: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 7. Initialize Data Collator ---\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100, # Ignore pad tokens in loss\n",
    "        pad_to_multiple_of=8 if USE_FP16 else None # Optimize padding for FP16\n",
    "    )\n",
    "    print(\"[INFO] Data collator initialized.\")\n",
    "\n",
    "    # --- 8. Define Metrics Computation ---\n",
    "    def compute_metrics_fn(eval_pred):\n",
    "        \"\"\"Calculates exact sequence match accuracy after decoding.\"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        if not isinstance(predictions, np.ndarray): predictions = np.array(predictions)\n",
    "        if not isinstance(labels, np.ndarray): labels = np.array(labels)\n",
    "\n",
    "        # Replace -100 with pad_token_id for decoding\n",
    "        labels = np.where(labels != -100, labels, tokenizer_for_metrics.pad_token_id)\n",
    "\n",
    "        try:\n",
    "            decoded_preds = tokenizer_for_metrics.batch_decode(predictions, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer_for_metrics.batch_decode(labels, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "             print(f\"[Error] Decoding failed in compute_metrics: {e}\", file=sys.stderr)\n",
    "             return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "        # Post-process and compare\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "        if len(decoded_preds) != len(decoded_labels):\n",
    "            print(f\"[Warning] Mismatch prediction/label count: {len(decoded_preds)} vs {len(decoded_labels)}\", file=sys.stderr)\n",
    "            return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "        matches = [pred == label for pred, label in zip(decoded_preds, decoded_labels)]\n",
    "        accuracy = np.mean(matches) if matches else 0.0\n",
    "        return {\"sequence_accuracy\": float(accuracy)}\n",
    "\n",
    "    # --- 9. Define Training Arguments ---\n",
    "    # Use effective GC flag based on earlier attempt\n",
    "    effective_gc = USE_GRADIENT_CHECKPOINTING if 'USE_GRADIENT_CHECKPOINTING_EFFECTIVE' not in globals() else USE_GRADIENT_CHECKPOINTING_EFFECTIVE\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR),\n",
    "        # Schedule\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        # Logging / Saving / Evaluation\n",
    "        logging_dir=str(OUTPUT_DIR / 'logs'),\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        evaluation_strategy=EVALUATION_STRATEGY,\n",
    "        save_strategy=SAVE_STRATEGY,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"sequence_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        # Generation\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=MAX_SEQ_LENGTH,\n",
    "        generation_num_beams=GENERATION_NUM_BEAMS,\n",
    "        # Advanced features\n",
    "        fp16=USE_FP16,\n",
    "        label_smoothing_factor=LABEL_SMOOTHING_FACTOR,\n",
    "        gradient_checkpointing=effective_gc, # Use the effective flag\n",
    "        # report_to=\"tensorboard\", # Optional\n",
    "    )\n",
    "    print(\"[INFO] Training arguments defined.\")\n",
    "    print(f\"[INFO] Effective Mixed Precision (FP16): {'Enabled' if USE_FP16 else 'Disabled'}\")\n",
    "    print(f\"[INFO] Effective Label Smoothing Factor: {LABEL_SMOOTHING_FACTOR}\")\n",
    "    print(f\"[INFO] Effective Gradient Checkpointing in Args: {training_args.gradient_checkpointing}\")\n",
    "\n",
    "\n",
    "    # --- 10. Initialize Trainer ---\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "    )\n",
    "    print(\"[INFO] Seq2SeqTrainer initialized.\")\n",
    "\n",
    "    # --- 11. Train the Model ---\n",
    "    print(f\"[INFO] Starting model training for {NUM_TRAIN_EPOCHS} epochs...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(\"[INFO] Training finished successfully.\")\n",
    "        if trainer.state.best_model_checkpoint:\n",
    "             print(f\"[INFO] Best model checkpoint saved at: {trainer.state.best_model_checkpoint}\")\n",
    "        else:\n",
    "             print(\"[Warning] No best model checkpoint recorded.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Training loop encountered an error: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 12. Evaluate on Test Set ---\n",
    "    print(\"[INFO] Evaluating final model on the test set...\")\n",
    "    try:\n",
    "        test_results = trainer.evaluate(\n",
    "            eval_dataset=test_dataset,\n",
    "            metric_key_prefix=\"test\"\n",
    "        )\n",
    "        trainer.log_metrics(\"test\", test_results)\n",
    "        trainer.save_metrics(\"test\", test_results)\n",
    "\n",
    "        if 'test_sequence_accuracy' in test_results:\n",
    "             print(f\"\\n--- Test Set Results ---\")\n",
    "             print(f\"Test Sequence Accuracy: {test_results['test_sequence_accuracy']:.4f}\")\n",
    "             print(f\"------------------------\")\n",
    "        else:\n",
    "             print(\"[Warning] 'test_sequence_accuracy' not found in test results.\")\n",
    "             print(\"Full test results:\", test_results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Evaluation on test set failed: {e}\", file=sys.stderr)\n",
    "        sys.exit(1) # Exit with error status\n",
    "\n",
    "    print(\"[INFO] Script finished successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Optional: Add argument parsing (argparse)\n",
    "    # Optional: Set random seeds\n",
    "    # SEED = 42; torch.manual_seed(SEED); np.random.seed(SEED); import random; random.seed(SEED)\n",
    "    # if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5: Evolutionary and Transformer Models for Symbolic Regression\n",
    "Model: Transformer model integrated with an evolutionary pipeline. It’s possible to start from previous year’s projects but should introduce a substantial innovation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 56\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     49\u001b[0m     AutoTokenizer,\n\u001b[1;32m     50\u001b[0m     EncoderDecoderModel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     TrainingArguments \u001b[38;5;66;03m# Explicit import\u001b[39;00m\n\u001b[1;32m     55\u001b[0m )\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m base, creator, tools, algorithms\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;66;03m# Optional, useful if adding manual loops or progress bars later\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# --- Configuration ---\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# File Paths\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Ensure these point to your actual preprocessed data files\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deap'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Evolutionary Hyperparameter Optimization for Transformer-Based Symbolic Regression.\n",
    "\n",
    "This script employs a Genetic Algorithm (GA), using the DEAP library, to optimize\n",
    "key hyperparameters (learning rate and dropout probability) for a standard\n",
    "Transformer Encoder-Decoder model (BERT-to-BERT) applied to a symbolic regression\n",
    "task using the QED 2→2 dataset.\n",
    "\n",
    "The optimization process works as follows:\n",
    "1.  A small, fixed subset of the training and validation data is selected for\n",
    "    rapid evaluation during the GA.\n",
    "2.  DEAP initializes a population of candidate hyperparameter sets (individuals).\n",
    "3.  Each individual is evaluated by:\n",
    "    a. Initializing a fresh BERT-to-BERT `EncoderDecoderModel`.\n",
    "    b. Configuring the model with the dropout specified by the individual.\n",
    "    c. Setting up `Seq2SeqTrainingArguments` with the learning rate from the\n",
    "       individual and settings suitable for a short training run.\n",
    "    d. Initializing a `Seq2SeqTrainer`.\n",
    "    e. Training the model for a fixed, small number of steps/epochs on the\n",
    "       training subset.\n",
    "    f. Evaluating the trained model on the validation subset using sequence\n",
    "       accuracy.\n",
    "    g. Returning the validation accuracy as the individual's fitness.\n",
    "4.  DEAP applies genetic operators (selection, crossover, mutation) to evolve\n",
    "    the population over several generations, aiming to maximize fitness\n",
    "    (validation accuracy).\n",
    "5.  After the GA completes, the best hyperparameter set found is identified.\n",
    "6.  A final `EncoderDecoderModel` is initialized.\n",
    "7.  A full training run is performed using the best hyperparameters found by the\n",
    "    GA, training on the complete training dataset and validating on the complete\n",
    "    validation dataset.\n",
    "8.  The final performance is reported based on evaluation on the held-out test set.\n",
    "\n",
    "Note: This script assumes the `deap` library is installed (`pip install deap`).\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    EncoderDecoderModel,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainingArguments # Explicit import\n",
    ")\n",
    "from deap import base, creator, tools, algorithms\n",
    "from tqdm.auto import tqdm # Optional, useful if adding manual loops or progress bars later\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# File Paths\n",
    "# Ensure these point to your actual preprocessed data files\n",
    "TRAIN_FILE = Path('qed_expressions_train.jsonl') # Adjust prefix if needed\n",
    "VAL_FILE   = Path('qed_expressions_val.jsonl')\n",
    "TEST_FILE  = Path('qed_expressions_test.jsonl')\n",
    "OUTPUT_DIR = Path('ga_transformer_symbolic_regression_output') # Main output directory\n",
    "GA_LOG_FILE = OUTPUT_DIR / \"ga_logbook.json\"                   # Log of GA progress\n",
    "BEST_HP_FILE = OUTPUT_DIR / \"best_hyperparameters.json\"         # Best HPs found\n",
    "GA_EVAL_OUTPUT_DIR = OUTPUT_DIR / \"ga_eval_runs\" # Temp dir for GA evaluation runs\n",
    "\n",
    "# Model and Tokenizer Configuration\n",
    "MODEL_ID = \"bert-base-uncased\" # Base model for Encoder-Decoder\n",
    "TOKENIZER_ID = \"bert-base-uncased\"\n",
    "MAX_SEQ_LENGTH = 128          # Max sequence length for tokenization\n",
    "\n",
    "# Genetic Algorithm Configuration (DEAP)\n",
    "POPULATION_SIZE = 20        # Number of HP sets evaluated per generation\n",
    "N_GENERATIONS = 10          # Total number of GA generations\n",
    "CX_PROB = 0.7               # Probability of crossover between individuals\n",
    "MUT_PROB = 0.2              # Probability of mutating an individual\n",
    "# Mutation parameters for Gaussian perturbation\n",
    "MUT_SIGMA_LR = 1e-5         # Std deviation for learning rate mutation\n",
    "MUT_SIGMA_DROPOUT = 0.05    # Std deviation for dropout mutation\n",
    "# Boundaries for hyperparameter values\n",
    "LR_BOUND_LOW = 1e-6\n",
    "LR_BOUND_HIGH = 1e-3\n",
    "DROPOUT_BOUND_LOW = 0.0\n",
    "DROPOUT_BOUND_HIGH = 0.5\n",
    "TOURNAMENT_SIZE = 3         # Selection pressure for tournament selection\n",
    "\n",
    "# GA Evaluation Run Configuration (Short runs on subsets for speed)\n",
    "GA_EVAL_TRAIN_SUBSET_SIZE = 512 # Number of training examples per GA eval run\n",
    "GA_EVAL_VAL_SUBSET_SIZE = 128   # Number of validation examples per GA eval run\n",
    "GA_EVAL_EPOCHS = 1              # Epochs per GA eval run (keep very small)\n",
    "# GA_EVAL_MAX_STEPS = 200       # Alternative: Fixed steps per GA eval run\n",
    "GA_EVAL_BATCH_SIZE = 16         # Batch size for GA eval runs\n",
    "\n",
    "# Final Training Configuration (using best HPs on full data)\n",
    "FINAL_TRAIN_EPOCHS = 5\n",
    "FINAL_BATCH_SIZE = 16\n",
    "FINAL_WEIGHT_DECAY = 0.01\n",
    "FINAL_WARMUP_STEPS = 300\n",
    "FINAL_LOGGING_STEPS = 50\n",
    "FINAL_USE_FP16 = torch.cuda.is_available() # Use FP16 if available for final run\n",
    "FINAL_LABEL_SMOOTHING = 0.1     # Optional label smoothing for final run\n",
    "FINAL_SAVE_TOTAL_LIMIT = 2      # Keep only the best and latest checkpoints\n",
    "\n",
    "# --- Helper Functions and Classes ---\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads data from a JSON Lines file, handling basic errors.\"\"\"\n",
    "    data = []\n",
    "    file_path = Path(file_path)\n",
    "    print(f\"[INFO] Loading data from: {file_path}\")\n",
    "    if not file_path.is_file():\n",
    "        print(f\"[Error] Data file not found: {file_path}\", file=sys.stderr)\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try: data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e: print(f\"[Warning] Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "        print(f\"[INFO] Successfully loaded {len(data)} records.\")\n",
    "        if not data: print(f\"[Warning] No valid records loaded from {file_path}.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load data from {file_path}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "def convert_tokens_to_strings(raw_data_list):\n",
    "    \"\"\"Converts lists of tokens from loaded data into source and target strings.\"\"\"\n",
    "    source_strings, target_strings, skipped_count = [], [], 0\n",
    "    if not raw_data_list: return source_strings, target_strings\n",
    "    for i, item in enumerate(raw_data_list):\n",
    "        input_toks, target_toks = item.get('input_tokens'), item.get('target_tokens')\n",
    "        if isinstance(input_toks, list) and isinstance(target_toks, list):\n",
    "            source_strings.append(\" \".join(map(str, input_toks)))\n",
    "            target_strings.append(\" \".join(map(str, target_toks)))\n",
    "        else:\n",
    "            print(f\"[Warning] Skipping item at index {i} due to missing/invalid data: {item}\"); skipped_count += 1\n",
    "    print(f\"[INFO] Converted {len(source_strings)} items to strings (skipped {skipped_count}).\")\n",
    "    if not source_strings: print(\"[Warning] No items successfully converted.\")\n",
    "    return source_strings, target_strings\n",
    "\n",
    "def encode_sequences(tokenizer, source_texts, target_texts, max_len):\n",
    "    \"\"\"Tokenizes source and target sequences, returning lists of IDs/masks.\"\"\"\n",
    "    print(f\"[INFO] Encoding sequence pairs with max_length={max_len}...\")\n",
    "    encoder_inputs = tokenizer(source_texts, max_length=max_len, padding='max_length', truncation=True, return_tensors=None)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        decoder_labels = tokenizer(target_texts, max_length=max_len, padding='max_length', truncation=True, return_tensors=None)\n",
    "    encoder_inputs['labels'] = decoder_labels['input_ids']\n",
    "    print(f\"[INFO] Encoding complete.\")\n",
    "    if not encoder_inputs.get('input_ids') or not encoder_inputs.get('labels') or len(encoder_inputs['input_ids']) != len(encoder_inputs['labels']):\n",
    "         print(\"[Warning] Encoding resulted in empty lists or length mismatch.\")\n",
    "    return encoder_inputs\n",
    "\n",
    "class SequencePairDataset(Dataset):\n",
    "    \"\"\"Simple PyTorch Dataset for holding tokenized sequence pair data (as lists).\"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        if not isinstance(encodings, dict) or 'input_ids' not in encodings: raise ValueError(\"Invalid encodings format.\")\n",
    "        self.encodings = encodings\n",
    "        try:\n",
    "            self.length = len(encodings['input_ids'])\n",
    "            for key in encodings:\n",
    "                 if not isinstance(encodings[key], list) or len(encodings[key]) != self.length: raise ValueError(f\"Inconsistent length for key '{key}'.\")\n",
    "        except Exception as e: raise ValueError(f\"Validation failed: {e}\")\n",
    "        if self.length == 0: raise ValueError(\"Input encodings are empty.\")\n",
    "        print(f\"[INFO] Created Dataset with {self.length} examples.\")\n",
    "\n",
    "    def __len__(self): return self.length\n",
    "    def __getitem__(self, idx):\n",
    "        if not 0 <= idx < self.length: raise IndexError(f\"Index {idx} out of bounds.\")\n",
    "        try: return {key: self.encodings[key][idx] for key in self.encodings}\n",
    "        except Exception as e: print(f\"[Error] Failed retrieval at index {idx}: {e}\", file=sys.stderr); raise\n",
    "\n",
    "# --- Global variables needed by evaluation function ---\n",
    "# These are populated in main() before the GA starts\n",
    "ga_train_dataset_subset = None\n",
    "ga_val_dataset_subset = None\n",
    "tokenizer_for_eval = None\n",
    "data_collator_for_eval = None\n",
    "compute_metrics_internal = None # Holds the metric calculation logic\n",
    "\n",
    "# --- DEAP Evaluation Function ---\n",
    "\n",
    "def evaluate_hyperparams(individual):\n",
    "    \"\"\"\n",
    "    Evaluates a hyperparameter set [learning_rate, dropout_rate] by training\n",
    "    a small model on data subsets and returning validation accuracy.\n",
    "\n",
    "    Args:\n",
    "        individual (deap.creator.Individual): List containing [LR, Dropout].\n",
    "\n",
    "    Returns:\n",
    "        tuple: Fitness value (validation_accuracy,). Must be a tuple.\n",
    "    \"\"\"\n",
    "    learning_rate, dropout_rate = individual[0], individual[1]\n",
    "    dropout_rate = max(DROPOUT_BOUND_LOW, min(DROPOUT_BOUND_HIGH, dropout_rate)) # Clamp dropout\n",
    "\n",
    "    run_id = f\"lr_{learning_rate:.1e}_drop_{dropout_rate:.3f}_{random.randint(1000,9999)}\"\n",
    "    run_output_dir = GA_EVAL_OUTPUT_DIR / run_id\n",
    "\n",
    "    print(f\"[GA Eval] Evaluating LR={learning_rate:.3e}, Dropout={dropout_rate:.4f}\")\n",
    "\n",
    "    try:\n",
    "        # 1. Initialize NEW model instance\n",
    "        model = EncoderDecoderModel.from_encoder_decoder_pretrained(MODEL_ID, MODEL_ID)\n",
    "        if model.config.encoder.vocab_size != len(tokenizer_for_eval):\n",
    "            model.resize_token_embeddings(len(tokenizer_for_eval))\n",
    "            model.config.encoder.vocab_size = len(tokenizer_for_eval)\n",
    "            model.config.decoder.vocab_size = len(tokenizer_for_eval)\n",
    "\n",
    "        # 2. Apply dropout from individual\n",
    "        model.config.dropout = dropout_rate\n",
    "        model.config.attention_dropout = dropout_rate\n",
    "        if hasattr(model.config, 'encoder'): model.config.encoder.dropout = dropout_rate; model.config.encoder.attention_dropout = dropout_rate\n",
    "        if hasattr(model.config, 'decoder'): model.config.decoder.dropout = dropout_rate; model.config.decoder.attention_dropout = dropout_rate\n",
    "\n",
    "        # 3. Configure standard seq2seq settings\n",
    "        model.config.decoder_start_token_id = tokenizer_for_eval.cls_token_id\n",
    "        model.config.eos_token_id = tokenizer_for_eval.sep_token_id\n",
    "        model.config.pad_token_id = tokenizer_for_eval.pad_token_id\n",
    "        model.config.max_length = MAX_SEQ_LENGTH\n",
    "\n",
    "        # 4. Minimal Training Arguments for GA eval run\n",
    "        eval_training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=str(run_output_dir),\n",
    "            num_train_epochs=GA_EVAL_EPOCHS,\n",
    "            # max_steps=GA_EVAL_MAX_STEPS, # Alternative\n",
    "            per_device_train_batch_size=GA_EVAL_BATCH_SIZE,\n",
    "            per_device_eval_batch_size=GA_EVAL_BATCH_SIZE,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=0.0, # Keep simple for GA eval\n",
    "            logging_steps=1000, # Reduce logging noise\n",
    "            evaluation_strategy=\"no\",\n",
    "            save_strategy=\"no\",\n",
    "            predict_with_generate=True, # Use generation for accuracy\n",
    "            generation_max_length=MAX_SEQ_LENGTH,\n",
    "            generation_num_beams=1, # Greedy search for speed\n",
    "            fp16=FINAL_USE_FP16, # Consistent FP16 usage\n",
    "            report_to=\"none\",\n",
    "            disable_tqdm=True,\n",
    "        )\n",
    "\n",
    "        # 5. Initialize Trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=eval_training_args,\n",
    "            train_dataset=ga_train_dataset_subset,\n",
    "            eval_dataset=ga_val_dataset_subset,\n",
    "            data_collator=data_collator_for_eval,\n",
    "            tokenizer=tokenizer_for_eval,\n",
    "            compute_metrics=compute_metrics_internal # Use the predefined metric logic\n",
    "        )\n",
    "\n",
    "        # 6. Train briefly\n",
    "        trainer.train()\n",
    "\n",
    "        # 7. Evaluate\n",
    "        eval_results = trainer.evaluate(eval_dataset=ga_val_dataset_subset)\n",
    "        accuracy = eval_results.get(\"eval_sequence_accuracy\", 0.0)\n",
    "\n",
    "        print(f\"[GA Eval Result] LR={learning_rate:.3e}, Dropout={dropout_rate:.4f} -> Val Acc={accuracy:.4f}\")\n",
    "        # Optional: Clean up temp dir: import shutil; shutil.rmtree(run_output_dir)\n",
    "\n",
    "        return (accuracy,) # Return fitness tuple\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[GA Eval Error] Failed for LR={learning_rate:.3e}, Dropout={dropout_rate:.4f}: {e}\", file=sys.stderr)\n",
    "        return (0.0,) # Return poor fitness on failure\n",
    "\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Orchestrates data loading, GA optimization, and final model training.\"\"\"\n",
    "    print(\"[INFO] Starting Evolutionary Hyperparameter Optimization Script...\")\n",
    "    # --- ***** Update Timestamp/Location ***** ---\n",
    "    try:\n",
    "        # Attempt to get timezone-aware UTC time\n",
    "        current_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "        current_time_str = current_time.strftime('%Y-%m-%d %H:%M:%S %Z') # Includes timezone name\n",
    "    except Exception:\n",
    "        # Fallback to naive UTC time\n",
    "        current_time = datetime.datetime.utcnow()\n",
    "        current_time_str = current_time.strftime('%Y-%m-%d %H:%M:%S UTC (naive)')\n",
    "    # Incorporate provided context\n",
    "    print(f\"[INFO] Current time: {current_time_str}\")\n",
    "    print(f\"[INFO] Location context: San Diego, CA, USA\")\n",
    "    print(f\"[INFO] Using Base Model: {MODEL_ID}\")\n",
    "\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    GA_EVAL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    try:\n",
    "        train_raw = load_jsonl(TRAIN_FILE)\n",
    "        val_raw = load_jsonl(VAL_FILE)\n",
    "        test_raw = load_jsonl(TEST_FILE)\n",
    "    except Exception as e: print(f\"[FATAL] Data loading failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "    if not train_raw or not val_raw or not test_raw: print(\"[FATAL] Datasets empty after loading.\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 2. Prepare Data Strings ---\n",
    "    train_src, train_tgt = convert_tokens_to_strings(train_raw)\n",
    "    val_src,   val_tgt   = convert_tokens_to_strings(val_raw)\n",
    "    test_src,  test_tgt  = convert_tokens_to_strings(test_raw)\n",
    "    if not train_src or not val_src or not test_src: print(\"[FATAL] Data conversion failed.\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 3. Initialize Tokenizer (Global for GA Eval) ---\n",
    "    global tokenizer_for_eval\n",
    "    try:\n",
    "        tokenizer_for_eval = AutoTokenizer.from_pretrained(TOKENIZER_ID)\n",
    "        if tokenizer_for_eval.pad_token is None: tokenizer_for_eval.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        if tokenizer_for_eval.cls_token is None: tokenizer_for_eval.add_special_tokens({'cls_token': '[CLS]'})\n",
    "        if tokenizer_for_eval.sep_token is None: tokenizer_for_eval.add_special_tokens({'sep_token': '[SEP]'})\n",
    "        pad_token_id = tokenizer_for_eval.pad_token_id\n",
    "    except Exception as e: print(f\"[FATAL] Tokenizer init failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "    print(f\"[INFO] Tokenizer initialized (Vocab: {len(tokenizer_for_eval)}, Pad ID: {pad_token_id}).\")\n",
    "\n",
    "    # --- 4. Tokenize Data & Create Subsets ---\n",
    "    try:\n",
    "        print(\"[INFO] Tokenizing full datasets...\")\n",
    "        train_enc = encode_sequences(tokenizer_for_eval, train_src, train_tgt, MAX_SEQ_LENGTH)\n",
    "        val_enc   = encode_sequences(tokenizer_for_eval, val_src,   val_tgt,   MAX_SEQ_LENGTH)\n",
    "        test_enc  = encode_sequences(tokenizer_for_eval, test_src,  test_tgt,  MAX_SEQ_LENGTH)\n",
    "\n",
    "        full_train_dataset = SequencePairDataset(train_enc)\n",
    "        full_val_dataset   = SequencePairDataset(val_enc)\n",
    "        full_test_dataset  = SequencePairDataset(test_enc)\n",
    "\n",
    "        print(f\"[INFO] Creating GA evaluation subsets (Train: {GA_EVAL_TRAIN_SUBSET_SIZE}, Val: {GA_EVAL_VAL_SUBSET_SIZE})\")\n",
    "        train_subset_indices = random.sample(range(len(full_train_dataset)), min(GA_EVAL_TRAIN_SUBSET_SIZE, len(full_train_dataset)))\n",
    "        val_subset_indices = random.sample(range(len(full_val_dataset)), min(GA_EVAL_VAL_SUBSET_SIZE, len(full_val_dataset)))\n",
    "\n",
    "        def subset_encodings(enc, indices): return {key: [enc[key][i] for i in indices] for key in enc}\n",
    "\n",
    "        global ga_train_dataset_subset, ga_val_dataset_subset\n",
    "        ga_train_dataset_subset = SequencePairDataset(subset_encodings(train_enc, train_subset_indices))\n",
    "        ga_val_dataset_subset = SequencePairDataset(subset_encodings(val_enc, val_subset_indices))\n",
    "\n",
    "    except Exception as e: print(f\"[FATAL] Data tokenization or subsetting failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 5. Setup Global Collator & Metrics Logic ---\n",
    "    global data_collator_for_eval, compute_metrics_internal\n",
    "    temp_model = EncoderDecoderModel.from_encoder_decoder_pretrained(MODEL_ID, MODEL_ID) # Need instance for collator setup\n",
    "    data_collator_for_eval = DataCollatorForSeq2Seq(tokenizer_for_eval, model=temp_model, label_pad_token_id=-100, pad_to_multiple_of=8 if FINAL_USE_FP16 else None)\n",
    "    del temp_model\n",
    "    print(\"[INFO] Data collator prepared.\")\n",
    "\n",
    "    # Define the internal metric calculation logic once\n",
    "    def compute_metrics_logic(eval_pred):\n",
    "        \"\"\"Internal logic for calculating sequence accuracy.\"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        if not isinstance(predictions, np.ndarray): predictions = np.array(predictions)\n",
    "        if not isinstance(labels, np.ndarray): labels = np.array(labels)\n",
    "        labels = np.where(labels != -100, labels, tokenizer_for_eval.pad_token_id)\n",
    "        try:\n",
    "            decoded_preds = tokenizer_for_eval.batch_decode(predictions, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer_for_eval.batch_decode(labels, skip_special_tokens=True)\n",
    "            decoded_preds = [p.strip() for p in decoded_preds]\n",
    "            decoded_labels = [l.strip() for l in decoded_labels]\n",
    "            if len(decoded_preds) != len(decoded_labels): return {\"sequence_accuracy\": 0.0}\n",
    "            matches = [p == l for p, l in zip(decoded_preds, decoded_labels)]\n",
    "            accuracy = np.mean(matches) if matches else 0.0\n",
    "            return {\"sequence_accuracy\": float(accuracy)}\n",
    "        except Exception as dec_e:\n",
    "             print(f\"[Metrics Error] Decoding failed: {dec_e}\", file=sys.stderr); return {\"sequence_accuracy\": 0.0}\n",
    "    compute_metrics_internal = compute_metrics_logic # Assign to global var\n",
    "\n",
    "    # --- 6. DEAP Genetic Algorithm Setup ---\n",
    "    print(\"[INFO] Setting up Genetic Algorithm (DEAP)...\")\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_lr\", random.uniform, LR_BOUND_LOW, LR_BOUND_HIGH)\n",
    "    toolbox.register(\"attr_dropout\", random.uniform, DROPOUT_BOUND_LOW, DROPOUT_BOUND_HIGH)\n",
    "    toolbox.register(\"individual\", tools.initCycle, creator.Individual, (toolbox.attr_lr, toolbox.attr_dropout), n=1)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "    # Ensure mutation respects boundaries - DEAP's mutGaussian doesn't inherently\n",
    "    # We handle clamping within the evaluate function, alternative is custom mutation\n",
    "    toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=[MUT_SIGMA_LR, MUT_SIGMA_DROPOUT], indpb=0.2)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=TOURNAMENT_SIZE)\n",
    "    toolbox.register(\"evaluate\", evaluate_hyperparams)\n",
    "\n",
    "    # --- 7. Run Genetic Algorithm ---\n",
    "    print(f\"[INFO] Starting GA optimization ({N_GENERATIONS} gens, Pop: {POPULATION_SIZE})...\")\n",
    "    population = toolbox.population(n=POPULATION_SIZE)\n",
    "    hall_of_fame = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean); stats.register(\"std\", np.std); stats.register(\"min\", np.min); stats.register(\"max\", np.max)\n",
    "\n",
    "    try:\n",
    "        population, logbook = algorithms.eaSimple(population, toolbox, cxpb=CX_PROB, mutpb=MUT_PROB, ngen=N_GENERATIONS, stats=stats, halloffame=hall_of_fame, verbose=True)\n",
    "        ga_success = True\n",
    "    except Exception as ga_e: print(f\"[ERROR] GA execution failed: {ga_e}\", file=sys.stderr); ga_success = False; logbook = None; best_individual = None\n",
    "\n",
    "    if logbook:\n",
    "        try:\n",
    "            with open(GA_LOG_FILE, 'w') as f: json.dump(logbook, f, indent=2)\n",
    "            print(f\"[INFO] GA logbook saved to {GA_LOG_FILE}\")\n",
    "        except Exception as log_e: print(f\"[Error] Failed to save GA logbook: {log_e}\", file=sys.stderr)\n",
    "\n",
    "    if not ga_success or not hall_of_fame: print(\"[FATAL] GA optimization failed or found no best individual. Exiting.\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    best_individual = hall_of_fame[0]\n",
    "    best_lr, best_dropout = best_individual[0], max(DROPOUT_BOUND_LOW, min(DROPOUT_BOUND_HIGH, best_individual[1])) # Clamp final best dropout\n",
    "    best_fitness = best_individual.fitness.values[0]\n",
    "\n",
    "    print(\"\\n--- GA Optimization Complete ---\")\n",
    "    print(f\"Best Individual: LR={best_lr:.3e}, Dropout={best_dropout:.4f}\")\n",
    "    print(f\"Best Validation Accuracy (on subset): {best_fitness:.4f}\")\n",
    "    print(\"----------------------------------\")\n",
    "\n",
    "    best_hp = {'learning_rate': best_lr, 'dropout_rate': best_dropout, 'validation_accuracy': best_fitness}\n",
    "    try:\n",
    "        with open(BEST_HP_FILE, 'w') as f: json.dump(best_hp, f, indent=2)\n",
    "        print(f\"[INFO] Best hyperparameters saved to {BEST_HP_FILE}\")\n",
    "    except Exception as hp_e: print(f\"[Error] Failed to save best hyperparameters: {hp_e}\", file=sys.stderr)\n",
    "\n",
    "    # --- 8. Final Training with Best Hyperparameters ---\n",
    "    print(\"\\n[INFO] Starting final model training using best hyperparameters found...\")\n",
    "    try:\n",
    "        final_model = EncoderDecoderModel.from_encoder_decoder_pretrained(MODEL_ID, MODEL_ID)\n",
    "        if final_model.config.encoder.vocab_size != len(tokenizer_for_eval):\n",
    "            final_model.resize_token_embeddings(len(tokenizer_for_eval))\n",
    "            final_model.config.encoder.vocab_size = len(tokenizer_for_eval)\n",
    "            final_model.config.decoder.vocab_size = len(tokenizer_for_eval)\n",
    "\n",
    "        final_model.config.dropout = best_dropout\n",
    "        final_model.config.attention_dropout = best_dropout\n",
    "        if hasattr(final_model.config, 'encoder'): final_model.config.encoder.dropout = best_dropout; final_model.config.encoder.attention_dropout = best_dropout\n",
    "        if hasattr(final_model.config, 'decoder'): final_model.config.decoder.dropout = best_dropout; final_model.config.decoder.attention_dropout = best_dropout\n",
    "\n",
    "        final_model.config.decoder_start_token_id = tokenizer_for_eval.cls_token_id\n",
    "        final_model.config.eos_token_id = tokenizer_for_eval.sep_token_id\n",
    "        final_model.config.pad_token_id = tokenizer_for_eval.pad_token_id\n",
    "        final_model.config.max_length = MAX_SEQ_LENGTH\n",
    "        final_model.config.num_beams = 4 # Use beam search for final evaluation\n",
    "        final_model.tie_weights()\n",
    "        # Optional final gradient checkpointing\n",
    "        # if FINAL_USE_GRADIENT_CHECKPOINTING: final_model.gradient_checkpointing_enable()\n",
    "\n",
    "    except Exception as model_e: print(f\"[FATAL] Failed to initialize final model: {model_e}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    final_training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR / \"final_model\"),\n",
    "        num_train_epochs=FINAL_TRAIN_EPOCHS,\n",
    "        per_device_train_batch_size=FINAL_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=FINAL_BATCH_SIZE,\n",
    "        learning_rate=best_lr, # Use best LR\n",
    "        weight_decay=FINAL_WEIGHT_DECAY,\n",
    "        warmup_steps=FINAL_WARMUP_STEPS,\n",
    "        logging_dir=str(OUTPUT_DIR / \"final_model\" / 'logs'),\n",
    "        logging_steps=FINAL_LOGGING_STEPS,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=FINAL_SAVE_TOTAL_LIMIT,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"sequence_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=MAX_SEQ_LENGTH,\n",
    "        generation_num_beams=4,\n",
    "        fp16=FINAL_USE_FP16,\n",
    "        label_smoothing_factor=FINAL_LABEL_SMOOTHING,\n",
    "        # gradient_checkpointing=FINAL_USE_GRADIENT_CHECKPOINTING, # Optional\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    final_trainer = Seq2SeqTrainer(\n",
    "        model=final_model, args=final_training_args,\n",
    "        train_dataset=full_train_dataset, eval_dataset=full_val_dataset,\n",
    "        data_collator=data_collator_for_eval, tokenizer=tokenizer_for_eval,\n",
    "        compute_metrics=compute_metrics_internal,\n",
    "    )\n",
    "\n",
    "    print(f\"[INFO] Starting final training run ({FINAL_TRAIN_EPOCHS} epochs)...\")\n",
    "    try:\n",
    "        train_result = final_trainer.train()\n",
    "        final_trainer.log_metrics(\"final_train\", train_result.metrics)\n",
    "        final_trainer.save_metrics(\"final_train\", train_result.metrics)\n",
    "        final_trainer.save_state()\n",
    "        print(\"[INFO] Final training complete.\")\n",
    "        if final_trainer.state.best_model_checkpoint: print(f\"[INFO] Best final model checkpoint: {final_trainer.state.best_model_checkpoint}\")\n",
    "    except Exception as train_e: print(f\"[FATAL] Final training failed: {train_e}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 9. Final Test Evaluation ---\n",
    "    print(\"[INFO] Evaluating final best model on the full test set...\")\n",
    "    try:\n",
    "        test_results = final_trainer.evaluate(eval_dataset=full_test_dataset, metric_key_prefix=\"test\")\n",
    "        final_trainer.log_metrics(\"test\", test_results)\n",
    "        final_trainer.save_metrics(\"test\", test_results)\n",
    "        if 'test_sequence_accuracy' in test_results:\n",
    "             print(f\"\\n--- Final Test Set Results ---\"); print(f\"Test Sequence Accuracy: {test_results['test_sequence_accuracy']:.4f}\"); print(f\"-----------------------------\")\n",
    "        else: print(\"[Warning] 'test_sequence_accuracy' not found.\", \"Full test results:\", test_results)\n",
    "    except Exception as test_e: print(f\"[Error] Final evaluation failed: {test_e}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    print(\"\\n[INFO] Script finished successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SEED = 42\n",
    "    random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6: Symbolic empirical representation of squared amplitudes in high-energy physics\n",
    "Model: Transformer with novel approach for tokenization, data representation and/or preprocessing that leads to better performance than basic tokenization with normalized indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a7f0800289466fba19c01cbc2550c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "tokenizers>=0.11.1,!=0.11.3,<0.14 is required for a normal functioning of this module, but found tokenizers==0.21.1.\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git main",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RobertaTokenizerFast \u001b[38;5;66;03m# Using RobertaTokenizer for BPE training\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;66;03m# Progress bars\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Optional: Learning rate scheduler\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# from transformers import get_linear_schedule_with_warmup\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# --- Configuration ---\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# File Paths\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/feynman_tokenizer_env/lib/python3.9/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     logging,\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     50\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/feynman_tokenizer_env/lib/python3.9/site-packages/transformers/dependency_versions_check.py:57\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeps\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, check dependency_versions_table.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/feynman_tokenizer_env/lib/python3.9/site-packages/transformers/utils/versions.py:117\u001b[0m, in \u001b[0;36mrequire_version_core\u001b[0;34m(requirement)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m hint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry: pip install transformers -U or pip install -e \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.[dev]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m if you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre working with git main\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/feynman_tokenizer_env/lib/python3.9/site-packages/transformers/utils/versions.py:111\u001b[0m, in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m want_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m op, want_ver \u001b[38;5;129;01min\u001b[39;00m wanted\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 111\u001b[0m         \u001b[43m_compare_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgot_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwant_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/feynman_tokenizer_env/lib/python3.9/site-packages/transformers/utils/versions.py:44\u001b[0m, in \u001b[0;36m_compare_versions\u001b[0;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to compare versions for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: need=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwant_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is unusual. Consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m reinstalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops[op](version\u001b[38;5;241m.\u001b[39mparse(got_ver), version\u001b[38;5;241m.\u001b[39mparse(want_ver)):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is required for a normal functioning of this module, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: tokenizers>=0.11.1,!=0.11.3,<0.14 is required for a normal functioning of this module, but found tokenizers==0.21.1.\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git main"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Fine-tuning Script for a Depth-Aware Transformer for Symbolic Regression.\n",
    "\n",
    "This script implements and trains a custom Transformer Encoder-Decoder model\n",
    "designed for symbolic regression tasks, specifically predicting squared amplitudes\n",
    "in High Energy Physics (HEP) from input amplitudes.\n",
    "\n",
    "Key features:\n",
    "- Custom BPE Tokenizer: Trains a Byte-Pair Encoding tokenizer from scratch on the\n",
    "  combined input and target sequences of the dataset, potentially capturing\n",
    "  domain-specific physics identifiers and operators more effectively.\n",
    "- Depth-Aware Embeddings: Calculates the parenthesis nesting depth for each token\n",
    "  in the input sequence and incorporates this information via dedicated depth\n",
    "  embeddings, adding structural awareness to the model's input representation.\n",
    "- Custom Transformer Model: Implements a standard Transformer encoder-decoder\n",
    "  using PyTorch's `nn.TransformerEncoderLayer` and `nn.TransformerDecoderLayer`,\n",
    "  modified to accept the combined token, position, and depth embeddings.\n",
    "- Manual PyTorch Training Loop: Includes standard training, validation, and\n",
    "  testing phases with loss calculation, optimization, metric computation (sequence\n",
    "  accuracy), and basic checkpointing.\n",
    "\n",
    "Workflow:\n",
    "1. Load raw data splits from JSONL files.\n",
    "2. Train a new BPE tokenizer on the corpus (if not already trained and saved)\n",
    "   or load a previously trained tokenizer.\n",
    "3. Define a custom `DepthDataset` that performs tokenization and calculates\n",
    "   parenthesis depth for each input sequence during initialization.\n",
    "4. Define the `DepthTransformer` model architecture using PyTorch's nn.Module.\n",
    "5. Set up DataLoaders, optimizer, loss criterion, and device (GPU/CPU).\n",
    "6. Implement the training loop, iterating through epochs and batches, performing\n",
    "   forward/backward passes, and updating model weights.\n",
    "7. Implement validation loop to monitor performance (sequence accuracy) and save\n",
    "   the best model checkpoint based on validation results.\n",
    "8. Implement the final test loop to evaluate the best model on the held-out\n",
    "   test set.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizerFast # Using RobertaTokenizer for BPE training\n",
    "from tqdm.auto import tqdm # Progress bars\n",
    "# Optional: Learning rate scheduler\n",
    "# from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# File Paths\n",
    "TRAIN_FILE = Path('qed_expressions_train.jsonl') # Adjust if using different names\n",
    "VAL_FILE   = Path('qed_expressions_val.jsonl')\n",
    "TEST_FILE  = Path('qed_expressions_test.jsonl')\n",
    "OUTPUT_DIR = Path('depth_aware_transformer_output')\n",
    "TOKENIZER_SAVE_DIR = OUTPUT_DIR / \"bpe_physics_tokenizer\" # Directory to save/load trained tokenizer\n",
    "CHECKPOINT_NAME = \"depth_transformer_best.pt\" # Name for the best model checkpoint\n",
    "\n",
    "# Tokenizer Configuration\n",
    "VOCAB_SIZE = 8000         # Target vocabulary size for BPE tokenizer\n",
    "MIN_FREQUENCY = 2         # Minimum frequency for tokens in BPE training\n",
    "# Special tokens common for sequence models\n",
    "SPECIAL_TOKENS = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "\n",
    "# Data and Model Configuration\n",
    "MAX_SEQ_LENGTH = 128      # Max sequence length for tokenization and model\n",
    "MAX_DEPTH = 50            # Maximum expected parenthesis nesting depth + buffer\n",
    "\n",
    "# Model Hyperparameters (DepthTransformer)\n",
    "D_MODEL = 512             # Embedding and hidden state dimension\n",
    "N_HEAD = 8                # Number of attention heads\n",
    "NUM_ENCODER_LAYERS = 6    # Number of layers in the Transformer encoder\n",
    "NUM_DECODER_LAYERS = 6    # Number of layers in the Transformer decoder\n",
    "DIM_FEEDFORWARD = 2048    # Dimension of the feed-forward networks\n",
    "DROPOUT = 0.1             # Dropout rate (can be added to model layers)\n",
    "\n",
    "# Training Hyperparameters\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 16           # Adjust based on GPU memory\n",
    "LEARNING_RATE = 5e-4\n",
    "OPTIMIZER_EPS = 1e-8\n",
    "WEIGHT_DECAY = 0.01\n",
    "# LR_SCHEDULER_TYPE = \"linear\" # Optional scheduler type\n",
    "# WARMUP_RATIO = 0.1         # Optional warmup ratio\n",
    "LOG_INTERVAL = 50         # Log training stats every N batches\n",
    "\n",
    "# --- Helper Functions and Classes ---\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads data from a JSON Lines file, handling basic errors.\"\"\"\n",
    "    # (Same implementation as previous script)\n",
    "    data = []\n",
    "    file_path = Path(file_path)\n",
    "    print(f\"[INFO] Loading data from: {file_path}\")\n",
    "    if not file_path.is_file():\n",
    "        print(f\"[Error] Data file not found: {file_path}\", file=sys.stderr)\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try: data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e: print(f\"[Warning] Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "        print(f\"[INFO] Successfully loaded {len(data)} records.\")\n",
    "        if not data: print(f\"[Warning] No valid records loaded from {file_path}.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load data from {file_path}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "\n",
    "def train_or_load_tokenizer(corpus_iterator, save_dir, vocab_size, min_freq, special_tokens):\n",
    "    \"\"\"Trains a new BPE tokenizer or loads it if it already exists.\"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    config_file = save_dir / \"tokenizer_config.json\"\n",
    "\n",
    "    if save_dir.exists() and config_file.exists():\n",
    "        print(f\"[INFO] Loading existing tokenizer from {save_dir}\")\n",
    "        try:\n",
    "            tokenizer = RobertaTokenizerFast.from_pretrained(str(save_dir))\n",
    "            # Verify core special tokens are present\n",
    "            if any(tok not in tokenizer.get_vocab() for tok in ['<s>', '<pad>', '</s>', '<unk>']):\n",
    "                 print(\"[Warning] Loaded tokenizer missing some standard special tokens. Recheck config.\")\n",
    "            return tokenizer\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Failed to load existing tokenizer: {e}. Attempting to retrain.\", file=sys.stderr)\n",
    "\n",
    "    print(f\"[INFO] Training new BPE tokenizer (Vocab: {vocab_size}, Min Freq: {min_freq})...\")\n",
    "    try:\n",
    "        # Initialize a base tokenizer (like Roberta's structure) but don't load weights\n",
    "        # We use RobertaTokenizerFast as it supports training from iterator\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\") # Base structure\n",
    "        print(\"[INFO] Training from iterator...\")\n",
    "        # The train_new_from_iterator method might not exist directly on RobertaTokenizerFast.\n",
    "        # Often, you use the underlying `tokenizers` library for this.\n",
    "        # Let's adapt using the `tokenizers` library approach if direct method fails.\n",
    "\n",
    "        try:\n",
    "             # Attempt direct method if available in specific transformers version\n",
    "             tokenizer.train_new_from_iterator(corpus_iterator, vocab_size=vocab_size, min_frequency=min_freq, special_tokens=special_tokens)\n",
    "        except AttributeError:\n",
    "             print(\"[INFO] `train_new_from_iterator` not found, using `tokenizers` library directly.\")\n",
    "             from tokenizers import ByteLevelBPETokenizer as TokenizersBPETokenizer\n",
    "\n",
    "             # Initialize BPE tokenizer from the `tokenizers` library\n",
    "             tk_lib_tokenizer = TokenizersBPETokenizer()\n",
    "             tk_lib_tokenizer.train_from_iterator(\n",
    "                 corpus_iterator,\n",
    "                 vocab_size=vocab_size,\n",
    "                 min_frequency=min_freq,\n",
    "                 special_tokens=special_tokens\n",
    "             )\n",
    "             # Need to save this intermediate tokenizer and then load it into RobertaTokenizerFast\n",
    "             temp_save_path = save_dir / \"temp_tokenizer_files\"\n",
    "             temp_save_path.mkdir(parents=True, exist_ok=True)\n",
    "             tk_lib_tokenizer.save_model(str(temp_save_path)) # Saves vocab.json and merges.txt\n",
    "\n",
    "             # Load the trained vocab/merges into the desired HF Tokenizer class\n",
    "             tokenizer = RobertaTokenizerFast.from_pretrained(str(temp_save_path), max_len=MAX_SEQ_LENGTH) # Or appropriate max_len\n",
    "             # Clean up temp files\n",
    "             import shutil; shutil.rmtree(temp_save_path)\n",
    "\n",
    "\n",
    "        print(\"[INFO] Tokenizer training complete.\")\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        tokenizer.save_pretrained(str(save_dir))\n",
    "        print(f\"[INFO] Tokenizer saved to {save_dir}\")\n",
    "        return tokenizer\n",
    "    except ImportError as e:\n",
    "         print(f\"[Error] Required library (`tokenizers`) not found for training: {e}. Please install it.\", file=sys.stderr)\n",
    "         raise\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Tokenizer training failed: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "\n",
    "class DepthDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that tokenizes sequences and calculates parenthesis nesting depth.\n",
    "\n",
    "    Tokenization and depth calculation are performed once during initialization.\n",
    "    Stores token IDs, attention masks, depth IDs, and label IDs.\n",
    "\n",
    "    Note: Stores all processed examples in memory. May be unsuitable for\n",
    "          extremely large datasets without modification (e.g., lazy processing\n",
    "          or memory mapping).\n",
    "    \"\"\"\n",
    "    def __init__(self, raw_data, tokenizer, max_len=128, max_depth=50):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_data (list[dict]): List of dictionaries, each with 'input_tokens'\n",
    "                                   and 'target_tokens' keys containing lists of strings.\n",
    "            tokenizer: Initialized Hugging Face tokenizer instance.\n",
    "            max_len (int): Maximum sequence length for padding/truncation.\n",
    "            max_depth (int): Maximum depth value to cap at; also determines depth embedding size.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.max_depth = max_depth\n",
    "        self.examples = [] # List to store processed examples\n",
    "\n",
    "        print(f\"[INFO] Processing dataset for DepthDataset (Max Len: {max_len}, Max Depth: {max_depth})...\")\n",
    "        skipped_count = 0\n",
    "        for i, ex in enumerate(tqdm(raw_data, desc=\"Processing Raw Data\")):\n",
    "            input_tokens = ex.get(\"input_tokens\")\n",
    "            target_tokens = ex.get(\"target_tokens\")\n",
    "\n",
    "            if not isinstance(input_tokens, list) or not isinstance(target_tokens, list):\n",
    "                print(f\"[Warning] Skipping example {i} due to missing or invalid token lists.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Join tokens for BPE tokenizer\n",
    "            src_str = \" \".join(map(str, input_tokens))\n",
    "            tgt_str = \" \".join(map(str, target_tokens))\n",
    "\n",
    "            # Tokenize source and target\n",
    "            # Note: We don't use return_tensors='pt' here; store lists/ints\n",
    "            src_enc = tokenizer(src_str, max_length=max_len, truncation=True, padding=\"max_length\", return_tensors=None)\n",
    "            # Use context manager for target tokenization (good practice)\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                tgt_enc = tokenizer(tgt_str, max_length=max_len, truncation=True, padding=\"max_length\", return_tensors=None)\n",
    "\n",
    "            # --- Calculate Parenthesis Depth ---\n",
    "            # Operates on the *original* input_tokens before BPE tokenization\n",
    "            current_depth = 0\n",
    "            depth_sequence = []\n",
    "            for tok in input_tokens:\n",
    "                # Assign depth *before* potential change for closing parenthesis\n",
    "                if tok == \")\":\n",
    "                    current_depth = max(0, current_depth - 1) # Decrement after assigning for ')'\n",
    "                    depth_sequence.append(min(current_depth, max_depth - 1)) # Cap depth\n",
    "                elif tok == \"(\":\n",
    "                    depth_sequence.append(min(current_depth, max_depth - 1)) # Cap depth\n",
    "                    current_depth += 1 # Increment after assigning for '('\n",
    "                else:\n",
    "                    depth_sequence.append(min(current_depth, max_depth - 1)) # Assign current depth\n",
    "\n",
    "            # --- Align Depth with Tokenized Sequence (Simple Approach) ---\n",
    "            # This is a simplification. A robust alignment would map original token\n",
    "            # positions to BPE token positions. Here, we just pad/truncate the\n",
    "            # depth sequence calculated on original tokens to match the BPE sequence length.\n",
    "            # This might misalign depth for tokens split by BPE.\n",
    "            # TODO: Implement a more robust alignment if needed.\n",
    "            if len(depth_sequence) >= max_len:\n",
    "                aligned_depth_ids = depth_sequence[:max_len]\n",
    "            else:\n",
    "                # Pad with depth 0 (assuming root level)\n",
    "                aligned_depth_ids = depth_sequence + [0] * (max_len - len(depth_sequence))\n",
    "\n",
    "            self.examples.append({\n",
    "                \"input_ids\":      src_enc[\"input_ids\"],\n",
    "                \"attention_mask\": src_enc[\"attention_mask\"],\n",
    "                \"depth_ids\":      aligned_depth_ids,\n",
    "                \"labels\":         tgt_enc[\"input_ids\"], # Target token IDs\n",
    "            })\n",
    "\n",
    "        if skipped_count > 0:\n",
    "            print(f\"[Warning] Skipped {skipped_count} examples during dataset processing.\")\n",
    "        if not self.examples:\n",
    "             raise ValueError(\"Dataset processing resulted in zero valid examples.\")\n",
    "        print(f\"[INFO] DepthDataset processed. Total examples: {len(self.examples)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of processed examples.\"\"\"\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a processed example by index.\n",
    "        Returns data as lists/ints; DataLoader handles tensor conversion.\n",
    "        \"\"\"\n",
    "        if not 0 <= idx < len(self.examples):\n",
    "            raise IndexError(f\"Index {idx} out of bounds.\")\n",
    "        return self.examples[idx]\n",
    "\n",
    "\n",
    "class DepthTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder-Decoder model incorporating token depth embeddings.\n",
    "\n",
    "    Input representation combines token, position, and parenthesis depth embeddings.\n",
    "    Uses standard PyTorch Transformer layers for sequence processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, max_len=128,\n",
    "                 max_depth=50, dropout=0.1, pad_token_id=None):\n",
    "        \"\"\"\n",
    "        Initializes the Depth-Aware Transformer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            d_model (int): Dimension of embeddings and hidden states.\n",
    "            nhead (int): Number of attention heads.\n",
    "            num_encoder_layers (int): Number of layers in the encoder stack.\n",
    "            num_decoder_layers (int): Number of layers in the decoder stack.\n",
    "            dim_feedforward (int): Dimension of the feed-forward networks.\n",
    "            max_len (int): Maximum sequence length for positional embeddings.\n",
    "            max_depth (int): Maximum depth value + 1 (size of depth embedding table).\n",
    "            dropout (float): Dropout rate.\n",
    "            pad_token_id (int): ID of the padding token for embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pad_token_id = pad_token_id if pad_token_id is not None else 0 # Default assumption\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model, padding_idx=self.pad_token_id)\n",
    "        self.positional_embedding = nn.Embedding(max_len, d_model) # Absolute positional\n",
    "        self.depth_embedding = nn.Embedding(max_depth, d_model) # Depth information\n",
    "\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Standard PyTorch Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        # Standard PyTorch Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        # Output Projection Layer\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        print(f\"[INFO] Initialized DepthTransformer:\")\n",
    "        print(f\"  - Vocab Size: {vocab_size}, d_model: {d_model}, Heads: {nhead}\")\n",
    "        print(f\"  - Max Len: {max_len}, Max Depth: {max_depth}\")\n",
    "        print(f\"  - Enc Layers: {num_encoder_layers}, Dec Layers: {num_decoder_layers}\")\n",
    "        print(f\"  - Feedforward Dim: {dim_feedforward}, Dropout: {dropout}\")\n",
    "\n",
    "\n",
    "    def forward(self, src_input_ids, src_attention_mask, src_depth_ids, tgt_input_ids):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the depth-aware Transformer.\n",
    "\n",
    "        Args:\n",
    "            src_input_ids (torch.Tensor): Source sequence token IDs (batch_size, src_seq_len).\n",
    "            src_attention_mask (torch.Tensor): Source sequence attention mask (batch_size, src_seq_len).\n",
    "                                               (1 for real tokens, 0 for padding).\n",
    "            src_depth_ids (torch.Tensor): Source sequence depth IDs (batch_size, src_seq_len).\n",
    "            tgt_input_ids (torch.Tensor): Target sequence token IDs (batch_size, tgt_seq_len),\n",
    "                                          typically shifted right for teacher forcing.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits (batch_size, tgt_seq_len, vocab_size).\n",
    "        \"\"\"\n",
    "        batch_size, src_seq_len = src_input_ids.shape\n",
    "        _, tgt_seq_len = tgt_input_ids.shape\n",
    "\n",
    "        # 1. Source Embeddings (Token + Position + Depth)\n",
    "        # Create position IDs (0 to seq_len-1)\n",
    "        src_pos_ids = torch.arange(src_seq_len, device=src_input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        src_emb = self.token_embedding(src_input_ids)           # (B, S, D)\n",
    "        src_pos = self.positional_embedding(src_pos_ids)       # (B, S, D)\n",
    "        src_dep = self.depth_embedding(src_depth_ids)         # (B, S, D)\n",
    "\n",
    "        src_combined_emb = src_emb + src_pos + src_dep # Combine embeddings\n",
    "        src_combined_emb = self.embedding_dropout(src_combined_emb)\n",
    "\n",
    "        # 2. Encoder\n",
    "        # PyTorch Transformer layers expect masks where True indicates a position *not* to attend to.\n",
    "        # src_attention_mask is (B, S) with 1 for non-pad, 0 for pad. Need (B, S) with True for pad.\n",
    "        src_key_padding_mask = (src_attention_mask == 0) # True where padded\n",
    "\n",
    "        # Pass through encoder stack\n",
    "        memory = self.encoder(src_combined_emb, src_key_padding_mask=src_key_padding_mask) # (B, S, D)\n",
    "\n",
    "        # 3. Target Embeddings (Token + Position) - No depth for target typically\n",
    "        # Target IDs are usually shifted right for teacher forcing (e.g., start with BOS, end before EOS)\n",
    "        tgt_pos_ids = torch.arange(tgt_seq_len, device=tgt_input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        tgt_emb = self.token_embedding(tgt_input_ids)           # (B, T, D)\n",
    "        tgt_pos = self.positional_embedding(tgt_pos_ids)       # (B, T, D)\n",
    "\n",
    "        tgt_combined_emb = tgt_emb + tgt_pos # Combine target embeddings\n",
    "        tgt_combined_emb = self.embedding_dropout(tgt_combined_emb)\n",
    "\n",
    "        # 4. Decoder\n",
    "        # Create target padding mask (True for padded positions)\n",
    "        # Assuming target uses same pad token id\n",
    "        # Need a mask for the target sequence itself\n",
    "        tgt_key_padding_mask = (tgt_input_ids == self.pad_token_id)\n",
    "\n",
    "        # Create causal mask (autoregressive mask) to prevent attending to future tokens\n",
    "        # Shape: (T, T)\n",
    "        tgt_causal_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len, device=tgt_input_ids.device)\n",
    "\n",
    "        # Pass through decoder stack\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=tgt_combined_emb,           # Target sequence embeddings (B, T, D)\n",
    "            memory=memory,                  # Encoder output (B, S, D)\n",
    "            tgt_mask=tgt_causal_mask,       # Causal mask (T, T)\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask, # Target padding mask (B, T)\n",
    "            memory_key_padding_mask=src_key_padding_mask # Source padding mask (B, S)\n",
    "        ) # Output shape: (B, T, D)\n",
    "\n",
    "        # 5. Output Projection\n",
    "        logits = self.output_projection(decoder_output) # (B, T, VocabSize)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "def calculate_sequence_accuracy(logits, labels, pad_token_id):\n",
    "    \"\"\"Calculates exact sequence match accuracy using PyTorch tensors.\"\"\"\n",
    "    if logits.shape[0] == 0: return 0.0\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    non_pad_mask = (labels != pad_token_id)\n",
    "    correct_tokens = (predictions == labels) & non_pad_mask\n",
    "    correct_sequences = (torch.sum(correct_tokens, dim=1) == torch.sum(non_pad_mask, dim=1))\n",
    "    accuracy = torch.mean(correct_sequences.float())\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Orchestrates tokenizer training, data processing, model training, and evaluation.\"\"\"\n",
    "    print(\"[INFO] Starting Depth-Aware Transformer Script...\")\n",
    "    try: current_time_str = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
    "    except Exception: current_time_str = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC (naive)')\n",
    "    # --- ***** Update Timestamp/Location ***** ---\n",
    "    print(f\"[INFO] Current time: {current_time_str}\")\n",
    "    print(f\"[INFO] Location context: San Diego, CA, USA\")\n",
    "\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- 1. Load Raw Data ---\n",
    "    try:\n",
    "        train_raw = load_jsonl(TRAIN_FILE)\n",
    "        val_raw = load_jsonl(VAL_FILE)\n",
    "        test_raw = load_jsonl(TEST_FILE)\n",
    "    except Exception as e: print(f\"[FATAL] Data loading failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "    if not train_raw or not val_raw or not test_raw: print(\"[FATAL] Datasets empty after loading.\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 2. Prepare Corpus and Train/Load Tokenizer ---\n",
    "    def corpus_gen(): # Generator to avoid loading all strings into memory at once\n",
    "        print(\"[INFO] Preparing corpus for tokenizer training...\")\n",
    "        count = 0\n",
    "        for split in (train_raw, val_raw, test_raw):\n",
    "            for ex in split:\n",
    "                if ex.get(\"input_tokens\"): yield \" \".join(map(str, ex[\"input_tokens\"]))\n",
    "                if ex.get(\"target_tokens\"): yield \" \".join(map(str, ex[\"target_tokens\"]))\n",
    "                count += 2\n",
    "        print(f\"[INFO] Corpus generator ready ({count} sequences).\")\n",
    "\n",
    "    try:\n",
    "        tokenizer = train_or_load_tokenizer(\n",
    "            corpus_iterator=corpus_gen(),\n",
    "            save_dir=TOKENIZER_SAVE_DIR,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            min_freq=MIN_FREQUENCY,\n",
    "            special_tokens=SPECIAL_TOKENS\n",
    "        )\n",
    "        vocab_size = len(tokenizer) # Get actual vocab size after training/loading\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "        if pad_token_id is None:\n",
    "             print(\"[Error] Tokenizer loaded without a pad token ID!\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Tokenizer training or loading failed: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    print(f\"[INFO] Tokenizer ready (Vocab size: {vocab_size}, Pad ID: {pad_token_id}).\")\n",
    "\n",
    "\n",
    "    # --- 3. Create Datasets ---\n",
    "    try:\n",
    "        print(\"[INFO] Creating DepthDatasets...\")\n",
    "        train_dataset = DepthDataset(train_raw, tokenizer, MAX_SEQ_LENGTH, MAX_DEPTH)\n",
    "        val_dataset   = DepthDataset(val_raw,   tokenizer, MAX_SEQ_LENGTH, MAX_DEPTH)\n",
    "        test_dataset  = DepthDataset(test_raw,  tokenizer, MAX_SEQ_LENGTH, MAX_DEPTH)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed to create datasets: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    if not train_dataset or not val_dataset or not test_dataset:\n",
    "         print(\"[FATAL] One or more datasets are empty after processing. Exiting.\", file=sys.stderr)\n",
    "         sys.exit(1)\n",
    "\n",
    "\n",
    "    # --- 4. Create DataLoaders ---\n",
    "    # Define a simple collate function to handle batching of list data into tensors\n",
    "    def collate_batch(batch):\n",
    "        elem = batch[0]\n",
    "        collated = {}\n",
    "        for key in elem.keys():\n",
    "            collated[key] = torch.tensor([d[key] for d in batch])\n",
    "        return collated\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch, num_workers=4, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch, num_workers=4, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch, num_workers=4, pin_memory=True)\n",
    "    print(\"[INFO] DataLoaders created.\")\n",
    "\n",
    "\n",
    "    # --- 5. Initialize Model, Optimizer, Criterion ---\n",
    "    print(f\"[INFO] Initializing DepthTransformer model...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        model = DepthTransformer(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=D_MODEL,\n",
    "            nhead=N_HEAD,\n",
    "            num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "            num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "            dim_feedforward=DIM_FEEDFORWARD,\n",
    "            max_len=MAX_SEQ_LENGTH,\n",
    "            max_depth=MAX_DEPTH,\n",
    "            dropout=DROPOUT,\n",
    "            pad_token_id=pad_token_id\n",
    "        ).to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed to initialize model: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        eps=OPTIMIZER_EPS,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id) # Ignore padding in loss\n",
    "    print(\"[INFO] Model, Optimizer, and Criterion initialized.\")\n",
    "\n",
    "    # Optional: Learning Rate Scheduler\n",
    "    # total_steps = len(train_loader) * NUM_EPOCHS\n",
    "    # warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "    # --- 6. Training Loop ---\n",
    "    print(f\"[INFO] Starting training for {NUM_EPOCHS} epochs...\")\n",
    "    best_val_accuracy = -1.0\n",
    "    best_epoch = -1\n",
    "    checkpoint_path = OUTPUT_DIR / CHECKPOINT_NAME\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False)\n",
    "\n",
    "        for step, batch in enumerate(train_pbar):\n",
    "            # Move batch data to the device\n",
    "            src_ids = batch[\"input_ids\"].to(device)\n",
    "            src_mask = batch[\"attention_mask\"].to(device) # Encoder padding mask source\n",
    "            depth_ids = batch[\"depth_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)          # Target sequence (B, T)\n",
    "\n",
    "            # Prepare decoder inputs (shifted right) and targets\n",
    "            # Decoder input: BOS token + sequence (excluding last token)\n",
    "            # Decoder target: sequence (excluding first token) + EOS token (implicitly handled by loss shifting)\n",
    "            # For CrossEntropyLoss, logits (B, T, V) should align with targets (B, T)\n",
    "            tgt_input = labels[:, :-1] # (B, T-1), excludes last token\n",
    "            tgt_output = labels[:, 1:]  # (B, T-1), excludes first token (BOS/CLS)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(src_input_ids=src_ids,\n",
    "                           src_attention_mask=src_mask,\n",
    "                           src_depth_ids=depth_ids,\n",
    "                           tgt_input_ids=tgt_input) # Pass shifted input (B, T-1, V)\n",
    "\n",
    "            # Calculate loss - compare logits with shifted *output* targets\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), # (B*(T-1), V)\n",
    "                             tgt_output.reshape(-1))         # (B*(T-1))\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            # Optional: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            # if scheduler: scheduler.step() # Update LR if scheduler is used\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Log training progress\n",
    "            if (step + 1) % LOG_INTERVAL == 0:\n",
    "                avg_loss = total_train_loss / LOG_INTERVAL\n",
    "                # current_lr = scheduler.get_last_lr()[0] if scheduler else LEARNING_RATE\n",
    "                current_lr = LEARNING_RATE # Simpler if no scheduler\n",
    "                train_pbar.set_postfix({'Avg Loss': f'{avg_loss:.4f}', 'LR': f'{current_lr:.2e}'})\n",
    "                total_train_loss = 0.0 # Reset accumulator\n",
    "\n",
    "        train_pbar.close()\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        total_val_accuracy = 0.0\n",
    "        total_val_samples = 0\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_pbar:\n",
    "                src_ids = batch[\"input_ids\"].to(device)\n",
    "                src_mask = batch[\"attention_mask\"].to(device)\n",
    "                depth_ids = batch[\"depth_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                tgt_input = labels[:, :-1]\n",
    "                tgt_output = labels[:, 1:]\n",
    "\n",
    "                logits = model(src_ids, src_mask, depth_ids, tgt_input)\n",
    "\n",
    "                batch_accuracy = calculate_sequence_accuracy(logits, tgt_output, pad_token_id)\n",
    "                total_val_accuracy += batch_accuracy * src_ids.size(0) # Weighted by batch size\n",
    "                total_val_samples += src_ids.size(0)\n",
    "\n",
    "        val_pbar.close()\n",
    "        epoch_val_accuracy = total_val_accuracy / total_val_samples if total_val_samples > 0 else 0.0\n",
    "        print(f\"Epoch {epoch+1} Validation Sequence Accuracy: {epoch_val_accuracy:.4f}\")\n",
    "\n",
    "        # --- Save Best Model Checkpoint ---\n",
    "        if epoch_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = epoch_val_accuracy\n",
    "            best_epoch = epoch + 1\n",
    "            try:\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"[INFO] New best model saved to {checkpoint_path} (Epoch {best_epoch}, Val Acc: {best_val_accuracy:.4f})\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed to save model checkpoint: {e}\", file=sys.stderr)\n",
    "\n",
    "    print(f\"\\n[INFO] Training complete. Best validation accuracy: {best_val_accuracy:.4f} at epoch {best_epoch}.\")\n",
    "\n",
    "    # --- 7. Final Test Evaluation ---\n",
    "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
    "    if checkpoint_path.exists():\n",
    "        print(f\"[INFO] Loading best model from: {checkpoint_path}\")\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "        except Exception as e:\n",
    "             print(f\"[Error] Failed to load best model checkpoint: {e}. Evaluating with the final state.\", file=sys.stderr)\n",
    "    else:\n",
    "        print(\"[Warning] Best model checkpoint not found. Evaluating with the final model state.\", file=sys.stderr)\n",
    "\n",
    "    model.eval()\n",
    "    total_test_accuracy = 0.0\n",
    "    total_test_samples = 0\n",
    "    test_pbar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_pbar:\n",
    "            src_ids = batch[\"input_ids\"].to(device)\n",
    "            src_mask = batch[\"attention_mask\"].to(device)\n",
    "            depth_ids = batch[\"depth_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            tgt_input = labels[:, :-1]\n",
    "            tgt_output = labels[:, 1:]\n",
    "\n",
    "            logits = model(src_ids, src_mask, depth_ids, tgt_input)\n",
    "\n",
    "            batch_accuracy = calculate_sequence_accuracy(logits, tgt_output, pad_token_id)\n",
    "            total_test_accuracy += batch_accuracy * src_ids.size(0)\n",
    "            total_test_samples += src_ids.size(0)\n",
    "\n",
    "    test_pbar.close()\n",
    "    final_test_accuracy = total_test_accuracy / total_test_samples if total_test_samples > 0 else 0.0\n",
    "    print(f\"\\nFinal Test Sequence Accuracy: {final_test_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\n[INFO] Script finished successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Optional: Seed everything\n",
    "    # SEED = 42\n",
    "    # torch.manual_seed(SEED); np.random.seed(SEED); import random; random.seed(SEED)\n",
    "    # if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7: Foundation models for symbolic regression tasks\n",
    "Model: Novel foundation model for symbolic regression tasks. Should be sufficiently novel beyond the current literature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'T5Block' from 'transformers' (/home/nikitas/anaconda3/envs/torch_cpu_env/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     55\u001b[0m     T5Tokenizer,\n\u001b[1;32m     56\u001b[0m     T5ForConditionalGeneration,\n\u001b[1;32m     57\u001b[0m     T5Config, \u001b[38;5;66;03m# To inspect T5 block structure if needed\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     T5Block, \u001b[38;5;66;03m# To check layer types\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     DataCollatorForSeq2Seq,\n\u001b[1;32m     60\u001b[0m     Seq2SeqTrainer,\n\u001b[1;32m     61\u001b[0m     Seq2SeqTrainingArguments,\n\u001b[1;32m     62\u001b[0m     TrainingArguments \u001b[38;5;66;03m# Explicit import\u001b[39;00m\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;66;03m# Optional progress bars\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# --- Configuration ---\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# File Paths\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'T5Block' from 'transformers' (/home/nikitas/anaconda3/envs/torch_cpu_env/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Fine-tuning Script for a T5 Model with Mixture-of-Experts (MoE) Layers\n",
    "for Symbolic Regression (Squared Amplitude Calculation).\n",
    "\n",
    "This script demonstrates modifying a standard T5 (`t5-small`) model by replacing\n",
    "its Feed-Forward Networks (FFNs) with custom Mixture-of-Experts (MoE) layers.\n",
    "The MoE layers potentially allow different \"expert\" networks within the model to\n",
    "specialize in processing different types of symbolic patterns found in the input\n",
    "sequences (e.g., polynomials, trigonometric functions).\n",
    "\n",
    "The script utilizes the Hugging Face Transformers library, including the\n",
    "`Seq2SeqTrainer` API for streamlined training and evaluation.\n",
    "\n",
    "Key features demonstrated:\n",
    "- T5 Base Model: Uses `t5-small` as the starting point.\n",
    "- Custom MoE Layer: Defines and integrates a `MoEFeedForward` module.\n",
    "- Model Patching: Dynamically replaces the standard FFNs in the loaded T5 model\n",
    "  with the custom MoE layers.\n",
    "- Advanced Training Techniques: Incorporates Gradient Checkpointing, Mixed\n",
    "  Precision Training (FP16), and Label Smoothing.\n",
    "- Standard Workflow: Follows data loading, preprocessing, tokenization, model\n",
    "  configuration, training, and evaluation steps.\n",
    "- Exact Match Accuracy (Logit-based): Evaluates performance using sequence\n",
    "  accuracy calculated directly from model logits (`predict_with_generate=False`).\n",
    "\n",
    "Workflow:\n",
    "1. Load pre-split data from JSONL files.\n",
    "2. Reconstruct source and target strings from token lists.\n",
    "3. Initialize the T5 tokenizer.\n",
    "4. Define the custom `MoEFeedForward` nn.Module.\n",
    "5. Load the base T5 model and patch its encoder/decoder blocks by replacing\n",
    "   standard FFN layers with instances of `MoEFeedForward`.\n",
    "6. Configure the modified model (special tokens, gradient checkpointing).\n",
    "7. Define a function to tokenize source/target string pairs.\n",
    "8. Wrap tokenized data into PyTorch Dataset objects.\n",
    "9. Instantiate `DataCollatorForSeq2Seq`.\n",
    "10. Define the `compute_metrics` function for sequence accuracy from logits.\n",
    "11. Configure `Seq2SeqTrainingArguments`.\n",
    "12. Initialize and run the `Seq2SeqTrainer`.\n",
    "13. Evaluate the final model on the test set.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Config, # To inspect T5 block structure if needed\n",
    "    T5Block, # To check layer types\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainingArguments # Explicit import\n",
    ")\n",
    "from tqdm.auto import tqdm # Optional progress bars\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# File Paths\n",
    "TRAIN_FILE = Path('qed_expressions_train.jsonl') # Adjust if needed\n",
    "VAL_FILE   = Path('qed_expressions_val.jsonl')\n",
    "TEST_FILE  = Path('qed_expressions_test.jsonl')\n",
    "OUTPUT_DIR = Path('t5_moe_symbolic_regression_output') # Output directory\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_ID = \"t5-small\" # Base T5 model identifier\n",
    "TOKENIZER_ID = \"t5-small\"\n",
    "\n",
    "# MoE Configuration\n",
    "NUM_EXPERTS = 4             # Number of experts in each MoE layer\n",
    "# d_ff (intermediate FFN dimension) will be taken from the base T5 config\n",
    "\n",
    "# Tokenizer and Data Processing Configuration\n",
    "MAX_SEQ_LENGTH = 128        # Max sequence length\n",
    "\n",
    "# Training Hyperparameters\n",
    "NUM_TRAIN_EPOCHS = 5\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 8\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_STEPS = 200\n",
    "LOGGING_STEPS = 50\n",
    "EVALUATION_STRATEGY = \"epoch\"\n",
    "SAVE_STRATEGY = \"epoch\"\n",
    "\n",
    "# Advanced Training Feature Flags/Values\n",
    "USE_FP16 = torch.cuda.is_available()\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "LABEL_SMOOTHING_FACTOR = 0.1\n",
    "\n",
    "# --- Helper Functions and Classes ---\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads data from a JSON Lines file, handling basic errors.\"\"\"\n",
    "    data = []\n",
    "    file_path = Path(file_path)\n",
    "    print(f\"[INFO] Loading data from: {file_path}\")\n",
    "    if not file_path.is_file():\n",
    "        print(f\"[Error] Data file not found: {file_path}\", file=sys.stderr)\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try: data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e: print(f\"[Warning] Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "        print(f\"[INFO] Successfully loaded {len(data)} records.\")\n",
    "        if not data: print(f\"[Warning] No valid records loaded from {file_path}.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load data from {file_path}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "def convert_tokens_to_strings(raw_data_list):\n",
    "    \"\"\"Converts lists of tokens from loaded data into source and target strings.\"\"\"\n",
    "    source_strings, target_strings, skipped_count = [], [], 0\n",
    "    if not raw_data_list: return source_strings, target_strings\n",
    "    for i, item in enumerate(raw_data_list):\n",
    "        input_toks, target_toks = item.get('input_tokens'), item.get('target_tokens')\n",
    "        if isinstance(input_toks, list) and isinstance(target_toks, list):\n",
    "            source_strings.append(\" \".join(map(str, input_toks)))\n",
    "            target_strings.append(\" \".join(map(str, target_toks)))\n",
    "        else:\n",
    "            print(f\"[Warning] Skipping item at index {i} due to missing/invalid data: {item}\"); skipped_count += 1\n",
    "    print(f\"[INFO] Converted {len(source_strings)} items to strings (skipped {skipped_count}).\")\n",
    "    if not source_strings: print(\"[Warning] No items successfully converted.\")\n",
    "    return source_strings, target_strings\n",
    "\n",
    "def encode_sequences(tokenizer, source_texts, target_texts, max_len):\n",
    "    \"\"\"Tokenizes source and target sequences, returning lists of IDs/masks.\"\"\"\n",
    "    print(f\"[INFO] Encoding sequence pairs with max_length={max_len}...\")\n",
    "    encoder_inputs = tokenizer(source_texts, max_length=max_len, padding='max_length', truncation=True, return_tensors=None)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        decoder_labels = tokenizer(target_texts, max_length=max_len, padding='max_length', truncation=True, return_tensors=None)\n",
    "    encoder_inputs['labels'] = decoder_labels['input_ids']\n",
    "    print(f\"[INFO] Encoding complete.\")\n",
    "    if not encoder_inputs.get('input_ids') or not encoder_inputs.get('labels') or len(encoder_inputs['input_ids']) != len(encoder_inputs['labels']):\n",
    "         print(\"[Warning] Encoding resulted in empty lists or length mismatch.\")\n",
    "    return encoder_inputs\n",
    "\n",
    "class SequencePairDataset(Dataset):\n",
    "    \"\"\"Simple PyTorch Dataset for holding tokenized sequence pair data (as lists).\"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        if not isinstance(encodings, dict) or 'input_ids' not in encodings: raise ValueError(\"Invalid encodings format.\")\n",
    "        self.encodings = encodings\n",
    "        try:\n",
    "            self.length = len(encodings['input_ids'])\n",
    "            for key in encodings:\n",
    "                 if not isinstance(encodings[key], list) or len(encodings[key]) != self.length: raise ValueError(f\"Inconsistent length for key '{key}'.\")\n",
    "        except Exception as e: raise ValueError(f\"Validation failed: {e}\")\n",
    "        if self.length == 0: raise ValueError(\"Input encodings are empty.\")\n",
    "        print(f\"[INFO] Created Dataset with {self.length} examples.\")\n",
    "\n",
    "    def __len__(self): return self.length\n",
    "    def __getitem__(self, idx):\n",
    "        if not 0 <= idx < self.length: raise IndexError(f\"Index {idx} out of bounds.\")\n",
    "        try: return {key: self.encodings[key][idx] for key in self.encodings}\n",
    "        except Exception as e: print(f\"[Error] Failed retrieval at index {idx}: {e}\", file=sys.stderr); raise\n",
    "\n",
    "# --- Custom MoE Layer ---\n",
    "\n",
    "class MoEFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Experts Feed-Forward Network Layer.\n",
    "\n",
    "    Routes input tokens to different 'expert' FFNs based on a gating mechanism.\n",
    "    The gating mechanism here uses the mean-pooled representation of the sequence\n",
    "    to decide the weights for combining expert outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, n_experts=4, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the MoE layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Input and output dimension of the layer.\n",
    "            d_ff (int): Intermediate dimension of the expert FFNs.\n",
    "            n_experts (int): Number of expert networks.\n",
    "            dropout_rate (float): Dropout rate for expert FFNs.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.n_experts = n_experts\n",
    "\n",
    "        # Define the expert networks (simple two-layer MLPs)\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, d_ff),\n",
    "                nn.ReLU(), # Or GeLU, SiLU etc. matching base model if known\n",
    "                nn.Dropout(dropout_rate), # Add dropout within experts\n",
    "                nn.Linear(d_ff, d_model)\n",
    "            ) for _ in range(n_experts)\n",
    "        ])\n",
    "\n",
    "        # Gating network: maps pooled sequence representation to expert weights\n",
    "        # Takes mean across sequence length dimension as input\n",
    "        self.gate = nn.Linear(d_model, n_experts)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        Forward pass through the MoE layer.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): Input tensor (batch_size, seq_len, d_model).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # hidden_states: (B, S, D)\n",
    "        batch_size, seq_len, d_model = hidden_states.shape\n",
    "\n",
    "        # 1. Gating: Use mean pooling for simplicity to get sequence-level weights\n",
    "        # Alternative: Could implement token-level gating for finer control.\n",
    "        pooled_hidden_states = hidden_states.mean(dim=1) # (B, D)\n",
    "\n",
    "        # Calculate gating scores (logits) and normalize to probabilities (weights)\n",
    "        gating_logits = self.gate(pooled_hidden_states) # (B, N_experts)\n",
    "        gating_weights = torch.softmax(gating_logits, dim=-1) # (B, N_experts)\n",
    "\n",
    "        # 2. Expert Evaluation\n",
    "        # Pass the full sequence through each expert\n",
    "        expert_outputs = []\n",
    "        for expert in self.experts:\n",
    "            expert_outputs.append(expert(hidden_states))\n",
    "        # Stack expert outputs: List[ (B, S, D) ] -> Tensor(N_experts, B, S, D)\n",
    "        expert_outputs_stacked = torch.stack(expert_outputs, dim=0)\n",
    "\n",
    "        # 3. Weighted Combination\n",
    "        # Reshape weights for broadcasting: (B, N_experts) -> (B, N_experts, 1, 1)\n",
    "        gating_weights_reshaped = gating_weights.unsqueeze(-1).unsqueeze(-1)\n",
    "        # Reshape expert outputs for broadcasting: (N_experts, B, S, D) -> (B, N_experts, S, D)\n",
    "        expert_outputs_permuted = expert_outputs_stacked.permute(1, 0, 2, 3)\n",
    "\n",
    "        # Weighted sum: (B, N_experts, 1, 1) * (B, N_experts, S, D) -> sum over N_experts axis\n",
    "        # Result: (B, S, D)\n",
    "        output = torch.sum(gating_weights_reshaped * expert_outputs_permuted, dim=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def replace_ffn_with_moe(model, n_experts):\n",
    "    \"\"\"\n",
    "    Replaces the standard FFN layers in a T5 model with MoEFeedForward layers.\n",
    "\n",
    "    Iterates through encoder and decoder blocks and replaces the appropriate layer.\n",
    "    Requires knowledge of the T5Block structure.\n",
    "\n",
    "    Args:\n",
    "        model: The T5ForConditionalGeneration model instance.\n",
    "        n_experts (int): Number of experts for the MoE layers.\n",
    "\n",
    "    Returns:\n",
    "        The modified model instance.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Patching T5 model with MoE layers (Experts: {n_experts})...\")\n",
    "    replaced_count = 0\n",
    "    try:\n",
    "        # Encoder blocks\n",
    "        if hasattr(model, 'encoder') and hasattr(model.encoder, 'block'):\n",
    "            for i, block in enumerate(model.encoder.block):\n",
    "                # T5Block structure: [LayerNorm, SelfAttention, Dropout], [LayerNorm, DenseReluDense, Dropout]\n",
    "                # The FFN (`DenseReluDense`) is typically the second layer in the second sub-block group.\n",
    "                # Accessing via attribute name is safer if known, e.g., block.layer[1].DenseReluDense\n",
    "                # Let's try accessing the assumed attribute location and verify type.\n",
    "                ffn_layer_attr = None\n",
    "                if hasattr(block, 'layer') and len(block.layer) > 1 and \\\n",
    "                   hasattr(block.layer[1], 'DenseReluDense') and \\\n",
    "                   isinstance(block.layer[1].DenseReluDense, nn.Module): # Check if it looks like the FFN component\n",
    "                    ffn_layer_attr = block.layer[1].DenseReluDense\n",
    "                    layer_target = block.layer[1]\n",
    "                    attr_name = 'DenseReluDense'\n",
    "\n",
    "                if ffn_layer_attr is not None:\n",
    "                    print(f\"  - Replacing Encoder Block {i} FFN...\")\n",
    "                    # Extract config needed for MoE layer\n",
    "                    d_model = model.config.d_model\n",
    "                    d_ff = model.config.d_ff\n",
    "                    dropout_rate = model.config.dropout_rate # Get dropout from config\n",
    "\n",
    "                    # Create and assign the new MoE layer\n",
    "                    setattr(layer_target, attr_name, MoEFeedForward(d_model, d_ff, n_experts, dropout_rate))\n",
    "                    replaced_count += 1\n",
    "                else:\n",
    "                     print(f\"  - [Warning] Could not find/replace FFN in Encoder Block {i} structure.\")\n",
    "\n",
    "        # Decoder blocks\n",
    "        if hasattr(model, 'decoder') and hasattr(model.decoder, 'block'):\n",
    "            for i, block in enumerate(model.decoder.block):\n",
    "                # T5DecoderBlock structure: [LN, SelfAttn, Drop], [LN, CrossAttn, Drop], [LN, FFN(DenseReluDense), Drop]\n",
    "                # The FFN is typically the second layer in the *third* sub-block group.\n",
    "                ffn_layer_attr = None\n",
    "                if hasattr(block, 'layer') and len(block.layer) > 2 and \\\n",
    "                   hasattr(block.layer[2], 'DenseReluDense') and \\\n",
    "                   isinstance(block.layer[2].DenseReluDense, nn.Module):\n",
    "                    ffn_layer_attr = block.layer[2].DenseReluDense\n",
    "                    layer_target = block.layer[2]\n",
    "                    attr_name = 'DenseReluDense'\n",
    "\n",
    "                if ffn_layer_attr is not None:\n",
    "                    print(f\"  - Replacing Decoder Block {i} FFN...\")\n",
    "                    d_model = model.config.d_model\n",
    "                    d_ff = model.config.d_ff\n",
    "                    dropout_rate = model.config.dropout_rate\n",
    "\n",
    "                    setattr(layer_target, attr_name, MoEFeedForward(d_model, d_ff, n_experts, dropout_rate))\n",
    "                    replaced_count += 1\n",
    "                else:\n",
    "                    print(f\"  - [Warning] Could not find/replace FFN in Decoder Block {i} structure.\")\n",
    "\n",
    "        if replaced_count == 0:\n",
    "             print(\"[Warning] No FFN layers were replaced. Check model structure and replacement logic.\")\n",
    "        else:\n",
    "             print(f\"[INFO] Successfully replaced {replaced_count} FFN layers with MoE.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed during model patching: {e}\", file=sys.stderr)\n",
    "        # Depending on severity, may want to raise or exit\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Orchestrates data loading, MoE model patching, training, and evaluation.\"\"\"\n",
    "    print(\"[INFO] Starting T5 with Mixture-of-Experts Fine-tuning Script...\")\n",
    "    # --- ***** Update Timestamp/Location ***** ---\n",
    "    try:\n",
    "        current_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "        current_time_str = current_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "    except Exception:\n",
    "        current_time = datetime.datetime.utcnow()\n",
    "        current_time_str = current_time.strftime('%Y-%m-%d %H:%M:%S UTC (naive)')\n",
    "    print(f\"[INFO] Current time: {current_time_str}\")\n",
    "    print(f\"[INFO] Location context: San Diego, CA, USA\")\n",
    "    print(f\"[INFO] Using Base Model: {MODEL_ID}\")\n",
    "\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    try:\n",
    "        train_raw = load_jsonl(TRAIN_FILE)\n",
    "        val_raw = load_jsonl(VAL_FILE)\n",
    "        test_raw = load_jsonl(TEST_FILE)\n",
    "    except Exception as e: print(f\"[FATAL] Data loading failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "    if not train_raw or not val_raw or not test_raw: print(\"[FATAL] Datasets empty after loading.\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 2. Prepare Data Strings ---\n",
    "    train_src, train_tgt = convert_tokens_to_strings(train_raw)\n",
    "    val_src,   val_tgt   = convert_tokens_to_strings(val_raw)\n",
    "    test_src,  test_tgt  = convert_tokens_to_strings(test_raw)\n",
    "    if not train_src or not val_src or not test_src: print(\"[FATAL] Data conversion failed.\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 3. Initialize Tokenizer ---\n",
    "    global tokenizer_for_metrics # For metrics function access\n",
    "    try:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_ID)\n",
    "        # T5 generally uses <pad>, </s>, <unk> - check if needed\n",
    "        if tokenizer.pad_token is None: tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "        # T5 uses EOS as decoder start token ID by default, check if BOS needed\n",
    "        # if tokenizer.bos_token is None: tokenizer.add_special_tokens({'bos_token': '<s>'})\n",
    "        tokenizer_for_metrics = tokenizer\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "    except Exception as e: print(f\"[FATAL] Tokenizer init failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "    print(f\"[INFO] Tokenizer initialized (Vocab: {tokenizer.vocab_size}, Pad ID: {pad_token_id}).\")\n",
    "\n",
    "    # --- 4. Load Base Model and Patch with MoE ---\n",
    "    print(f\"[INFO] Loading base model: {MODEL_ID}\")\n",
    "    try:\n",
    "        model = T5ForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "\n",
    "        # Patch the loaded model\n",
    "        model = replace_ffn_with_moe(model, n_experts=NUM_EXPERTS)\n",
    "\n",
    "        # Resize embeddings if vocab changed\n",
    "        if model.config.vocab_size != len(tokenizer):\n",
    "             print(f\"[INFO] Resizing model embeddings to {len(tokenizer)}\")\n",
    "             model.resize_token_embeddings(len(tokenizer))\n",
    "             model.config.vocab_size = len(tokenizer)\n",
    "\n",
    "        # Configure standard seq2seq settings (T5 specific)\n",
    "        model.config.decoder_start_token_id = tokenizer.pad_token_id # T5 starts decoder with pad\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        # Configure generation defaults\n",
    "        model.config.max_length = MAX_SEQ_LENGTH\n",
    "        model.config.num_beams = 1 # Default to greedy for logits; beams set in TrainingArgs if needed\n",
    "\n",
    "        # Enable Gradient Checkpointing if configured\n",
    "        if USE_GRADIENT_CHECKPOINTING:\n",
    "            try:\n",
    "                 model.gradient_checkpointing_enable()\n",
    "                 print(\"[INFO] Gradient Checkpointing enabled on the model.\")\n",
    "            except Exception as gc_e:\n",
    "                 print(f\"[Warning] Failed to enable gradient checkpointing on model: {gc_e}\")\n",
    "                 global USE_GRADIENT_CHECKPOINTING_EFFECTIVE\n",
    "                 USE_GRADIENT_CHECKPOINTING_EFFECTIVE = False\n",
    "        else:\n",
    "             USE_GRADIENT_CHECKPOINTING_EFFECTIVE = False\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed to initialize or patch the T5 model: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    # --- 5. Tokenize Data ---\n",
    "    try:\n",
    "        train_encodings = encode_sequences(tokenizer, train_src, train_tgt, MAX_SEQ_LENGTH)\n",
    "        val_encodings   = encode_sequences(tokenizer, val_src,   val_tgt,   MAX_SEQ_LENGTH)\n",
    "        test_encodings  = encode_sequences(tokenizer, test_src,  test_tgt,  MAX_SEQ_LENGTH)\n",
    "    except Exception as e: print(f\"[FATAL] Data tokenization failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "    if not train_encodings.get('input_ids') or not val_encodings.get('input_ids') or not test_encodings.get('input_ids'):\n",
    "        print(\"[FATAL] Tokenization resulted in empty encodings.\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 6. Create Datasets ---\n",
    "    try:\n",
    "        train_dataset = SequencePairDataset(train_encodings)\n",
    "        val_dataset   = SequencePairDataset(val_encodings)\n",
    "        test_dataset  = SequencePairDataset(test_encodings)\n",
    "    except ValueError as e: print(f\"[FATAL] Dataset creation failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 7. Initialize Data Collator ---\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100, # Ignore pad tokens in loss\n",
    "        pad_to_multiple_of=8 if USE_FP16 else None\n",
    "    )\n",
    "    print(\"[INFO] Data collator initialized.\")\n",
    "\n",
    "    # --- 8. Define Metrics Computation (from Logits) ---\n",
    "    def compute_metrics_fn(eval_pred):\n",
    "        \"\"\"Calculates exact sequence match accuracy from logits.\"\"\"\n",
    "        logits, labels = eval_pred # Trainer passes logits when predict_with_generate=False\n",
    "        if not isinstance(logits, np.ndarray): logits = np.array(logits)\n",
    "        if not isinstance(labels, np.ndarray): labels = np.array(labels)\n",
    "\n",
    "        # Replace -100 in labels for accuracy calculation comparison if needed,\n",
    "        # but accuracy function should handle pad_token_id masking.\n",
    "        # labels_for_acc = np.where(labels != -100, labels, tokenizer_for_metrics.pad_token_id)\n",
    "\n",
    "        try:\n",
    "            # Use PyTorch tensor version for consistency and potential device placement\n",
    "            logits_torch = torch.from_numpy(logits)\n",
    "            labels_torch = torch.from_numpy(labels)\n",
    "            # Pass the original labels (with -100 potentially) and pad_token_id\n",
    "            accuracy = calculate_sequence_accuracy_torch(logits_torch, labels_torch, tokenizer_for_metrics.pad_token_id)\n",
    "            return {\"sequence_accuracy\": accuracy}\n",
    "        except Exception as met_e:\n",
    "             print(f\"[Error] compute_metrics failed: {met_e}\", file=sys.stderr)\n",
    "             return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "    def calculate_sequence_accuracy_torch(logits, labels, pad_token_id):\n",
    "        \"\"\"Calculates exact sequence match accuracy using PyTorch tensors.\"\"\"\n",
    "        # Ensure inputs are tensors\n",
    "        if not isinstance(logits, torch.Tensor): logits = torch.tensor(logits)\n",
    "        if not isinstance(labels, torch.Tensor): labels = torch.tensor(labels)\n",
    "\n",
    "        if logits.shape[0] == 0: return 0.0\n",
    "        predictions = torch.argmax(logits, dim=-1) # (Batch, SeqLen)\n",
    "\n",
    "        # Create mask for non-padding tokens in labels (use pad_token_id directly)\n",
    "        # Labels might contain -100, treat them as padding for accuracy check\n",
    "        non_pad_mask = (labels != pad_token_id) & (labels != -100)\n",
    "\n",
    "        correct_tokens = (predictions == labels) & non_pad_mask\n",
    "        correct_sequences = (torch.sum(correct_tokens, dim=1) == torch.sum(non_pad_mask, dim=1))\n",
    "        accuracy = torch.mean(correct_sequences.float())\n",
    "        return accuracy.item()\n",
    "\n",
    "\n",
    "    # --- 9. Define Training Arguments ---\n",
    "    effective_gc = USE_GRADIENT_CHECKPOINTING if 'USE_GRADIENT_CHECKPOINTING_EFFECTIVE' not in globals() else USE_GRADIENT_CHECKPOINTING_EFFECTIVE\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR),\n",
    "        # Schedule\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        # Logging / Saving / Evaluation\n",
    "        logging_dir=str(OUTPUT_DIR / 'logs'),\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        evaluation_strategy=EVALUATION_STRATEGY,\n",
    "        save_strategy=SAVE_STRATEGY,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"sequence_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        # Use logits for evaluation (faster if only accuracy needed)\n",
    "        predict_with_generate=False,\n",
    "        # Advanced features\n",
    "        fp16=USE_FP16,\n",
    "        label_smoothing_factor=LABEL_SMOOTHING_FACTOR,\n",
    "        gradient_checkpointing=effective_gc,\n",
    "        # report_to=\"tensorboard\", # Optional\n",
    "    )\n",
    "    print(\"[INFO] Training arguments defined.\")\n",
    "    print(f\"[INFO] Effective Mixed Precision (FP16): {'Enabled' if USE_FP16 else 'Disabled'}\")\n",
    "    print(f\"[INFO] Effective Label Smoothing Factor: {LABEL_SMOOTHING_FACTOR}\")\n",
    "    print(f\"[INFO] Effective Gradient Checkpointing in Args: {training_args.gradient_checkpointing}\")\n",
    "\n",
    "    # --- 10. Initialize Trainer ---\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "    )\n",
    "    print(\"[INFO] Seq2SeqTrainer initialized.\")\n",
    "\n",
    "    # --- 11. Train the Model ---\n",
    "    print(f\"[INFO] Starting model training ({NUM_TRAIN_EPOCHS} epochs)...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics); trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(\"[INFO] Training finished successfully.\")\n",
    "        if trainer.state.best_model_checkpoint: print(f\"[INFO] Best model checkpoint: {trainer.state.best_model_checkpoint}\")\n",
    "    except Exception as e: print(f\"[FATAL] Training failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 12. Evaluate on Test Set ---\n",
    "    print(\"[INFO] Evaluating final model on the test set...\")\n",
    "    try:\n",
    "        test_results = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")\n",
    "        trainer.log_metrics(\"test\", test_results); trainer.save_metrics(\"test\", test_results)\n",
    "        if 'test_sequence_accuracy' in test_results:\n",
    "             print(f\"\\n--- Test Set Results ---\"); print(f\"Test Sequence Accuracy: {test_results['test_sequence_accuracy']:.4f}\"); print(f\"-----------------------------\")\n",
    "        else: print(\"[Warning] Test accuracy metric not found.\", \"Results:\", test_results)\n",
    "    except Exception as e: print(f\"[Error] Final evaluation failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    print(\"\\n[INFO] Script finished successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Optional: Seeds for reproducibility\n",
    "    # SEED = 42; random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "    # if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t5_pip_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
