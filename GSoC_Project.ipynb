{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic AI Tests 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Task 1.1. Dataset preprocessing \n",
    "Dataset:\n",
    "\n",
    "https://space.mit.edu/home/tegmark/aifeynman.html \n",
    "Note: The authors of this dataset are not affiliated with ML4SCI\n",
    "\n",
    "Description:\n",
    "Download the Feynman_with_units.tar.gz features and corresponding FeynmanEquations.csv targets. Preprocess and tokenize the target data and document your rationale for choice of tokenization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Script Start ---\n",
      "\n",
      "[Step 1] Setting up directories and validating paths...\n",
      "  Input CSV: /home/nikitas/Desktop/send_Miche/GOOGLE/FeynmanEquations.csv\n",
      "  Numeric Data Dir: /home/nikitas/Desktop/send_Miche/GOOGLE/Feynman_with_units\n",
      "  Tokenizer Output Dir: feynman_tokenizer\n",
      "\n",
      "[Step 2] Loading filenames from 'Filename' column and validating CSV...\n",
      "  CSV Header: ['Filename', 'Number', 'Output', 'Formula', '# variables', 'v1_name', 'v1_low', 'v1_high', 'v2_name', 'v2_low', 'v2_high', 'v3_name', 'v3_low', 'v3_high', 'v4_name', 'v4_low', 'v4_high', 'v5_name', 'v5_low', 'v5_high', 'v6_name', 'v6_low', 'v6_high', 'v7_name', 'v7_low', 'v7_high', 'v8_name', 'v8_low', 'v8_high', 'v9_name', 'v9_low', 'v9_high', 'v10_name', 'v10_low', 'v10_high']\n",
      "  Successfully loaded 100 non-empty filenames.\n",
      "\n",
      "[Step 3] Training Byte-Level BPE Tokenizer on 'Formula' column...\n",
      "  Starting training with vocab_size=10000, min_frequency=2\n",
      "   [Iterator] Initializing for column: 'Formula'...\n",
      "   [Iterator] Finished yielding 100 equations.\n",
      "\n",
      "\n",
      "\n",
      "  Tokenizer training complete. Vocabulary size: 330\n",
      "  Tokenizer vocabulary and merges saved to: feynman_tokenizer\n",
      "  Full tokenizer config saved to: feynman_tokenizer/tokenizer.json\n",
      "\n",
      "[Step 4] Wrapping tokenizer with Hugging Face PreTrainedTokenizerFast...\n",
      "  Hugging Face tokenizer wrapper created successfully.\n",
      "  Testing tokenizer on the first valid equation...\n",
      "   [Iterator] Initializing for column: 'Formula'...\n",
      "    Equation Sample (from 'Formula'): exp(-theta**2/2)/sqrt(2*pi)\n",
      "    → Tokens (14): ['exp', '(-', 'theta', '**', '2', '/', '2', ')/', 'sqrt', '(', '2', '*', 'pi', ')']\n",
      "    → IDs (14): [290, 304, 283, 261, 22, 19, 22, 298, 281, 12, 22, 14, 264, 13]\n",
      "\n",
      "[Step 5] Mapping filenames to numeric data files in '/home/nikitas/Desktop/send_Miche/GOOGLE/Feynman_with_units'...\n",
      "  Checking for 100 potential files...\n",
      "  Checked 100 filenames.\n",
      "  Found 100 corresponding numeric files.\n",
      "  Example mapping: 'I.6.2a' -> '/home/nikitas/Desktop/send_Miche/GOOGLE/Feynman_with_units/I.6.2a'\n",
      "\n",
      "[Step 6] Usage Notes\n",
      "--------------------\n",
      " - Equations Tokenization: Use the 'hf_tokenizer' object.\n",
      "   Example: `encoded = hf_tokenizer(list_of_eq_strings, padding=True, truncation=True, max_length=128)`\n",
      " - Numeric Data Loading: Use the 'numeric_file_paths' dictionary and the `load_numeric_data` function.\n",
      "   Example: `data = load_numeric_data('some_filename.txt', numeric_file_paths)`\n",
      "   (Ensure 'some_filename.txt' is a key in the dictionary)\n",
      "--------------------\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Script to train a Byte-Level BPE tokenizer on LaTeX equations from a CSV file\n",
    "and prepare file paths for corresponding numeric data.\n",
    "\n",
    "This script performs the following steps:\n",
    "1. Defines paths for input CSV, numeric data directory, and tokenizer output.\n",
    "2. Loads filenames and validates essential columns from the input CSV.\n",
    "3. Trains a ByteLevelBPETokenizer on the equation strings found in the CSV.\n",
    "4. Saves the trained tokenizer (vocab, merges, full config).\n",
    "5. Wraps the trained tokenizer using Hugging Face's PreTrainedTokenizerFast.\n",
    "6. Tests the wrapped tokenizer on a sample equation.\n",
    "7. Maps filenames from the CSV to their corresponding numeric data files.\n",
    "8. Provides usage notes for the tokenizer and data loading.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gc # For explicit garbage collection, if needed (see notes).\n",
    "import sys # For exiting script on critical errors.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Input Paths\n",
    "# Path to the CSV file containing filenames and LaTeX equations.\n",
    "CSV_PATH = Path(r\"/home/nikitas/Desktop/send_Miche/GOOGLE/FeynmanEquations.csv\")\n",
    "# Directory containing the corresponding numeric data files (named according to the 'Filename' column).\n",
    "NUMERIC_DATA_DIR = Path(r\"/home/nikitas/Desktop/send_Miche/GOOGLE/Feynman_with_units\")\n",
    "\n",
    "# Output Path\n",
    "# Directory where the trained tokenizer files will be saved.\n",
    "TOKENIZER_OUTPUT_DIR = Path(\"feynman_tokenizer\")\n",
    "\n",
    "# CSV Column Names\n",
    "# Ensure these exactly match the headers in your CSV file.\n",
    "FILENAME_COLUMN = 'Filename' # Column containing the base name of the numeric files.\n",
    "EQUATION_COLUMN = 'Formula'  # Column containing the LaTeX equation strings.\n",
    "\n",
    "# Tokenizer Settings\n",
    "VOCAB_SIZE = 10_000     # Desired vocabulary size for the BPE tokenizer.\n",
    "MIN_FREQUENCY = 2       # Minimum frequency for a token to be included in the vocab.\n",
    "SPECIAL_TOKENS = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"] # Standard special tokens.\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def equation_iterator(csv_file_path, target_column):\n",
    "    \"\"\"\n",
    "    Creates a generator to efficiently yield equations from a specific column\n",
    "    in a potentially large CSV file.\n",
    "\n",
    "    Reads the CSV in chunks to minimize memory usage. Skips NaN or empty values.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (Path): Path object pointing to the CSV file.\n",
    "        target_column (str): The exact name of the column containing equations.\n",
    "\n",
    "    Yields:\n",
    "        str: Non-empty equation strings from the specified column.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the csv_file_path does not exist (handled internally).\n",
    "        KeyError: If the target_column is not found in the CSV (handled internally).\n",
    "        Exception: For other pandas read errors (handled internally).\n",
    "    \"\"\"\n",
    "    chunk_size = 1000 # Process N rows at a time. Adjust based on memory constraints.\n",
    "    print(f\"   [Iterator] Initializing for column: '{target_column}'...\")\n",
    "\n",
    "    if not csv_file_path.is_file():\n",
    "        print(f\"   [Iterator Error] CSV file not found at: {csv_file_path}\")\n",
    "        return # Stop iteration if file is missing\n",
    "\n",
    "    try:\n",
    "        # Use chunking for memory efficiency.\n",
    "        # skipinitialspace handles potential spaces after delimiters.\n",
    "        iterator = pd.read_csv(\n",
    "            csv_file_path,\n",
    "            usecols=[target_column],\n",
    "            chunksize=chunk_size,\n",
    "            skipinitialspace=True,\n",
    "            low_memory=False # Can sometimes help with mixed types, but monitor memory.\n",
    "        )\n",
    "\n",
    "        processed_count = 0\n",
    "        for chunk in iterator:\n",
    "            for equation in chunk[target_column]:\n",
    "                # Convert to string and check for NaN/null/empty values\n",
    "                eq_str = str(equation).strip()\n",
    "                if pd.isna(equation) or not eq_str or eq_str.lower() == 'nan':\n",
    "                    # Optionally log skipped values if debugging is needed\n",
    "                    # print(f\"   [Iterator] Skipping empty/NaN value.\")\n",
    "                    continue\n",
    "                yield eq_str\n",
    "                processed_count += 1\n",
    "\n",
    "            # Optional: Explicit garbage collection after processing a large chunk.\n",
    "            # This is often not strictly necessary as Python's GC handles objects\n",
    "            # going out of scope, but can sometimes be helpful in tight memory\n",
    "            # situations or long-running processes. Profile if considering uncommenting.\n",
    "            # gc.collect()\n",
    "\n",
    "        print(f\"   [Iterator] Finished yielding {processed_count} equations.\")\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"   [Iterator Error] Column '{target_column}' not found in CSV: {csv_file_path}\")\n",
    "        # Stop iteration if the required column is missing\n",
    "        return\n",
    "    except Exception as e:\n",
    "        # Catch other potential errors during file reading or processing\n",
    "        print(f\"   [Iterator Error] Failed reading chunks from {csv_file_path} using column '{target_column}': {e}\")\n",
    "        # Stop iteration on unexpected errors\n",
    "        return\n",
    "\n",
    "\n",
    "def load_numeric_data(filename, paths_dict):\n",
    "    \"\"\"\n",
    "    Loads numeric data from a file specified by its filename.\n",
    "\n",
    "    Assumes the file contains numeric data loadable by np.loadtxt.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The base filename (e.g., 'data1.txt').\n",
    "        paths_dict (dict): A dictionary mapping filenames to their full Path objects.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The loaded numpy array if successful.\n",
    "        None: If the file is not found in the dictionary or cannot be loaded.\n",
    "    \"\"\"\n",
    "    file_path = paths_dict.get(filename)\n",
    "    if file_path is None:\n",
    "        print(f\"   [Data Loader Warning] No path found for numeric file '{filename}'.\")\n",
    "        return None\n",
    "    if not file_path.is_file():\n",
    "        print(f\"   [Data Loader Warning] File path exists in map but file not found on disk: '{file_path}'.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Load the data, assuming standard text format for numpy\n",
    "        data_array = np.loadtxt(file_path)\n",
    "        return data_array\n",
    "    except Exception as e:\n",
    "        print(f\"   [Data Loader Error] Failed to load numeric file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main Script Execution ---\n",
    "\n",
    "print(\"--- Script Start ---\")\n",
    "\n",
    "# 1. Setup and Path Validation\n",
    "print(\"\\n[Step 1] Setting up directories and validating paths...\")\n",
    "TOKENIZER_OUTPUT_DIR.mkdir(parents=True, exist_ok=True) # Ensure output dir exists\n",
    "\n",
    "if not CSV_PATH.is_file():\n",
    "    print(f\"[Error] Input CSV file not found: {CSV_PATH}\")\n",
    "    sys.exit(1) # Critical error, cannot proceed\n",
    "if not NUMERIC_DATA_DIR.is_dir():\n",
    "    print(f\"[Warning] Numeric data directory not found: {NUMERIC_DATA_DIR}\")\n",
    "    # Allow script to continue, but numeric file mapping will likely fail.\n",
    "\n",
    "print(f\"  Input CSV: {CSV_PATH}\")\n",
    "print(f\"  Numeric Data Dir: {NUMERIC_DATA_DIR}\")\n",
    "print(f\"  Tokenizer Output Dir: {TOKENIZER_OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "# 2. Load Filenames and Validate CSV Structure\n",
    "print(f\"\\n[Step 2] Loading filenames from '{FILENAME_COLUMN}' column and validating CSV...\")\n",
    "filenames = []\n",
    "try:\n",
    "    # Quickly read header to check for essential columns before loading data\n",
    "    header = pd.read_csv(CSV_PATH, nrows=0, skipinitialspace=True).columns.tolist()\n",
    "    print(f\"  CSV Header: {header}\")\n",
    "    if FILENAME_COLUMN not in header:\n",
    "        raise ValueError(f\"Required filename column '{FILENAME_COLUMN}' not found in header.\")\n",
    "    if EQUATION_COLUMN not in header:\n",
    "        raise ValueError(f\"Required equation column '{EQUATION_COLUMN}' not found in header.\")\n",
    "\n",
    "    # Load only the necessary filename column\n",
    "    filenames_series = pd.read_csv(CSV_PATH, usecols=[FILENAME_COLUMN], skipinitialspace=True)[FILENAME_COLUMN]\n",
    "    # Convert to list of strings, handling potential non-string types gracefully\n",
    "    filenames = [str(fn).strip() for fn in filenames_series.tolist() if pd.notna(fn)]\n",
    "    # Filter out any empty strings that might result\n",
    "    filenames = [fn for fn in filenames if fn]\n",
    "\n",
    "    print(f\"  Successfully loaded {len(filenames)} non-empty filenames.\")\n",
    "    if not filenames:\n",
    "         print(\"  [Warning] No valid filenames were loaded from the CSV.\")\n",
    "\n",
    "except FileNotFoundError: # Should be caught by Step 1, but as fallback.\n",
    "    print(f\"[Error] CSV file disappeared after initial check: {CSV_PATH}\")\n",
    "    sys.exit(1)\n",
    "except ValueError as ve: # Specific error for missing columns\n",
    "    print(f\"[Error] CSV Column Validation Failed: {ve}\")\n",
    "    sys.exit(1) # Critical error if required columns are missing\n",
    "except Exception as e:\n",
    "    print(f\"[Error] Failed to read CSV header or filenames column '{FILENAME_COLUMN}': {e}\")\n",
    "    # Depending on the error, decide if exit is needed.\n",
    "    # If filenames are non-critical for a part of the process, could warn instead.\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# 3. Train BPE Tokenizer\n",
    "print(f\"\\n[Step 3] Training Byte-Level BPE Tokenizer on '{EQUATION_COLUMN}' column...\")\n",
    "\n",
    "# Create the BPE tokenizer instance\n",
    "bpe_tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "print(f\"  Starting training with vocab_size={VOCAB_SIZE}, min_frequency={MIN_FREQUENCY}\")\n",
    "try:\n",
    "    # Train using the efficient iterator\n",
    "    # The iterator handles its own errors internally (e.g., column not found)\n",
    "    bpe_tokenizer.train_from_iterator(\n",
    "        equation_iterator(CSV_PATH, EQUATION_COLUMN),\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        min_frequency=MIN_FREQUENCY,\n",
    "        special_tokens=SPECIAL_TOKENS\n",
    "    )\n",
    "\n",
    "    print(f\"  Tokenizer training complete. Vocabulary size: {bpe_tokenizer.get_vocab_size()}\")\n",
    "\n",
    "    # Save the tokenizer components (vocabulary and merge rules)\n",
    "    # Use str() for compatibility with older library versions if needed\n",
    "    bpe_tokenizer.save_model(str(TOKENIZER_OUTPUT_DIR))\n",
    "    print(f\"  Tokenizer vocabulary and merges saved to: {TOKENIZER_OUTPUT_DIR}\")\n",
    "\n",
    "    # Save the full tokenizer configuration as tokenizer.json for Hugging Face\n",
    "    tokenizer_json_path = TOKENIZER_OUTPUT_DIR / \"tokenizer.json\"\n",
    "    bpe_tokenizer._tokenizer.save(str(tokenizer_json_path))\n",
    "    print(f\"  Full tokenizer config saved to: {tokenizer_json_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[Error] Tokenizer training failed: {e}\")\n",
    "    # Decide if script should stop; likely yes if tokenizer is core goal.\n",
    "    tokenizer_json_path = None # Ensure path variable is None if saving failed\n",
    "    print(\"  [Warning] Skipping subsequent steps that depend on the tokenizer.\")\n",
    "\n",
    "\n",
    "# 4. Wrap with PreTrainedTokenizerFast (Hugging Face)\n",
    "print(\"\\n[Step 4] Wrapping tokenizer with Hugging Face PreTrainedTokenizerFast...\")\n",
    "\n",
    "hf_tokenizer = None # Initialize to None\n",
    "if 'tokenizer_json_path' in locals() and tokenizer_json_path and tokenizer_json_path.exists():\n",
    "    try:\n",
    "        hf_tokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_file=str(tokenizer_json_path),\n",
    "            bos_token=\"<s>\", # Begin-of-sequence\n",
    "            eos_token=\"</s>\", # End-of-sequence\n",
    "            unk_token=\"<unk>\", # Unknown token\n",
    "            pad_token=\"<pad>\", # Padding token\n",
    "            mask_token=\"<mask>\", # Mask token (if used in models like BERT)\n",
    "        )\n",
    "        print(\"  Hugging Face tokenizer wrapper created successfully.\")\n",
    "\n",
    "        # Perform a quick test on the first available equation\n",
    "        print(\"  Testing tokenizer on the first valid equation...\")\n",
    "        try:\n",
    "            # Get the first valid equation using a fresh iterator instance\n",
    "            test_iterator = equation_iterator(CSV_PATH, EQUATION_COLUMN)\n",
    "            first_equation = next(test_iterator, None) # Use default=None to avoid StopIteration\n",
    "            del test_iterator # Clean up iterator\n",
    "\n",
    "            if first_equation:\n",
    "                print(f\"    Equation Sample (from '{EQUATION_COLUMN}'): {first_equation}\")\n",
    "                tokens = hf_tokenizer.tokenize(first_equation)\n",
    "                print(f\"    → Tokens ({len(tokens)}): {tokens}\")\n",
    "                encoded_ids = hf_tokenizer.encode(first_equation)\n",
    "                print(f\"    → IDs ({len(encoded_ids)}): {encoded_ids}\")\n",
    "            else:\n",
    "                print(\"    [Warning] Could not retrieve the first equation for testing (CSV empty or contains no valid equations?).\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    [Warning] Error during tokenizer test on first equation: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load tokenizer with PreTrainedTokenizerFast: {e}\")\n",
    "        # hf_tokenizer remains None\n",
    "\n",
    "else:\n",
    "     print(\"  [Skipped] Tokenizer file 'tokenizer.json' not found. Likely due to errors in Step 3.\")\n",
    "\n",
    "\n",
    "# 5. Map Filenames to Numeric Data Files\n",
    "print(f\"\\n[Step 5] Mapping filenames to numeric data files in '{NUMERIC_DATA_DIR}'...\")\n",
    "\n",
    "numeric_file_paths = {}\n",
    "missing_files_count = 0\n",
    "checked_files_count = 0\n",
    "\n",
    "if not filenames:\n",
    "     print(\"  [Warning] No filenames loaded in Step 2. Cannot map numeric files.\")\n",
    "elif not NUMERIC_DATA_DIR.is_dir():\n",
    "     print(f\"  [Warning] Numeric data directory '{NUMERIC_DATA_DIR}' not found. Cannot map files.\")\n",
    "else:\n",
    "    print(f\"  Checking for {len(filenames)} potential files...\")\n",
    "    for fn in filenames:\n",
    "        checked_files_count += 1\n",
    "        # Construct the full expected path\n",
    "        potential_path = NUMERIC_DATA_DIR / fn\n",
    "        if potential_path.is_file():\n",
    "            numeric_file_paths[fn] = potential_path\n",
    "        else:\n",
    "            # Only log missing files if verbose logging is needed, otherwise just count.\n",
    "            # if missing_files_count < 10: # Log first few missing\n",
    "            #     print(f\"    - Missing: {fn} (expected at {potential_path})\")\n",
    "            # elif missing_files_count == 10:\n",
    "            #      print(\"    - ... (further missing files not listed)\")\n",
    "            missing_files_count += 1\n",
    "\n",
    "    found_files_count = len(numeric_file_paths)\n",
    "    print(f\"  Checked {checked_files_count} filenames.\")\n",
    "    print(f\"  Found {found_files_count} corresponding numeric files.\")\n",
    "    if missing_files_count > 0:\n",
    "        print(f\"  [Warning] Could not find {missing_files_count} numeric files in '{NUMERIC_DATA_DIR}'.\")\n",
    "\n",
    "    # Show an example path if available\n",
    "    if numeric_file_paths:\n",
    "         example_fn = next(iter(numeric_file_paths)) # Get first key\n",
    "         print(f\"  Example mapping: '{example_fn}' -> '{numeric_file_paths[example_fn]}'\")\n",
    "\n",
    "\n",
    "# 6. Usage Information\n",
    "print(\"\\n[Step 6] Usage Notes\")\n",
    "print(\"-\" * 20)\n",
    "if hf_tokenizer:\n",
    "    print(\" - Equations Tokenization: Use the 'hf_tokenizer' object.\")\n",
    "    print(\"   Example: `encoded = hf_tokenizer(list_of_eq_strings, padding=True, truncation=True, max_length=128)`\")\n",
    "else:\n",
    "    print(\" - Equations Tokenization: FAILED (hf_tokenizer was not created).\")\n",
    "\n",
    "if numeric_file_paths:\n",
    "    print(\" - Numeric Data Loading: Use the 'numeric_file_paths' dictionary and the `load_numeric_data` function.\")\n",
    "    print(\"   Example: `data = load_numeric_data('some_filename.txt', numeric_file_paths)`\")\n",
    "    print(\"   (Ensure 'some_filename.txt' is a key in the dictionary)\")\n",
    "elif filenames: # Filenames were loaded, but mapping failed\n",
    "    print(\" - Numeric Data Loading: Mapping FAILED (numeric_file_paths dictionary is empty or was not created).\")\n",
    "    print(\"   Check Step 5 warnings and the NUMERIC_DATA_DIR path.\")\n",
    "else: # No filenames loaded\n",
    "     print(\" - Numeric Data Loading: Cannot load numeric data (no filenames loaded from CSV in Step 2).\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Task 1.2 Dataset preprocessing\n",
    "Dataset:\n",
    "\n",
    "https://alabama.box.com/s/xhgr2onrn503jyse2fs5vxtapg0oifcs \n",
    "\n",
    "Dataset:\n",
    "Download the dataset (split across 10 files) and preprocess and tokenize the target data and document your rationale for choice of tokenization. Data file is formatted with rows like \n",
    "“event type : Feynman diagram : amplitude : squared amplitude”\n",
    "Here the amplitudes are the input sequences and squared amplitudes are the target sequences. Note that indices like _123456 grow over the course of the dataset and should be normalized for each amplitude and squared amplitude. Use an 80-10-10 split of train-val-test across all files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting QED expression preprocessing script...\n",
      "[INFO] Random seed set to: 42\n",
      "[INFO] Loading data from pattern: /home/nikitas/Desktop/send_Miche/GOOGLE/SYMBA - Test Data-selected/QED-2-to-2-diag-TreeLevel-*.txt\n",
      "[INFO] Found 10 files matching pattern. Processing...\n",
      "[INFO] Processed 10 files.\n",
      "[INFO] Successfully loaded and preprocessed 15552 examples.\n",
      "[INFO] Splitting and saving dataset with prefix 'qed_expressions'...\n",
      "[INFO] Splitting data: 12441 train, 1555 validation, 1556 test examples.\n",
      "  Successfully saved train set to: qed_expressions_train.jsonl\n",
      "  Successfully saved val set to: qed_expressions_val.jsonl\n",
      "  Successfully saved test set to: qed_expressions_test.jsonl\n",
      "[INFO] Script finished successfully.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Data Preprocessing and Splitting Pipeline for QED 2-to-2 Tree-Level Expressions.\n",
    "\n",
    "This script performs the following actions:\n",
    "1.  Loads raw text data containing QED amplitude and squared-amplitude expressions\n",
    "    from files matching a specified pattern.\n",
    "2.  Parses each line, expecting a specific format (event:diagram:amplitude:squared_amplitude).\n",
    "3.  Applies index normalization to numeric subscripts (e.g., _123 -> _0) within\n",
    "    each expression separately to reduce vocabulary size and improve generalization.\n",
    "4.  Tokenizes the normalized expressions using a custom regex designed for\n",
    "    mathematical and symbolic content (identifiers, numbers, operators).\n",
    "5.  Structures the tokenized data into input/target pairs.\n",
    "6.  Randomly shuffles the dataset.\n",
    "7.  Splits the data into training, validation, and testing sets based on specified fractions.\n",
    "8.  Saves each dataset split into a separate JSON Lines (.jsonl) file.\n",
    "\n",
    "Rationale for Design Choices:\n",
    "-   Custom Regex Tokenization: Provides fine-grained control over token boundaries,\n",
    "    effectively separating mathematical identifiers (e.g., m_e, gamma_1), numbers,\n",
    "    operators (+, -, *, /), and punctuation crucial for representing the\n",
    "    structure of the expressions. Avoids reliance on external subword models,\n",
    "    leading to a potentially smaller, more interpretable vocabulary specific to\n",
    "    the domain.\n",
    "-   Index Normalization: Numeric indices attached to symbols (e.g., `p_1`, `k_23`)\n",
    "    can grow arbitrarily large across a large dataset. Normalizing these indices\n",
    "    within each *individual* expression (e.g., `p_1`, `k_23` -> `p_0`, `k_1`)\n",
    "    prevents the vocabulary from exploding and discourages the model from merely\n",
    "    memorizing absolute index values, promoting focus on the symbolic structure.\n",
    "-   Standard Splits: An 80/10/10 split for train/validation/test sets allows for\n",
    "    standard model training, hyperparameter tuning, and final performance evaluation.\n",
    "-   JSON Lines Format: A convenient format for storing sequence data, where each\n",
    "    line is an independent JSON object, facilitating easy reading and processing.\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Input data pattern (use raw string r\"...\" for Windows paths if needed)\n",
    "# Update this path to point to your dataset files.\n",
    "# Example Linux/macOS: \"/path/to/data/QED-2-to-2-diag-TreeLevel-*.txt\"\n",
    "# Example Windows: r\"C:\\path\\to\\data\\QED-2-to-2-diag-TreeLevel-*.txt\"\n",
    "FILES_PATTERN = r\"/home/nikitas/Desktop/send_Miche/GOOGLE/SYMBA - Test Data-selected/QED-2-to-2-diag-TreeLevel-*.txt\"\n",
    "\n",
    "# Output file prefix (e.g., 'qed_data' -> 'qed_data_train.jsonl', etc.)\n",
    "OUTPUT_PREFIX = 'qed_expressions'\n",
    "\n",
    "# Dataset split ratios\n",
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC = 0.1\n",
    "# TEST_FRAC is implicitly calculated as 1.0 - TRAIN_FRAC - VAL_FRAC\n",
    "\n",
    "# Random seed for reproducibility of shuffling and splitting\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# --- Constants ---\n",
    "\n",
    "# Regex pattern to capture tokens:\n",
    "# - Group 1: Identifiers ([A-Za-z_]\\w*) - Starts with letter or underscore, followed by word chars.\n",
    "# - Group 2: Numbers (\\d+) - Integer numbers.\n",
    "# - Group 3: Operators/Special Chars (\\*\\*|\\^|[+\\-*/=()_,:]) - Includes power (**), caret (^),\n",
    "#           basic arithmetic, parentheses, comma, colon. Added '^' explicitly.\n",
    "TOKEN_PATTERN = re.compile(r\"([A-Za-z_]\\w*|\\d+|\\*\\*|\\^|[+\\-*/=()_,:])\")\n",
    "\n",
    "# Regex pattern to find numeric subscripts (e.g., \"_123\", \"_4\")\n",
    "INDEX_PATTERN = re.compile(r'_\\d+')\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def tokenize(expression_string):\n",
    "    \"\"\"\n",
    "    Tokenizes a mathematical expression string using the pre-defined TOKEN_PATTERN.\n",
    "\n",
    "    Args:\n",
    "        expression_string (str): The mathematical expression to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of tokens found in the expression. Returns an empty\n",
    "                   list if the input is empty or None.\n",
    "    \"\"\"\n",
    "    if not expression_string:\n",
    "        return []\n",
    "    return TOKEN_PATTERN.findall(expression_string)\n",
    "\n",
    "def normalize_indices(expression_string):\n",
    "    \"\"\"\n",
    "    Normalizes numeric subscripts within an expression string.\n",
    "\n",
    "    Finds all occurrences of `_ suivi par des chiffres` (e.g., `_1`, `_123`) and replaces\n",
    "    them sequentially with `_0`, `_1`, `_2`, ... within the context of this single\n",
    "    expression. This ensures that the same logical index appearing multiple times\n",
    "    (e.g., `p_1 ... p_1`) gets the same normalized index (e.g., `p_0 ... p_0`).\n",
    "\n",
    "    Args:\n",
    "        expression_string (str): The expression containing potential numeric subscripts.\n",
    "\n",
    "    Returns:\n",
    "        str: The expression with its numeric subscripts normalized.\n",
    "    \"\"\"\n",
    "    if not expression_string:\n",
    "        return \"\"\n",
    "\n",
    "    # A dictionary to map original indices (e.g., '_123') to normalized ones ('_0').\n",
    "    index_mapping = {}\n",
    "    # Using a list for 'counter' allows modification within the nested function (_repl).\n",
    "    # This is a common technique to emulate mutable integer state in closures.\n",
    "    counter = [0]\n",
    "\n",
    "    def _replace_match(match):\n",
    "        \"\"\"Nested function to handle the replacement logic for re.sub.\"\"\"\n",
    "        original_index = match.group(0) # The matched subscript (e.g., '_123')\n",
    "        if original_index not in index_mapping:\n",
    "            # Assign the next available normalized index if this is the first time\n",
    "            # we encounter this specific original index within this expression.\n",
    "            normalized_index = f\"_{counter[0]}\"\n",
    "            index_mapping[original_index] = normalized_index\n",
    "            counter[0] += 1\n",
    "        # Return the mapped normalized index (either newly created or previously stored).\n",
    "        return index_mapping[original_index]\n",
    "\n",
    "    # Substitute all occurrences using the _replace_match logic.\n",
    "    return INDEX_PATTERN.sub(_replace_match, expression_string)\n",
    "\n",
    "def load_and_preprocess(file_glob_pattern):\n",
    "    \"\"\"\n",
    "    Loads data from files matching the pattern, preprocesses, and tokenizes it.\n",
    "\n",
    "    Expects each non-empty line in the files to have the format:\n",
    "    'event_info : diagram_info : amplitude_expression : squared_amplitude_expression'\n",
    "\n",
    "    Lines not matching this format are skipped, and a warning is printed.\n",
    "\n",
    "    Args:\n",
    "        file_glob_pattern (str): A glob pattern matching the input text files.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, where each dictionary represents\n",
    "                    a preprocessed data point with keys 'input_tokens'\n",
    "                    (from amplitude) and 'target_tokens' (from squared amplitude).\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    skipped_lines = 0\n",
    "    processed_files = 0\n",
    "\n",
    "    # Use sorted() for deterministic file processing order, helpful for debugging.\n",
    "    file_paths = sorted(glob.glob(file_glob_pattern))\n",
    "\n",
    "    if not file_paths:\n",
    "        print(f\"[Warning] No files found matching pattern: {file_glob_pattern}\")\n",
    "        return []\n",
    "\n",
    "    print(f\"[INFO] Found {len(file_paths)} files matching pattern. Processing...\")\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        processed_files += 1\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                for line_num, line in enumerate(infile, 1):\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue # Skip empty lines\n",
    "\n",
    "                    parts = line.split(' : ')\n",
    "                    # Expecting 4 parts based on the specified format\n",
    "                    if len(parts) != 4:\n",
    "                        # print(f\"[Warning] Skipping malformed line {line_num} in {file_path}: Expected 4 parts separated by ' : ', found {len(parts)}. Content: '{line[:100]}...'\")\n",
    "                        skipped_lines += 1\n",
    "                        continue\n",
    "\n",
    "                    # Unpack (we only need the expressions for tokenization)\n",
    "                    _event_info, _diagram_info, amplitude_expr, sq_amplitude_expr = parts\n",
    "\n",
    "                    # 1. Normalize indices\n",
    "                    normalized_amplitude = normalize_indices(amplitude_expr)\n",
    "                    normalized_sq_amplitude = normalize_indices(sq_amplitude_expr)\n",
    "\n",
    "                    # 2. Tokenize\n",
    "                    amplitude_tokens = tokenize(normalized_amplitude)\n",
    "                    sq_amplitude_tokens = tokenize(normalized_sq_amplitude)\n",
    "\n",
    "                    # Append structured data\n",
    "                    if amplitude_tokens and sq_amplitude_tokens: # Only add if both are non-empty after processing\n",
    "                        dataset.append({\n",
    "                            'input_tokens': amplitude_tokens,   # Typically the 'source' sequence\n",
    "                            'target_tokens': sq_amplitude_tokens # Typically the 'target' sequence\n",
    "                        })\n",
    "                    else:\n",
    "                        # print(f\"[Warning] Skipping line {line_num} in {file_path} due to empty tokens after processing.\")\n",
    "                        skipped_lines += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to process file {file_path}: {e}\")\n",
    "            # Decide if one faulty file should stop the whole process or just be skipped\n",
    "            continue # Skip this file and continue with the next\n",
    "\n",
    "    print(f\"[INFO] Processed {processed_files} files.\")\n",
    "    if skipped_lines > 0:\n",
    "        print(f\"[Warning] Skipped {skipped_lines} lines due to formatting issues or empty results.\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def split_and_save_dataset(data, train_frac, val_frac, output_prefix):\n",
    "    \"\"\"\n",
    "    Shuffles the dataset, splits it into train, validation, and test sets,\n",
    "    and saves each split to a JSON Lines file.\n",
    "\n",
    "    Args:\n",
    "        data (list[dict]): The list of preprocessed data points.\n",
    "        train_frac (float): Fraction of data for the training set (e.g., 0.8).\n",
    "        val_frac (float): Fraction of data for the validation set (e.g., 0.1).\n",
    "        output_prefix (str): The prefix for the output filenames.\n",
    "                             Files will be named f\"{output_prefix}_train.jsonl\", etc.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"[Warning] No data provided to split and save.\")\n",
    "        return\n",
    "\n",
    "    # Ensure fractions are valid\n",
    "    if not (0 < train_frac < 1 and 0 < val_frac < 1 and (train_frac + val_frac) < 1):\n",
    "        print(f\"[Error] Invalid split fractions: train={train_frac}, val={val_frac}. Must be between 0 and 1, and sum < 1.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Shuffle the data in place for random splitting\n",
    "    random.shuffle(data)\n",
    "\n",
    "    n_total = len(data)\n",
    "    n_train = int(train_frac * n_total)\n",
    "    n_val = int(val_frac * n_total)\n",
    "    n_test = n_total - n_train - n_val # Remainder goes to test set\n",
    "\n",
    "    # Perform the splits\n",
    "    splits = {\n",
    "        'train': data[:n_train],\n",
    "        'val': data[n_train : n_train + n_val],\n",
    "        'test': data[n_train + n_val :]\n",
    "    }\n",
    "\n",
    "    print(f\"[INFO] Splitting data: {n_train} train, {n_val} validation, {n_test} test examples.\")\n",
    "\n",
    "    # Save each split\n",
    "    output_dir = Path('.') # Save in the current directory, or specify another Path object\n",
    "    output_dir.mkdir(parents=True, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "    for split_name, subset in splits.items():\n",
    "        output_filename = output_dir / f\"{output_prefix}_{split_name}.jsonl\"\n",
    "        try:\n",
    "            with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "                for entry in subset:\n",
    "                    # Write each dictionary as a JSON string on its own line\n",
    "                    outfile.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "            print(f\"  Successfully saved {split_name} set to: {output_filename}\")\n",
    "        except IOError as e:\n",
    "            print(f\"[Error] Failed to write {split_name} set to {output_filename}: {e}\")\n",
    "        except Exception as e:\n",
    "             print(f\"[Error] An unexpected error occurred while writing {split_name} set: {e}\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the preprocessing and splitting.\"\"\"\n",
    "    print(\"[INFO] Starting QED expression preprocessing script...\")\n",
    "\n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(RANDOM_SEED)\n",
    "    print(f\"[INFO] Random seed set to: {RANDOM_SEED}\")\n",
    "\n",
    "    # 1. Load and preprocess data\n",
    "    print(f\"[INFO] Loading data from pattern: {FILES_PATTERN}\")\n",
    "    preprocessed_data = load_and_preprocess(FILES_PATTERN)\n",
    "\n",
    "    if not preprocessed_data:\n",
    "        print(\"[Error] No data loaded. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"[INFO] Successfully loaded and preprocessed {len(preprocessed_data)} examples.\")\n",
    "\n",
    "    # 2. Split and save the dataset\n",
    "    print(f\"[INFO] Splitting and saving dataset with prefix '{OUTPUT_PREFIX}'...\")\n",
    "    split_and_save_dataset(\n",
    "        data=preprocessed_data,\n",
    "        train_frac=TRAIN_FRAC,\n",
    "        val_frac=VAL_FRAC,\n",
    "        output_prefix=OUTPUT_PREFIX\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Script finished successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Task 2: Train/Evaluate Transformer model\n",
    "Train a generic next-token-prediction Transformer model to map the input data to the tokenized output sequences. Evaluate performance on the test set using sequence accuracy as a metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting Sequence-to-Sequence Model Training and Evaluation...\n",
      "[INFO] Loading data from: qed_expressions_train.jsonl\n",
      "[INFO] Successfully loaded 12441 records.\n",
      "[INFO] Loading data from: qed_expressions_val.jsonl\n",
      "[INFO] Successfully loaded 1555 records.\n",
      "[INFO] Loading data from: qed_expressions_test.jsonl\n",
      "[INFO] Successfully loaded 1556 records.\n",
      "[INFO] Converted 12441 items to strings.\n",
      "[INFO] Converted 1555 items to strings.\n",
      "[INFO] Converted 1556 items to strings.\n",
      "[INFO] Initializing tokenizer: bert-base-uncased\n",
      "[INFO] Encoding sequence pairs with max_length=128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikitas/anaconda3/envs/torch_cpu_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Encoding complete.\n",
      "[INFO] Encoding sequence pairs with max_length=128...\n",
      "[INFO] Encoding complete.\n",
      "[INFO] Encoding sequence pairs with max_length=128...\n",
      "[INFO] Encoding complete.\n",
      "[INFO] Created Dataset with 12441 examples.\n",
      "[INFO] Created Dataset with 1555 examples.\n",
      "[INFO] Created Dataset with 1556 examples.\n",
      "[INFO] Initializing Encoder-Decoder model (bert-base-uncased -> bert-base-uncased)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model configuration complete.\n",
      "[INFO] Data collator initialized.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 500\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Script finished successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 500\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 425\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Data collator initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# 8. Define Training Arguments\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Directory for checkpoints and logs\u001b[39;49;00m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_TRAIN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPER_DEVICE_TRAIN_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPER_DEVICE_EVAL_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWEIGHT_DECAY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWARMUP_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Directory for TensorBoard logs\u001b[39;49;00m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOGGING_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEVALUATION_STRATEGY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVE_STRATEGY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# Keep only the last 2 checkpoints\u001b[39;49;00m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Load the best checkpoint found during training at the end\u001b[39;49;00m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msequence_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# Use sequence accuracy to determine the best model\u001b[39;49;00m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# Higher accuracy is better\u001b[39;49;00m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_with_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPREDICT_WITH_GENERATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use logits for faster evaluation if True metrics aren't needed\u001b[39;49;00m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# report_to=\"tensorboard\",               # Example: enable reporting to TensorBoard\u001b[39;49;00m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# fp16=torch.cuda.is_available(),        # Enable mixed-precision training if CUDA is available\u001b[39;49;00m\n\u001b[1;32m    444\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Training arguments defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# 9. Initialize Trainer\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "End-to-end Training and Evaluation Script for a Sequence-to-Sequence Model\n",
    "on the Preprocessed QED 2-to-2 Tree-Level Dataset.\n",
    "\n",
    "This script performs the following steps:\n",
    "1.  Loads preprocessed training, validation, and test datasets from JSONL files.\n",
    "2.  Prepares the data by joining token lists into whitespace-separated strings.\n",
    "3.  Initializes a Hugging Face tokenizer (e.g., BERT's).\n",
    "4.  Tokenizes the input (amplitude) and target (squared amplitude) sequences,\n",
    "    padding/truncating them to a maximum length.\n",
    "5.  Creates custom PyTorch Dataset objects for each split.\n",
    "6.  Instantiates a Hugging Face Encoder-Decoder model (e.g., BERT-to-BERT).\n",
    "7.  Configures essential model parameters (special tokens, sequence length, etc.).\n",
    "8.  Sets up a Data Collator for handling dynamic padding within batches.\n",
    "9.  Defines a custom metric function to calculate sequence-level accuracy.\n",
    "10. Configures training arguments using `Seq2SeqTrainingArguments`.\n",
    "11. Initializes a `Seq2SeqTrainer`.\n",
    "12. Runs the training process on the training set, evaluating on the validation set.\n",
    "13. Evaluates the final trained model on the test set.\n",
    "14. Prints the test set sequence accuracy.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    EncoderDecoderModel,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainingArguments # Explicit import can be clearer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# File Paths\n",
    "# Assumes the script runs in a directory containing these files.\n",
    "# Consider using absolute paths or command-line arguments for more flexibility.\n",
    "TRAIN_FILE = Path('qed_expressions_train.jsonl') # Updated from 'data_train.jsonl' based on previous script\n",
    "VAL_FILE   = Path('qed_expressions_val.jsonl')   # Updated from 'data_val.jsonl'\n",
    "TEST_FILE  = Path('qed_expressions_test.jsonl')  # Updated from 'data_test.jsonl'\n",
    "OUTPUT_DIR = Path('qed_model_output') # Directory for checkpoints and logs\n",
    "\n",
    "# Model Configuration\n",
    "# Using bert-base-uncased for both encoder and decoder as an example.\n",
    "# Other combinations (e.g., DistilBERT encoder, BERT decoder) are possible.\n",
    "ENCODER_MODEL_ID = \"bert-base-uncased\"\n",
    "DECODER_MODEL_ID = \"bert-base-uncased\" # Must match encoder if weights are tied easily\n",
    "\n",
    "# Tokenizer and Data Processing Configuration\n",
    "MAX_SEQ_LENGTH = 128 # Maximum sequence length for padding/truncation\n",
    "\n",
    "# Training Hyperparameters\n",
    "NUM_TRAIN_EPOCHS = 5\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 16\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_STEPS = 500\n",
    "LOGGING_STEPS = 100\n",
    "EVALUATION_STRATEGY = \"epoch\" # Evaluate at the end of each epoch\n",
    "SAVE_STRATEGY = \"epoch\"       # Save a checkpoint at the end of each epoch\n",
    "# Set to True if generation metrics (BLEU, ROUGE) are needed during evaluation.\n",
    "# Set to False to use raw logits for metrics like accuracy, which is faster.\n",
    "PREDICT_WITH_GENERATE = False\n",
    "\n",
    "# --- Helper Functions and Classes ---\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Loads a JSON Lines (.jsonl) file.\n",
    "\n",
    "    Each line in the file is expected to be a valid JSON object.\n",
    "    Blank lines or lines containing only whitespace are skipped.\n",
    "\n",
    "    Args:\n",
    "        file_path (Path or str): The path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries loaded from the file.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file_path does not exist.\n",
    "        json.JSONDecodeError: If a line contains invalid JSON.\n",
    "        Exception: For other potential I/O errors.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    file_path = Path(file_path) # Ensure it's a Path object\n",
    "    print(f\"[INFO] Loading data from: {file_path}\")\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"[Warning] Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "                        continue\n",
    "        print(f\"[INFO] Successfully loaded {len(data)} records.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[Error] Data file not found: {file_path}\")\n",
    "        raise # Re-raise the exception to be handled by the caller\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load data from {file_path}: {e}\")\n",
    "        raise # Re-raise the exception\n",
    "\n",
    "\n",
    "def convert_tokens_to_strings(raw_data_list):\n",
    "    \"\"\"\n",
    "    Converts lists of tokens into single whitespace-joined strings.\n",
    "\n",
    "    Assumes each item in the input list is a dictionary with keys\n",
    "    'input_tokens' and 'target_tokens', where the values are lists of strings.\n",
    "\n",
    "    Args:\n",
    "        raw_data_list (list[dict]): The list of raw data dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[str], list[str]]: A tuple containing two lists:\n",
    "                                     - input strings\n",
    "                                     - target strings\n",
    "    \"\"\"\n",
    "    input_strings = []\n",
    "    target_strings = []\n",
    "    if not raw_data_list:\n",
    "        return input_strings, target_strings\n",
    "\n",
    "    for item in raw_data_list:\n",
    "        # Check if keys exist and values are lists, handle potential errors gracefully\n",
    "        input_toks = item.get('input_tokens')\n",
    "        target_toks = item.get('target_tokens')\n",
    "\n",
    "        if isinstance(input_toks, list) and isinstance(target_toks, list):\n",
    "            input_strings.append(\" \".join(map(str, input_toks)))  # Ensure tokens are strings\n",
    "            target_strings.append(\" \".join(map(str, target_toks)))\n",
    "        else:\n",
    "            print(f\"[Warning] Skipping item due to missing/invalid keys or non-list values: {item}\")\n",
    "\n",
    "    print(f\"[INFO] Converted {len(input_strings)} items to strings.\")\n",
    "    return input_strings, target_strings\n",
    "\n",
    "\n",
    "def encode_sequence_pairs(tokenizer, input_strings, target_strings, max_len):\n",
    "    \"\"\"\n",
    "    Tokenizes input and target string pairs using the provided tokenizer.\n",
    "\n",
    "    Pads and truncates sequences to the specified maximum length.\n",
    "    Sets up the target sequences correctly for decoder training.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: An initialized Hugging Face tokenizer instance.\n",
    "        input_strings (list[str]): List of input sequences.\n",
    "        target_strings (list[str]): List of target sequences.\n",
    "        max_len (int): The maximum sequence length for padding/truncation.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing tokenized 'input_ids', 'attention_mask',\n",
    "              and 'labels' (tokenized target IDs).\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Encoding sequence pairs with max_length={max_len}...\")\n",
    "\n",
    "    # Tokenize the input sequences (for the encoder)\n",
    "    encoder_inputs = tokenizer(\n",
    "        input_strings,\n",
    "        padding='max_length',   # Pad to max_len\n",
    "        truncation=True,        # Truncate sequences longer than max_len\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"     # Return PyTorch tensors (though we convert later in Dataset)\n",
    "    )\n",
    "    encoder_inputs.pop(\"token_type_ids\", None) # Not needed for BERT encoder in this context\n",
    "\n",
    "    # Tokenize the target sequences (for the decoder)\n",
    "    # Use tokenizer in \"target mode\" to handle special tokens appropriately for decoder inputs\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        decoder_labels = tokenizer(\n",
    "            target_strings,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    # The 'labels' for the model are the input_ids of the *target* sequence.\n",
    "    # The model will typically shift these internally to create decoder_input_ids.\n",
    "    encoder_inputs['labels'] = decoder_labels['input_ids']\n",
    "\n",
    "    # Detach tensors and convert back to lists for easier handling in Dataset __getitem__\n",
    "    # Note: If memory allows, keeping tensors might be slightly faster, but lists are more flexible.\n",
    "    encoded_data = {k: v.tolist() for k, v in encoder_inputs.items()}\n",
    "\n",
    "    print(f\"[INFO] Encoding complete.\")\n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset to wrap tokenized sequence data.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encodings (dict): A dictionary where keys are 'input_ids',\n",
    "                              'attention_mask', 'labels', etc., and values\n",
    "                              are lists of tokenized sequences.\n",
    "        \"\"\"\n",
    "        if not isinstance(encodings, dict) or not encodings:\n",
    "             raise ValueError(\"Encodings must be a non-empty dictionary.\")\n",
    "        # Find a key to determine the length (e.g., 'input_ids')\n",
    "        self.length = 0\n",
    "        if 'input_ids' in encodings and isinstance(encodings['input_ids'], list):\n",
    "             self.length = len(encodings['input_ids'])\n",
    "        elif encodings:\n",
    "             # Fallback: Use the length of the first list found\n",
    "             first_key = next(iter(encodings))\n",
    "             if isinstance(encodings[first_key], list):\n",
    "                 self.length = len(encodings[first_key])\n",
    "\n",
    "        if self.length == 0:\n",
    "             raise ValueError(\"Could not determine dataset length from encodings.\")\n",
    "\n",
    "        self.encodings = encodings\n",
    "        print(f\"[INFO] Created Dataset with {self.length} examples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the tokenized data for a single sample and converts it to tensors.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are feature names (e.g., 'input_ids')\n",
    "                  and values are the corresponding PyTorch tensors for the sample.\n",
    "        \"\"\"\n",
    "        # Fetch the data for the given index and convert each list item to a tensor\n",
    "        try:\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            return item\n",
    "        except IndexError:\n",
    "            print(f\"[Error] Index {idx} out of bounds for dataset of length {self.length}.\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to retrieve or convert item at index {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def compute_sequence_accuracy(predictions, labels, pad_token_id):\n",
    "    \"\"\"\n",
    "    Calculates exact sequence match accuracy.\n",
    "\n",
    "    Compares predicted sequences to label sequences, ignoring padding tokens.\n",
    "    A prediction is correct only if the entire non-padded sequence matches the label.\n",
    "\n",
    "    Args:\n",
    "        predictions (np.ndarray): Logits output by the model (shape: batch_size, seq_len, vocab_size).\n",
    "        labels (np.ndarray): Ground truth label IDs (shape: batch_size, seq_len).\n",
    "        pad_token_id (int): The ID of the padding token to ignore.\n",
    "\n",
    "    Returns:\n",
    "        float: The fraction of sequences that match exactly.\n",
    "    \"\"\"\n",
    "    # Get the predicted token IDs by taking the argmax along the vocabulary dimension\n",
    "    pred_ids = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Create a mask to ignore padding tokens in the labels\n",
    "    non_padding_mask = (labels != pad_token_id)\n",
    "\n",
    "    # Check equality element-wise for non-padded positions\n",
    "    correct_tokens = (pred_ids == labels) & non_padding_mask\n",
    "\n",
    "    # Check if all non-padded tokens in a sequence are correct\n",
    "    # We sum the mask and the correct tokens for each sequence.\n",
    "    # If the sums are equal, it means every non-padded token was predicted correctly.\n",
    "    correct_sequences = (np.sum(correct_tokens, axis=1) == np.sum(non_padding_mask, axis=1))\n",
    "\n",
    "    # Calculate the mean accuracy over the batch\n",
    "    accuracy = np.mean(correct_sequences)\n",
    "    return float(accuracy)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Computes metrics for evaluation. Called by the Trainer.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (EvalPrediction): A tuple containing predictions (logits) and labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping metric names to their values.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # predictions are logits, labels are the actual token IDs\n",
    "\n",
    "    # Ensure pad_token_id is accessible, assuming tokenizer is globally defined or passed\n",
    "    # If not global, it needs to be passed or accessed differently.\n",
    "    # For simplicity here, we assume 'tokenizer' is accessible.\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    seq_acc = compute_sequence_accuracy(predictions, labels, pad_token_id)\n",
    "\n",
    "    return {\"sequence_accuracy\": seq_acc}\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Orchestrates the entire training and evaluation pipeline.\"\"\"\n",
    "    print(\"[INFO] Starting Sequence-to-Sequence Model Training and Evaluation...\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    try:\n",
    "        train_raw_data = load_jsonl(TRAIN_FILE)\n",
    "        val_raw_data = load_jsonl(VAL_FILE)\n",
    "        test_raw_data = load_jsonl(TEST_FILE)\n",
    "    except Exception:\n",
    "         print(\"[Error] Failed to load one or more data files. Exiting.\")\n",
    "         sys.exit(1)\n",
    "\n",
    "    if not train_raw_data or not val_raw_data or not test_raw_data:\n",
    "        print(\"[Error] One or more datasets are empty after loading. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 2. Prepare Data (Tokens to Strings)\n",
    "    train_inputs, train_targets = convert_tokens_to_strings(train_raw_data)\n",
    "    val_inputs,   val_targets   = convert_tokens_to_strings(val_raw_data)\n",
    "    test_inputs,  test_targets  = convert_tokens_to_strings(test_raw_data)\n",
    "\n",
    "    if not train_inputs or not val_inputs or not test_inputs:\n",
    "         print(\"[Error] Data conversion resulted in empty lists. Check input data format. Exiting.\")\n",
    "         sys.exit(1)\n",
    "\n",
    "\n",
    "    # 3. Initialize Tokenizer\n",
    "    print(f\"[INFO] Initializing tokenizer: {ENCODER_MODEL_ID}\")\n",
    "    # Making tokenizer global for access in compute_metrics (simplification)\n",
    "    # A better approach in larger projects might involve passing it explicitly or using a class.\n",
    "    global tokenizer\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(ENCODER_MODEL_ID)\n",
    "        # Add special tokens if they are not already present? BERT usually has them.\n",
    "        # tokenizer.add_special_tokens({'bos_token':'<s>', 'eos_token':'</s>'}) # Example if needed\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to initialize tokenizer {ENCODER_MODEL_ID}: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 4. Tokenize Data\n",
    "    try:\n",
    "        train_encodings = encode_sequence_pairs(tokenizer, train_inputs, train_targets, MAX_SEQ_LENGTH)\n",
    "        val_encodings   = encode_sequence_pairs(tokenizer, val_inputs,   val_targets,   MAX_SEQ_LENGTH)\n",
    "        test_encodings  = encode_sequence_pairs(tokenizer, test_inputs,  test_targets,  MAX_SEQ_LENGTH)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed during data tokenization: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    # 5. Create Datasets\n",
    "    try:\n",
    "        train_dataset = SequenceDataset(train_encodings)\n",
    "        val_dataset   = SequenceDataset(val_encodings)\n",
    "        test_dataset  = SequenceDataset(test_encodings)\n",
    "    except ValueError as e:\n",
    "         print(f\"[Error] Failed to create Dataset objects: {e}\")\n",
    "         sys.exit(1)\n",
    "\n",
    "    # 6. Initialize Model\n",
    "    print(f\"[INFO] Initializing Encoder-Decoder model ({ENCODER_MODEL_ID} -> {DECODER_MODEL_ID})\")\n",
    "    try:\n",
    "        model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "            ENCODER_MODEL_ID,\n",
    "            DECODER_MODEL_ID,\n",
    "        )\n",
    "\n",
    "        # --- Configure Model ---\n",
    "        # Tie weights if encoder and decoder are compatible (e.g., same architecture/vocab)\n",
    "        # This reduces parameters and can improve performance. Check documentation if unsure.\n",
    "        # model.tie_weights() # Uncomment if appropriate for the chosen models\n",
    "\n",
    "        # Set special token IDs for generation (though not used if predict_with_generate=False)\n",
    "        # These ensure the decoder starts and ends sequences correctly if generation is enabled.\n",
    "        model.config.decoder_start_token_id = tokenizer.cls_token_id # Often [CLS] for BERT-like models\n",
    "        model.config.eos_token_id           = tokenizer.sep_token_id # Often [SEP]\n",
    "        model.config.pad_token_id           = tokenizer.pad_token_id # Usually 0\n",
    "\n",
    "        # Ensure critical parameters are aligned between the main config and encoder/decoder configs\n",
    "        model.config.vocab_size             = model.config.encoder.vocab_size # Use encoder's vocab size\n",
    "\n",
    "        # Set parameters relevant for training and generation control\n",
    "        model.config.max_length             = MAX_SEQ_LENGTH # Max length for generation if enabled\n",
    "        model.config.early_stopping         = True           # Example: enable early stopping for generation\n",
    "        model.config.no_repeat_ngram_size   = 3              # Example: prevent trigram repetition during generation\n",
    "        # model.config.length_penalty         = 2.0            # Example: encourage longer sequences\n",
    "        # model.config.num_beams              = 4              # Example: use beam search for generation\n",
    "\n",
    "        # Optional: Set dropout rates explicitly if defaults are not desired\n",
    "        # model.config.dropout                = 0.1\n",
    "        # model.config.attention_dropout      = 0.1\n",
    "\n",
    "        print(\"[INFO] Model configuration complete.\")\n",
    "        # print(model.config) # Uncomment to inspect the full configuration\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to initialize or configure the model: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 7. Initialize Data Collator\n",
    "    # Dynamically pads sequences within each batch to the maximum length in that batch.\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=tokenizer.pad_token_id # Use pad token ID for labels\n",
    "    )\n",
    "    print(\"[INFO] Data collator initialized.\")\n",
    "\n",
    "    # 8. Define Training Arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR),              # Directory for checkpoints and logs\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        logging_dir=str(OUTPUT_DIR / 'logs'),    # Directory for TensorBoard logs\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        evaluation_strategy=EVALUATION_STRATEGY,\n",
    "        save_strategy=SAVE_STRATEGY,\n",
    "        save_total_limit=2,                      # Keep only the last 2 checkpoints\n",
    "        load_best_model_at_end=True,             # Load the best checkpoint found during training at the end\n",
    "        metric_for_best_model=\"sequence_accuracy\",# Use sequence accuracy to determine the best model\n",
    "        greater_is_better=True,                  # Higher accuracy is better\n",
    "        predict_with_generate=PREDICT_WITH_GENERATE, # Use logits for faster evaluation if True metrics aren't needed\n",
    "        # report_to=\"tensorboard\",               # Example: enable reporting to TensorBoard\n",
    "        # fp16=torch.cuda.is_available(),        # Enable mixed-precision training if CUDA is available\n",
    "    )\n",
    "    print(\"[INFO] Training arguments defined.\")\n",
    "\n",
    "    # 9. Initialize Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics, # Pass the custom metric function\n",
    "    )\n",
    "    print(\"[INFO] Seq2SeqTrainer initialized.\")\n",
    "\n",
    "    # 10. Train the Model\n",
    "    print(\"[INFO] Starting model training...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        # Log training metrics\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state() # Save optimizer, scheduler state etc.\n",
    "        # Save the final best model explicitly (though save_strategy='epoch' does it too)\n",
    "        trainer.save_model(str(OUTPUT_DIR / \"best_model\"))\n",
    "        print(\"[INFO] Training finished successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Training failed: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    # 11. Evaluate on Test Set\n",
    "    print(\"[INFO] Evaluating model on the test set...\")\n",
    "    try:\n",
    "        test_results = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")\n",
    "        trainer.log_metrics(\"test\", test_results)\n",
    "        trainer.save_metrics(\"test\", test_results)\n",
    "\n",
    "        # Print the key metric\n",
    "        if 'test_sequence_accuracy' in test_results:\n",
    "             print(f\"\\n--- Test Set Results ---\")\n",
    "             print(f\"Test Sequence Accuracy: {test_results['test_sequence_accuracy']:.4f}\")\n",
    "             print(f\"------------------------\")\n",
    "        else:\n",
    "             print(\"[Warning] 'test_sequence_accuracy' not found in evaluation results.\")\n",
    "             print(\"Test results:\", test_results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Evaluation on test set failed: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"[INFO] Script finished successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Test 3: Train/Evaluate advanced model\n",
    "Repeat task two including checking sequence accuracy but with a model that leverages some slightly more advanced techniques. The model you use should relate to the project you’re applying for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting T5 Sequence-to-Sequence Model Training and Evaluation...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 436\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Script finished successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;66;03m# Set a random seed for reproducibility if desired, affects shuffling, dropout, etc.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m# SEED = 42\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;66;03m# if torch.cuda.is_available():\u001b[39;00m\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;66;03m#     torch.cuda.manual_seed_all(SEED)\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 233\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Orchestrates the T5 training and evaluation pipeline.\"\"\"\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Starting T5 Sequence-to-Sequence Model Training and Evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Current date/time (UTC): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mutcnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS UTC\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Use PyTorch's datetime\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# --- 1. Load Data ---\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_cpu_env/lib/python3.10/site-packages/torch/__init__.py:2562\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 2562\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'datetime'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "End-to-end Training and Evaluation Script for a T5 Sequence-to-Sequence Model\n",
    "on the Preprocessed QED 2-to-2 Tree-Level Dataset.\n",
    "\n",
    "This script utilizes several advanced Hugging Face Transformers features:\n",
    "- T5 Model: Leverages the t5-base pre-trained model for conditional generation.\n",
    "- Gradient Checkpointing: Reduces GPU memory usage during training, allowing\n",
    "  for larger models or batch sizes, at the cost of a small computational overhead.\n",
    "- Mixed Precision Training (FP16): Speeds up training and further reduces memory\n",
    "  usage on compatible GPUs (NVIDIA Volta architecture or newer) by performing\n",
    "  certain operations in half-precision floating-point format.\n",
    "- Label Smoothing: A regularization technique that prevents the model from\n",
    "  becoming overconfident in its predictions, potentially improving generalization.\n",
    "- Beam Search Generation: Used during evaluation (`predict_with_generate=True`)\n",
    "  to generate more fluent and potentially more accurate output sequences compared\n",
    "  to greedy decoding.\n",
    "- Exact Match Accuracy: Evaluates the model based on whether the generated\n",
    "  sequence exactly matches the target sequence after decoding and stripping whitespace.\n",
    "\n",
    "Workflow:\n",
    "1. Load preprocessed data splits (train, validation, test) from JSONL files.\n",
    "2. Convert token lists in the data back into whitespace-separated strings.\n",
    "3. Initialize the T5 tokenizer and T5 model (`t5-base`).\n",
    "4. Enable gradient checkpointing on the model.\n",
    "5. Define an encoding function to tokenize input/target strings using the T5 tokenizer.\n",
    "6. Create PyTorch Dataset objects for each data split.\n",
    "7. Set up a Data Collator for dynamic padding within batches.\n",
    "8. Define a custom metric function (`compute_metrics`) that uses generated\n",
    "   predictions (due to `predict_with_generate=True`) and calculates exact\n",
    "   sequence match accuracy after decoding.\n",
    "9. Configure `Seq2SeqTrainingArguments`, enabling advanced features like\n",
    "   FP16, gradient checkpointing, label smoothing, and generation parameters.\n",
    "10. Initialize the `Seq2SeqTrainer`.\n",
    "11. Train the model.\n",
    "12. Evaluate the final model on the test set using the defined metric.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime # <--- ***** FIX: Import the standard datetime module *****\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainingArguments # Explicit import for clarity\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# File Paths\n",
    "# Assumes the script runs in a directory containing these files.\n",
    "# Use absolute paths or command-line arguments if needed.\n",
    "TRAIN_FILE = Path('qed_expressions_train.jsonl') # Path to the training data JSONL file\n",
    "VAL_FILE   = Path('qed_expressions_val.jsonl')   # Path to the validation data JSONL file\n",
    "TEST_FILE  = Path('qed_expressions_test.jsonl')  # Path to the test data JSONL file\n",
    "OUTPUT_DIR = Path('qed_t5_model_output') # Directory for checkpoints, logs, and final model\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_ID = \"t5-base\" # Pre-trained T5 model identifier from Hugging Face Hub\n",
    "\n",
    "# Tokenizer and Data Processing Configuration\n",
    "MAX_SEQ_LENGTH = 128 # Maximum sequence length for input and target tokenization\n",
    "\n",
    "# Training Hyperparameters\n",
    "NUM_TRAIN_EPOCHS = 5\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 8 # Adjust based on GPU memory\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 8  # Adjust based on GPU memory\n",
    "LEARNING_RATE = 3e-4            # Typical learning rate for T5 fine-tuning\n",
    "WEIGHT_DECAY = 0.01             # Weight decay for regularization\n",
    "WARMUP_STEPS = 200              # Number of linear warmup steps for the learning rate scheduler\n",
    "LOGGING_STEPS = 50              # Log metrics every N steps\n",
    "EVALUATION_STRATEGY = \"epoch\"   # Evaluate performance at the end of each epoch\n",
    "SAVE_STRATEGY = \"epoch\"         # Save a model checkpoint at the end of each epoch\n",
    "\n",
    "# Advanced Training Feature Flags/Values\n",
    "# Enable mixed precision if a CUDA GPU is available and requirements are met\n",
    "USE_FP16 = torch.cuda.is_available()\n",
    "USE_GRADIENT_CHECKPOINTING = True # Enable gradient checkpointing to save memory\n",
    "LABEL_SMOOTHING_FACTOR = 0.1      # Apply label smoothing regularization (0.0 means no smoothing)\n",
    "\n",
    "# Generation Configuration (used during evaluation with predict_with_generate=True)\n",
    "GENERATION_NUM_BEAMS = 4          # Number of beams for beam search decoding\n",
    "\n",
    "# --- Helper Functions and Classes ---\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads a JSON Lines (.jsonl) file, skipping blank/whitespace-only lines.\"\"\"\n",
    "    data = []\n",
    "    file_path = Path(file_path)\n",
    "    print(f\"[INFO] Loading data from: {file_path}\")\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"[Warning] Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "                        continue\n",
    "        print(f\"[INFO] Successfully loaded {len(data)} records.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[Error] Data file not found: {file_path}\", file=sys.stderr)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load data from {file_path}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "\n",
    "def convert_tokens_to_strings(raw_data_list):\n",
    "    \"\"\"Converts lists of tokens into single whitespace-joined strings.\"\"\"\n",
    "    input_strings = []\n",
    "    target_strings = []\n",
    "    if not raw_data_list:\n",
    "        return input_strings, target_strings\n",
    "\n",
    "    skipped_count = 0\n",
    "    for i, item in enumerate(raw_data_list):\n",
    "        input_toks = item.get('input_tokens')\n",
    "        target_toks = item.get('target_tokens')\n",
    "\n",
    "        if isinstance(input_toks, list) and isinstance(target_toks, list):\n",
    "            # Ensure all tokens are strings before joining\n",
    "            input_strings.append(\" \".join(map(str, input_toks)))\n",
    "            target_strings.append(\" \".join(map(str, target_toks)))\n",
    "        else:\n",
    "            print(f\"[Warning] Skipping item at index {i} due to missing/invalid keys or non-list values: {item}\")\n",
    "            skipped_count += 1\n",
    "\n",
    "    print(f\"[INFO] Converted {len(input_strings)} items to strings (skipped {skipped_count}).\")\n",
    "    return input_strings, target_strings\n",
    "\n",
    "\n",
    "def encode_sequences(tokenizer, input_strings, target_strings, max_len):\n",
    "    \"\"\"\n",
    "    Tokenizes input and target string pairs using the T5 tokenizer.\n",
    "\n",
    "    Pads and truncates sequences to the specified maximum length.\n",
    "    Prepares labels correctly for T5 training.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: An initialized T5Tokenizer instance.\n",
    "        input_strings (list[str]): List of input sequences (add prefix if needed for T5 task).\n",
    "        target_strings (list[str]): List of target sequences.\n",
    "        max_len (int): The maximum sequence length for padding/truncation.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing tokenized 'input_ids', 'attention_mask',\n",
    "              and 'labels' (tokenized target IDs). T5 does not use token_type_ids.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Encoding sequence pairs with max_length={max_len}...\")\n",
    "\n",
    "    # Note: T5 often benefits from a task-specific prefix, e.g., \"translate English to German: \"\n",
    "    # Add such a prefix to input_strings here if applicable to your task.\n",
    "    # Example: input_strings = [f\"summarize: {s}\" for s in input_strings]\n",
    "\n",
    "    # Tokenize the input sequences (for the encoder)\n",
    "    encoder_inputs = tokenizer(\n",
    "        input_strings,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',   # Pad to max_len\n",
    "        truncation=True,        # Truncate sequences longer than max_len\n",
    "        return_tensors=None     # Return lists, collator will handle tensor conversion\n",
    "    )\n",
    "\n",
    "    # Tokenize the target sequences (for the decoder labels)\n",
    "    # Use tokenizer in target mode context is good practice, although less critical for T5 label encoding.\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        decoder_labels = tokenizer(\n",
    "            target_strings,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "    # The 'labels' for T5 are the input_ids of the target sequence.\n",
    "    # Padding tokens in labels are typically replaced with -100 by the DataCollator\n",
    "    # so they are ignored in the loss calculation.\n",
    "    encoder_inputs['labels'] = decoder_labels['input_ids']\n",
    "\n",
    "    print(f\"[INFO] Encoding complete.\")\n",
    "    return encoder_inputs\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset wrapper for tokenized sequence data.\"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encodings (dict): Dictionary from tokenizer {'input_ids': [...], 'attention_mask': [...], 'labels': [...]}.\n",
    "        \"\"\"\n",
    "        if not isinstance(encodings, dict) or 'input_ids' not in encodings:\n",
    "            raise ValueError(\"Encodings must be a dictionary containing at least 'input_ids'.\")\n",
    "        self.encodings = encodings\n",
    "        self.length = len(encodings['input_ids'])\n",
    "        if self.length == 0:\n",
    "            raise ValueError(\"Input encodings are empty.\")\n",
    "        print(f\"[INFO] Created Dataset with {self.length} examples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the tokenized data for a single sample.\n",
    "        The DataCollator will handle tensor conversion and padding alignment.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Return a dictionary slice for the given index\n",
    "            return {key: val[idx] for key, val in self.encodings.items()}\n",
    "        except IndexError:\n",
    "            print(f\"[Error] Index {idx} out of bounds for dataset of length {self.length}.\", file=sys.stderr)\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to retrieve item at index {idx}: {e}\", file=sys.stderr)\n",
    "            raise\n",
    "\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Orchestrates the T5 training and evaluation pipeline.\"\"\"\n",
    "    print(\"[INFO] Starting T5 Sequence-to-Sequence Model Training and Evaluation...\")\n",
    "    # --- ***** FIX: Use standard datetime module ***** ---\n",
    "    try:\n",
    "        # Use timezone-aware UTC time (recommended)\n",
    "        current_time_str = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
    "    except Exception:\n",
    "        # Fallback if timezone object isn't available (older Python?) or other issue\n",
    "        current_time_str = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC (naive)')\n",
    "    print(f\"[INFO] Current date/time (UTC): {current_time_str}\")\n",
    "\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    try:\n",
    "        train_raw_data = load_jsonl(TRAIN_FILE)\n",
    "        val_raw_data = load_jsonl(VAL_FILE)\n",
    "        test_raw_data = load_jsonl(TEST_FILE)\n",
    "    except Exception:\n",
    "        print(f\"[Error] Failed to load one or more data files ({TRAIN_FILE}, {VAL_FILE}, {TEST_FILE}). Exiting.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not train_raw_data or not val_raw_data or not test_raw_data:\n",
    "        print(\"[Error] One or more datasets are empty after loading. Exiting.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 2. Prepare Data (Tokens to Strings) ---\n",
    "    train_inputs, train_targets = convert_tokens_to_strings(train_raw_data)\n",
    "    val_inputs,   val_targets   = convert_tokens_to_strings(val_raw_data)\n",
    "    test_inputs,  test_targets  = convert_tokens_to_strings(test_raw_data)\n",
    "\n",
    "    if not train_inputs or not val_inputs or not test_inputs:\n",
    "        print(\"[Error] Data conversion resulted in empty lists. Check input data format. Exiting.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 3. Initialize Tokenizer and Model ---\n",
    "    print(f\"[INFO] Initializing Tokenizer and Model: {MODEL_ID}\")\n",
    "    try:\n",
    "        # Using legacy=False is recommended for T5Tokenizer for T5v1.1+ behavior, but True is default for t5-base/large\n",
    "        tokenizer = T5Tokenizer.from_pretrained(MODEL_ID) # legacy=False might be better if using newer T5 variants\n",
    "        model = T5ForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to initialize tokenizer or model '{MODEL_ID}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Enable Gradient Checkpointing *on the model* if specified\n",
    "    # This needs to be done before training starts.\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        try:\n",
    "            model.gradient_checkpointing_enable()\n",
    "            print(\"[INFO] Gradient Checkpointing enabled on the model.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Failed to enable gradient checkpointing on model: {e}. Training will proceed without it.\")\n",
    "            USE_GRADIENT_CHECKPOINTING = False # Ensure training arg matches reality\n",
    "\n",
    "    # --- 4. Tokenize Data ---\n",
    "    try:\n",
    "        # Making tokenizer global for access in compute_metrics (simplification)\n",
    "        # Consider passing via functools.partial if this becomes complex.\n",
    "        global tokenizer_for_metrics\n",
    "        tokenizer_for_metrics = tokenizer\n",
    "\n",
    "        train_encodings = encode_sequences(tokenizer, train_inputs, train_targets, MAX_SEQ_LENGTH)\n",
    "        val_encodings   = encode_sequences(tokenizer, val_inputs,   val_targets,   MAX_SEQ_LENGTH)\n",
    "        test_encodings  = encode_sequences(tokenizer, test_inputs,  test_targets,  MAX_SEQ_LENGTH)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed during data tokenization: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 5. Create Datasets ---\n",
    "    try:\n",
    "        train_dataset = SequenceDataset(train_encodings)\n",
    "        val_dataset   = SequenceDataset(val_encodings)\n",
    "        test_dataset  = SequenceDataset(test_encodings)\n",
    "    except ValueError as e:\n",
    "        print(f\"[Error] Failed to create Dataset objects: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 6. Initialize Data Collator ---\n",
    "    # Handles dynamic padding and prepares decoder input IDs and labels correctly for T5.\n",
    "    # It automatically creates `decoder_input_ids` by shifting `labels` and replaces padded label tokens with -100.\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100, # Standard practice for ignoring padding in loss\n",
    "        pad_to_multiple_of=8 if USE_FP16 else None # Optimize padding for FP16 tensor cores\n",
    "    )\n",
    "    print(\"[INFO] Data collator initialized.\")\n",
    "\n",
    "    # --- 7. Define Metrics Computation ---\n",
    "    # This function will be called by the Trainer during evaluation.\n",
    "    # It receives generated token IDs because predict_with_generate=True.\n",
    "    def compute_metrics_fn(eval_pred):\n",
    "        \"\"\"Calculates exact match accuracy between decoded predictions and labels.\"\"\"\n",
    "        # eval_pred is a tuple (predictions, labels)\n",
    "        # For seq2seq with predict_with_generate=True, predictions are generated token IDs (np.ndarray)\n",
    "        # Labels are also token IDs (np.ndarray), potentially padded with -100.\n",
    "        predictions, labels = eval_pred\n",
    "\n",
    "        # Ensure predictions and labels are numpy arrays\n",
    "        if not isinstance(predictions, np.ndarray): predictions = np.array(predictions)\n",
    "        if not isinstance(labels, np.ndarray): labels = np.array(labels)\n",
    "\n",
    "        # Replace -100 in labels with the pad_token_id for decoding purposes.\n",
    "        labels = np.where(labels != -100, labels, tokenizer_for_metrics.pad_token_id)\n",
    "\n",
    "        # Decode predicted IDs and label IDs to strings.\n",
    "        # skip_special_tokens=True removes tokens like <pad>, </s> etc.\n",
    "        try:\n",
    "            decoded_preds = tokenizer_for_metrics.batch_decode(predictions, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer_for_metrics.batch_decode(labels, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "             print(f\"[Error] Failed to decode predictions/labels in compute_metrics: {e}\", file=sys.stderr)\n",
    "             # Return a default low score or re-raise depending on desired behavior\n",
    "             return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "\n",
    "        # Post-process: Strip leading/trailing whitespace for robust comparison\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "        # Calculate exact matches\n",
    "        if len(decoded_preds) != len(decoded_labels):\n",
    "             print(f\"[Warning] Mismatch in number of predictions ({len(decoded_preds)}) and labels ({len(decoded_labels)}) after decoding.\", file=sys.stderr)\n",
    "             return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "        matches = [pred == label for pred, label in zip(decoded_preds, decoded_labels)]\n",
    "        accuracy = np.mean(matches) if matches else 0.0 # Handle empty case\n",
    "\n",
    "        return {\"sequence_accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "    # --- 8. Define Training Arguments ---\n",
    "    # Enables advanced features configured earlier.\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR),\n",
    "        # Batch sizes, epochs, learning rate...\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        # Logging, evaluation, saving strategies...\n",
    "        logging_dir=str(OUTPUT_DIR / 'logs'),\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        evaluation_strategy=EVALUATION_STRATEGY,\n",
    "        save_strategy=SAVE_STRATEGY,\n",
    "        save_total_limit=2,             # Keep only the last 2 checkpoints\n",
    "        load_best_model_at_end=True,    # Load best model checkpoint at the end of training\n",
    "        metric_for_best_model=\"sequence_accuracy\", # Metric to determine the \"best\" model\n",
    "        greater_is_better=True,         # Higher accuracy is better\n",
    "        # Generation settings (used because predict_with_generate=True)\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=MAX_SEQ_LENGTH, # Should match tokenizer max_length for consistency\n",
    "        generation_num_beams=GENERATION_NUM_BEAMS,\n",
    "        # Advanced features\n",
    "        fp16=USE_FP16, # Enable mixed precision training\n",
    "        label_smoothing_factor=LABEL_SMOOTHING_FACTOR, # Enable label smoothing\n",
    "        # Enable gradient checkpointing within the Trainer's control flow\n",
    "        gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "        # report_to=\"tensorboard\",      # Example: uncomment to enable TensorBoard logging\n",
    "    )\n",
    "    print(\"[INFO] Training arguments defined.\")\n",
    "    # Print status of key features\n",
    "    print(f\"[INFO] Mixed Precision (FP16): {'Enabled' if USE_FP16 else 'Disabled'}\")\n",
    "    print(f\"[INFO] Label Smoothing Factor: {LABEL_SMOOTHING_FACTOR}\")\n",
    "    print(f\"[INFO] Gradient Checkpointing: {'Enabled' if USE_GRADIENT_CHECKPOINTING else 'Disabled'}\")\n",
    "\n",
    "\n",
    "    # --- 9. Initialize Trainer ---\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,           # Pass tokenizer for saving/loading and decoding if needed\n",
    "        compute_metrics=compute_metrics_fn, # Pass the metric calculation function\n",
    "    )\n",
    "    print(\"[INFO] Seq2SeqTrainer initialized.\")\n",
    "\n",
    "    # --- 10. Train the Model ---\n",
    "    print(f\"[INFO] Starting model training for {NUM_TRAIN_EPOCHS} epochs...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        # Log and save training metrics and state\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        # Save the final best model checkpoint explicitly (already done by load_best_model_at_end + save_strategy)\n",
    "        # trainer.save_model(str(OUTPUT_DIR / \"best_model\")) # Can keep if explicit save desired\n",
    "        print(\"[INFO] Training finished successfully.\")\n",
    "        print(f\"[INFO] Best model saved to: {trainer.state.best_model_checkpoint}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Training failed: {e}\", file=sys.stderr)\n",
    "        # Consider cleanup or further diagnostics here if needed\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 11. Evaluate on Test Set ---\n",
    "    print(\"[INFO] Evaluating model on the test set...\")\n",
    "    try:\n",
    "        test_results = trainer.evaluate(\n",
    "            eval_dataset=test_dataset,\n",
    "            metric_key_prefix=\"test\" # Prefix metrics with 'test_' (e.g., 'test_sequence_accuracy')\n",
    "        )\n",
    "        trainer.log_metrics(\"test\", test_results)\n",
    "        trainer.save_metrics(\"test\", test_results)\n",
    "\n",
    "        # Print the key metric clearly\n",
    "        if 'test_sequence_accuracy' in test_results:\n",
    "             print(f\"\\n--- Test Set Results ---\")\n",
    "             print(f\"Test Sequence Accuracy: {test_results['test_sequence_accuracy']:.4f}\")\n",
    "             print(f\"------------------------\")\n",
    "        else:\n",
    "             print(\"[Warning] 'test_sequence_accuracy' not found in evaluation results.\")\n",
    "             print(\"Full test results:\", test_results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Evaluation on test set failed: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"[INFO] Script finished successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Optional: Set random seeds for reproducibility\n",
    "    # SEED = 42\n",
    "    # torch.manual_seed(SEED)\n",
    "    # np.random.seed(SEED)\n",
    "    # # import random # If using random module\n",
    "    # # random.seed(SEED)\n",
    "    # if torch.cuda.is_available():\n",
    "    #     torch.cuda.manual_seed_all(SEED)\n",
    "    #     # May need deterministic algorithms for full reproducibility, potentially slower\n",
    "    #     # torch.backends.cudnn.deterministic = True\n",
    "    #     # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1: Next-Generation Transformer Models for Symbolic Calculations of Squared Amplitudes in HEP\n",
    "Model: Transformer model with a contemporary innovation added such as KAN layers, reinforcement learning, genetic algorithms, specialized long-sequence attention, etc. which improves the performance compared to a basic transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BigBirdPegasusTokenizer' from 'transformers' (/home/nikitas/anaconda3/envs/torch_cpu_env/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     54\u001b[0m     BigBirdPegasusForConditionalGeneration,\n\u001b[1;32m     55\u001b[0m     BigBirdPegasusTokenizer,\n\u001b[1;32m     56\u001b[0m     DataCollatorForSeq2Seq,\n\u001b[1;32m     57\u001b[0m     Seq2SeqTrainer,\n\u001b[1;32m     58\u001b[0m     Seq2SeqTrainingArguments,\n\u001b[1;32m     59\u001b[0m     TrainingArguments \u001b[38;5;66;03m# Explicit import for clarity\u001b[39;00m\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# --- Configuration ---\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# File Paths (adjust if your preprocessed files have different names/locations)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m TRAIN_FILE \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqed_expressions_train.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Training data\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BigBirdPegasusTokenizer' from 'transformers' (/home/nikitas/anaconda3/envs/torch_cpu_env/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "End-to-end Fine-tuning and Evaluation Script for BigBird-Pegasus\n",
    "on the Preprocessed QED 2-to-2 Tree-Level Dataset.\n",
    "\n",
    "This script leverages the BigBird-Pegasus model, specifically designed for handling\n",
    "long sequences efficiently using block-sparse attention mechanisms. It incorporates\n",
    "several advanced training techniques:\n",
    "\n",
    "- Gradient Checkpointing: Reduces GPU memory footprint during training, enabling\n",
    "  fine-tuning of large models like BigBird-Pegasus on systems with limited VRAM,\n",
    "  at the cost of slightly increased computation time.\n",
    "- Mixed Precision Training (FP16): Accelerates training and further decreases\n",
    "  memory usage on compatible hardware (NVIDIA Volta GPUs or newer) by utilizing\n",
    "  half-precision floating-point numbers for certain computations.\n",
    "- Label Smoothing: A regularization technique applied to the loss function to\n",
    "  prevent the model from becoming overconfident, potentially enhancing robustness\n",
    "  and generalization.\n",
    "- Beam Search Generation: Employed during evaluation (`predict_with_generate=True`)\n",
    "  to produce output sequences by exploring multiple hypotheses, often leading to\n",
    "  more coherent results than simple greedy decoding.\n",
    "- Exact Match Accuracy: The primary evaluation metric, measuring the percentage\n",
    "  of generated sequences that exactly match the ground truth target sequences\n",
    "  after decoding and normalization (stripping whitespace).\n",
    "\n",
    "Workflow Overview:\n",
    "1.  Load pre-split training, validation, and test datasets from JSONL files.\n",
    "2.  Reconstruct source (input) and target (label) strings from token lists.\n",
    "3.  Initialize the BigBird-Pegasus tokenizer and model.\n",
    "4.  Enable gradient checkpointing on the loaded model instance.\n",
    "5.  Define a function to tokenize source/target string pairs, handling truncation\n",
    "    and padding according to the specified maximum sequence length.\n",
    "6.  Wrap the tokenized data into PyTorch Dataset objects.\n",
    "7.  Instantiate a Data Collator suitable for sequence-to-sequence tasks, which\n",
    "    handles dynamic batch padding and prepares decoder inputs/labels.\n",
    "8.  Define a function (`compute_metrics`) for calculating sequence accuracy based\n",
    "    on comparing decoded generated sequences with decoded labels.\n",
    "9.  Configure `Seq2SeqTrainingArguments`, setting hyperparameters and enabling\n",
    "    the advanced features (FP16, gradient checkpointing, label smoothing, etc.).\n",
    "10. Initialize the `Seq2SeqTrainer`.\n",
    "11. Execute the training loop.\n",
    "12. Perform final evaluation on the held-out test set.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    BigBirdPegasusForConditionalGeneration,\n",
    "    BigBirdPegasusTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainingArguments # Explicit import for clarity\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# File Paths (adjust if your preprocessed files have different names/locations)\n",
    "TRAIN_FILE = Path('qed_expressions_train.jsonl') # Training data\n",
    "VAL_FILE   = Path('qed_expressions_val.jsonl')   # Validation data\n",
    "TEST_FILE  = Path('qed_expressions_test.jsonl')  # Test data\n",
    "OUTPUT_DIR = Path('qed_bigbird_pegasus_output')  # Checkpoints and logs directory\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_ID = \"google/bigbird-pegasus-large-arxiv\" # BigBird specialized for scientific text\n",
    "\n",
    "# Tokenizer and Data Processing Configuration\n",
    "# BigBird supports longer sequences compared to standard transformers\n",
    "MAX_SEQ_LENGTH = 1024 # Max sequence length for inputs and targets\n",
    "\n",
    "# Training Hyperparameters\n",
    "# Note: BigBird is large; adjust batch sizes based on available GPU memory.\n",
    "NUM_TRAIN_EPOCHS = 3 # Adjust as needed\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 2 # Likely needs to be small (e.g., 1, 2, 4)\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 4  # Can often be slightly larger than train batch size\n",
    "LEARNING_RATE = 2e-5            # Common starting point for fine-tuning large models\n",
    "WEIGHT_DECAY = 0.01             # Regularization parameter\n",
    "WARMUP_STEPS = 250              # Linear learning rate warmup\n",
    "LOGGING_STEPS = 50              # Frequency of logging metrics\n",
    "EVALUATION_STRATEGY = \"epoch\"   # Evaluate every epoch\n",
    "SAVE_STRATEGY = \"epoch\"         # Save checkpoint every epoch\n",
    "\n",
    "# Advanced Training Feature Flags/Values\n",
    "USE_FP16 = torch.cuda.is_available() # Enable mixed precision if CUDA available\n",
    "USE_GRADIENT_CHECKPOINTING = True    # Enable gradient checkpointing for memory saving\n",
    "LABEL_SMOOTHING_FACTOR = 0.1         # Apply label smoothing (0.0 disables)\n",
    "\n",
    "# Generation Configuration (for evaluation with predict_with_generate=True)\n",
    "GENERATION_NUM_BEAMS = 4             # Number of beams for beam search\n",
    "\n",
    "# --- Helper Functions and Classes ---\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads data from a JSON Lines file, handling basic errors.\"\"\"\n",
    "    data = []\n",
    "    file_path = Path(file_path)\n",
    "    print(f\"[INFO] Loading data from: {file_path}\")\n",
    "    if not file_path.is_file():\n",
    "        print(f\"[Error] Data file not found: {file_path}\", file=sys.stderr)\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"[Warning] Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "                        continue\n",
    "        print(f\"[INFO] Successfully loaded {len(data)} records.\")\n",
    "        if not data:\n",
    "             print(f\"[Warning] No valid records loaded from {file_path}.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load data from {file_path}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "\n",
    "def convert_tokens_to_strings(raw_data_list):\n",
    "    \"\"\"Converts lists of tokens from loaded data into source and target strings.\"\"\"\n",
    "    source_strings = []\n",
    "    target_strings = []\n",
    "    if not raw_data_list:\n",
    "        return source_strings, target_strings\n",
    "\n",
    "    skipped_count = 0\n",
    "    for i, item in enumerate(raw_data_list):\n",
    "        input_toks = item.get('input_tokens')\n",
    "        target_toks = item.get('target_tokens')\n",
    "\n",
    "        if isinstance(input_toks, list) and isinstance(target_toks, list):\n",
    "            source_strings.append(\" \".join(map(str, input_toks)))\n",
    "            target_strings.append(\" \".join(map(str, target_toks)))\n",
    "        else:\n",
    "            print(f\"[Warning] Skipping item at index {i} due to missing/invalid keys or non-list values: {item}\")\n",
    "            skipped_count += 1\n",
    "\n",
    "    print(f\"[INFO] Converted {len(source_strings)} items to strings (skipped {skipped_count}).\")\n",
    "    if not source_strings:\n",
    "        print(\"[Warning] No items successfully converted to strings.\")\n",
    "    return source_strings, target_strings\n",
    "\n",
    "\n",
    "def encode_sequences(tokenizer, source_texts, target_texts, max_len):\n",
    "    \"\"\"\n",
    "    Tokenizes source and target text sequences using the provided tokenizer.\n",
    "\n",
    "    Handles padding to max_len and truncation for sequences exceeding max_len.\n",
    "    Prepares the 'labels' field required for sequence-to-sequence model training.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Initialized Hugging Face tokenizer instance.\n",
    "        source_texts (list[str]): List of source sequences for the encoder.\n",
    "        target_texts (list[str]): List of target sequences for the decoder labels.\n",
    "        max_len (int): Maximum sequence length.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing 'input_ids', 'attention_mask', and 'labels'.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Encoding sequence pairs with max_length={max_len}...\")\n",
    "\n",
    "    # Tokenize source texts\n",
    "    encoder_inputs = tokenizer(\n",
    "        source_texts,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',   # Pad shorter sequences to max_len\n",
    "        truncation=True,        # Truncate longer sequences to max_len\n",
    "        return_tensors=None     # Return lists; collator manages tensor conversion\n",
    "    )\n",
    "\n",
    "    # Tokenize target texts to create labels\n",
    "    # Using the context manager ensures correct handling if tokenizer has specific target modes\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        decoder_labels = tokenizer(\n",
    "            target_texts,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "    # Assign the tokenized target IDs as 'labels' for the model\n",
    "    encoder_inputs['labels'] = decoder_labels['input_ids']\n",
    "\n",
    "    print(f\"[INFO] Encoding complete.\")\n",
    "    # Validate encoding results basic check\n",
    "    if not encoder_inputs['input_ids'] or not encoder_inputs['labels']:\n",
    "        print(\"[Warning] Encoding resulted in empty lists for input_ids or labels.\")\n",
    "    elif len(encoder_inputs['input_ids']) != len(encoder_inputs['labels']):\n",
    "         print(\"[Warning] Mismatch between number of encoded inputs and labels.\")\n",
    "\n",
    "    return encoder_inputs\n",
    "\n",
    "\n",
    "class SequencePairDataset(Dataset):\n",
    "    \"\"\"Simple PyTorch Dataset for holding tokenized sequence pairs.\"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encodings (dict): A dictionary from the tokenizer, containing lists\n",
    "                              for 'input_ids', 'attention_mask', 'labels'.\n",
    "        \"\"\"\n",
    "        if not isinstance(encodings, dict) or 'input_ids' not in encodings:\n",
    "            raise ValueError(\"Encodings must be a dictionary containing at least 'input_ids'.\")\n",
    "        self.encodings = encodings\n",
    "        try:\n",
    "            self.length = len(encodings['input_ids'])\n",
    "            # Basic validation\n",
    "            for key in encodings:\n",
    "                if not isinstance(encodings[key], list) or len(encodings[key]) != self.length:\n",
    "                    raise ValueError(f\"Encoding key '{key}' is not a list or has inconsistent length.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to validate encodings: {e}\")\n",
    "\n",
    "        if self.length == 0:\n",
    "            raise ValueError(\"Input encodings are empty.\")\n",
    "        print(f\"[INFO] Created Dataset with {self.length} examples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the tokenized data for a single sample index.\n",
    "        Tensor conversion is typically handled by the DataCollator.\n",
    "        \"\"\"\n",
    "        if not 0 <= idx < self.length:\n",
    "             raise IndexError(f\"Index {idx} out of bounds for dataset of length {self.length}.\")\n",
    "        try:\n",
    "            # Return a dictionary containing the data for the specified index\n",
    "            return {key: self.encodings[key][idx] for key in self.encodings}\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to retrieve item at index {idx}: {e}\", file=sys.stderr)\n",
    "            # Depending on severity, could return None or re-raise\n",
    "            raise\n",
    "\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Orchestrates the BigBird-Pegasus fine-tuning and evaluation pipeline.\"\"\"\n",
    "    print(\"[INFO] Starting BigBird-Pegasus Fine-tuning Script...\")\n",
    "    try:\n",
    "        # Use timezone-aware UTC time\n",
    "        current_time_str = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
    "    except Exception:\n",
    "        current_time_str = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC (naive)') # Fallback\n",
    "    print(f\"[INFO] Current date/time (UTC): {current_time_str}\")\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    try:\n",
    "        train_raw_data = load_jsonl(TRAIN_FILE)\n",
    "        val_raw_data = load_jsonl(VAL_FILE)\n",
    "        test_raw_data = load_jsonl(TEST_FILE)\n",
    "    except Exception as e: # Catch errors from load_jsonl (FileNotFound, JSONDecode, etc.)\n",
    "        print(f\"[FATAL] Critical error during data loading: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not train_raw_data or not val_raw_data or not test_raw_data:\n",
    "        print(\"[FATAL] One or more required datasets (train, val, test) are empty or failed to load. Exiting.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 2. Prepare Data (Tokens to Strings) ---\n",
    "    train_sources, train_targets = convert_tokens_to_strings(train_raw_data)\n",
    "    val_sources,   val_targets   = convert_tokens_to_strings(val_raw_data)\n",
    "    test_sources,  test_targets  = convert_tokens_to_strings(test_raw_data)\n",
    "\n",
    "    if not train_sources or not val_sources or not test_sources:\n",
    "        print(\"[FATAL] Data conversion resulted in one or more empty lists. Check input data format. Exiting.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 3. Initialize Tokenizer and Model ---\n",
    "    print(f\"[INFO] Initializing Tokenizer and Model: {MODEL_ID}\")\n",
    "    try:\n",
    "        tokenizer = BigBirdPegasusTokenizer.from_pretrained(MODEL_ID)\n",
    "        model = BigBirdPegasusForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed to initialize tokenizer or model '{MODEL_ID}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Enable Gradient Checkpointing on the model if configured\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        try:\n",
    "            model.gradient_checkpointing_enable()\n",
    "            print(\"[INFO] Gradient Checkpointing enabled on the model.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Failed to enable gradient checkpointing on model: {e}. Training argument will be disabled.\", file=sys.stderr)\n",
    "            # Ensure the training argument reflects this failure\n",
    "            global USE_GRADIENT_CHECKPOINTING_EFFECTIVE\n",
    "            USE_GRADIENT_CHECKPOINTING_EFFECTIVE = False\n",
    "    else:\n",
    "         USE_GRADIENT_CHECKPOINTING_EFFECTIVE = False\n",
    "\n",
    "    # --- 4. Tokenize Data ---\n",
    "    try:\n",
    "        # Define tokenizer globally for metric computation simplicity\n",
    "        global tokenizer_for_metrics\n",
    "        tokenizer_for_metrics = tokenizer\n",
    "\n",
    "        train_encodings = encode_sequences(tokenizer, train_sources, train_targets, MAX_SEQ_LENGTH)\n",
    "        val_encodings   = encode_sequences(tokenizer, val_sources,   val_targets,   MAX_SEQ_LENGTH)\n",
    "        test_encodings  = encode_sequences(tokenizer, test_sources,  test_targets,  MAX_SEQ_LENGTH)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed during data tokenization: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "     # Validate that encodings are not empty after tokenization\n",
    "    if not train_encodings or not train_encodings.get('input_ids') or \\\n",
    "       not val_encodings or not val_encodings.get('input_ids') or \\\n",
    "       not test_encodings or not test_encodings.get('input_ids'):\n",
    "         print(\"[FATAL] Tokenization resulted in empty encodings for one or more splits. Exiting.\", file=sys.stderr)\n",
    "         sys.exit(1)\n",
    "\n",
    "    # --- 5. Create Datasets ---\n",
    "    try:\n",
    "        train_dataset = SequencePairDataset(train_encodings)\n",
    "        val_dataset   = SequencePairDataset(val_encodings)\n",
    "        test_dataset  = SequencePairDataset(test_encodings)\n",
    "    except ValueError as e:\n",
    "        print(f\"[FATAL] Failed to create Dataset objects: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 6. Initialize Data Collator ---\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100, # Ignore padding tokens in loss calculation\n",
    "        pad_to_multiple_of=8 if USE_FP16 else None # Optimize for FP16 if used\n",
    "    )\n",
    "    print(\"[INFO] Data collator initialized.\")\n",
    "\n",
    "    # --- 7. Define Metrics Computation ---\n",
    "    def compute_metrics_fn(eval_pred):\n",
    "        \"\"\"Calculates exact sequence match accuracy after decoding.\"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        if not isinstance(predictions, np.ndarray): predictions = np.array(predictions)\n",
    "        if not isinstance(labels, np.ndarray): labels = np.array(labels)\n",
    "\n",
    "        # Replace -100 (ignore index) with pad_token_id for decoding\n",
    "        labels = np.where(labels != -100, labels, tokenizer_for_metrics.pad_token_id)\n",
    "\n",
    "        try:\n",
    "            decoded_preds = tokenizer_for_metrics.batch_decode(predictions, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer_for_metrics.batch_decode(labels, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "             print(f\"[Error] Decoding failed in compute_metrics: {e}\", file=sys.stderr)\n",
    "             return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "        # Post-processing: strip whitespace for comparison\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "        if len(decoded_preds) != len(decoded_labels):\n",
    "            print(f\"[Warning] Mismatch in prediction/label count after decoding: {len(decoded_preds)} vs {len(decoded_labels)}\", file=sys.stderr)\n",
    "            return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "        matches = [pred == label for pred, label in zip(decoded_preds, decoded_labels)]\n",
    "        accuracy = np.mean(matches) if matches else 0.0\n",
    "        return {\"sequence_accuracy\": float(accuracy)}\n",
    "\n",
    "    # --- 8. Define Training Arguments ---\n",
    "    # Update effective GC flag if model enabling failed\n",
    "    effective_gc = USE_GRADIENT_CHECKPOINTING if 'USE_GRADIENT_CHECKPOINTING_EFFECTIVE' not in globals() else USE_GRADIENT_CHECKPOINTING_EFFECTIVE\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR),\n",
    "        # Core training parameters\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        # Logging, saving, evaluation strategies\n",
    "        logging_dir=str(OUTPUT_DIR / 'logs'),\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        evaluation_strategy=EVALUATION_STRATEGY,\n",
    "        save_strategy=SAVE_STRATEGY,\n",
    "        save_total_limit=2,            # Keep only last 2 checkpoints\n",
    "        load_best_model_at_end=True,   # Reload best model found during training\n",
    "        metric_for_best_model=\"sequence_accuracy\", # Metric to define \"best\"\n",
    "        greater_is_better=True,        # Higher accuracy is better\n",
    "        # Generation configuration\n",
    "        predict_with_generate=True,    # Use model.generate() for evaluation\n",
    "        generation_max_length=MAX_SEQ_LENGTH, # Max length during generation\n",
    "        generation_num_beams=GENERATION_NUM_BEAMS, # Beam search width\n",
    "        # Advanced features\n",
    "        fp16=USE_FP16,\n",
    "        label_smoothing_factor=LABEL_SMOOTHING_FACTOR,\n",
    "        gradient_checkpointing=effective_gc, # Use effective flag based on model setup\n",
    "        # report_to=\"tensorboard\",     # Optional: Enable TensorBoard logging\n",
    "    )\n",
    "    print(\"[INFO] Training arguments defined.\")\n",
    "    print(f\"[INFO] Effective Mixed Precision (FP16): {'Enabled' if USE_FP16 else 'Disabled'}\")\n",
    "    print(f\"[INFO] Effective Label Smoothing Factor: {LABEL_SMOOTHING_FACTOR}\")\n",
    "    print(f\"[INFO] Effective Gradient Checkpointing: {'Enabled' if effective_gc else 'Disabled'}\")\n",
    "\n",
    "    # --- 9. Initialize Trainer ---\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "    )\n",
    "    print(\"[INFO] Seq2SeqTrainer initialized.\")\n",
    "\n",
    "    # --- 10. Train the Model ---\n",
    "    print(f\"[INFO] Starting model training for {NUM_TRAIN_EPOCHS} epochs...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        # Log and save final training state and metrics\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        # Best model is loaded automatically if load_best_model_at_end=True\n",
    "        print(\"[INFO] Training finished successfully.\")\n",
    "        if trainer.state.best_model_checkpoint:\n",
    "             print(f\"[INFO] Best model checkpoint saved at: {trainer.state.best_model_checkpoint}\")\n",
    "        else:\n",
    "             print(\"[Warning] No best model checkpoint recorded by trainer state.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Training loop encountered an error: {e}\", file=sys.stderr)\n",
    "        # Potentially log more details about the state here\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 11. Evaluate on Test Set ---\n",
    "    print(\"[INFO] Evaluating final model on the test set...\")\n",
    "    try:\n",
    "        test_results = trainer.evaluate(\n",
    "            eval_dataset=test_dataset,\n",
    "            metric_key_prefix=\"test\" # Prefix test metrics (e.g., test_loss, test_sequence_accuracy)\n",
    "        )\n",
    "        trainer.log_metrics(\"test\", test_results)\n",
    "        trainer.save_metrics(\"test\", test_results)\n",
    "\n",
    "        # Report final test accuracy clearly\n",
    "        if 'test_sequence_accuracy' in test_results:\n",
    "             print(f\"\\n--- Test Set Results ---\")\n",
    "             print(f\"Test Sequence Accuracy: {test_results['test_sequence_accuracy']:.4f}\")\n",
    "             print(f\"------------------------\")\n",
    "        else:\n",
    "             print(\"[Warning] 'test_sequence_accuracy' metric not found in test results.\")\n",
    "             print(\"Full test results:\", test_results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Evaluation on test set failed: {e}\", file=sys.stderr)\n",
    "        # The model is trained, but test evaluation failed.\n",
    "        sys.exit(1) # Or choose to exit with a different code\n",
    "\n",
    "    print(\"[INFO] Script finished successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Optional: Add argument parsing here (e.g., using argparse) to override\n",
    "    # configuration variables like paths, batch sizes, epochs, etc.\n",
    "\n",
    "    # Optional: Set random seeds for reproducibility across libraries\n",
    "    # SEED = 42\n",
    "    # torch.manual_seed(SEED)\n",
    "    # np.random.seed(SEED)\n",
    "    # import random; random.seed(SEED)\n",
    "    # if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "    # Consider also torch.backends.cudnn.deterministic = True / benchmark = False for stricter reproducibility\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2: State-space Models for Squared Amplitude Calculation in High-Energy Physics\n",
    "Model: State-space model such as mamba or other model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] 'mamba_ssm' library not found. Please install it (`pip install mamba_ssm causal-conv1d>=1.1.0`) or replace StateSpaceLayer with your implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting Custom SSM Sequence-to-Sequence Script...\n",
      "[INFO] Current date/time (UTC): 2025-04-08 17:34:48 UTC\n",
      "[INFO] Using CPU\n",
      "[INFO] Initializing Tokenizer: bert-base-uncased\n",
      "[INFO] Tokenizer Vocab Size: 30522, Pad Token ID: 0\n",
      "[INFO] Loading data from: qed_expressions_train.jsonl\n",
      "[INFO] Successfully loaded 12441 records.\n",
      "[INFO] Loading data from: qed_expressions_val.jsonl\n",
      "[INFO] Successfully loaded 1555 records.\n",
      "[INFO] Loading data from: qed_expressions_test.jsonl\n",
      "[INFO] Successfully loaded 1556 records.\n",
      "[INFO] Converted 12441 items to strings (skipped 0).\n",
      "[INFO] Converted 1555 items to strings (skipped 0).\n",
      "[INFO] Converted 1556 items to strings (skipped 0).\n",
      "[INFO] Pre-tokenizing dataset with max_length=256...\n",
      "[INFO] Pre-tokenization complete. Dataset size: 12441 examples.\n",
      "[INFO] Pre-tokenizing dataset with max_length=256...\n",
      "[INFO] Pre-tokenization complete. Dataset size: 1555 examples.\n",
      "[INFO] Pre-tokenizing dataset with max_length=256...\n",
      "[INFO] Pre-tokenization complete. Dataset size: 1556 examples.\n",
      "[INFO] DataLoaders created.\n",
      "[INFO] Initializing SSMEncoderDecoder model...\n",
      "[Warning] Using Dummy StateSpaceLayer as 'mamba_ssm' not found.\n",
      "[Warning] Using Dummy StateSpaceLayer as 'mamba_ssm' not found.\n",
      "[Warning] Using Dummy StateSpaceLayer as 'mamba_ssm' not found.\n",
      "[Warning] Using Dummy StateSpaceLayer as 'mamba_ssm' not found.\n",
      "[Warning] Using Dummy StateSpaceLayer as 'mamba_ssm' not found.\n",
      "[Warning] Using Dummy StateSpaceLayer as 'mamba_ssm' not found.\n",
      "[INFO] Initialized SSMEncoderDecoder model:\n",
      "  - Vocab Size: 30522\n",
      "  - Embedding Dim (d_model): 512\n",
      "  - Max Sequence Length: 256\n",
      "  - Encoder SSM Layers: 6\n",
      "  - SSM Kwargs: {'d_state': 16, 'd_conv': 4, 'expand': 2}\n",
      "  - Pad Token ID: 0\n",
      "[INFO] Optimizer and Learning Rate Scheduler initialized.\n",
      "[INFO] Starting training for 5 epochs...\n",
      "\n",
      "--- Epoch 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/778 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                         \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 596\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[INFO] Script finished successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;66;03m# Optional: Add argument parsing (argparse) here for flexibility\u001b[39;00m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;66;03m# Optional: Set random seeds for reproducibility\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;66;03m# import random; random.seed(SEED)\u001b[39;00m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 478\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    474\u001b[0m train_pbar \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Training\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_pbar):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# Move batch to device\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m    479\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# May be unused by model but good practice\u001b[39;00m\n\u001b[1;32m    480\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "End-to-end Training and Evaluation Script for a Custom Sequence-to-Sequence\n",
    "Model using State Space Model (SSM) layers (based on Mamba architecture concepts)\n",
    "on the Preprocessed QED 2-to-2 Tree-Level Dataset.\n",
    "\n",
    "This script implements a manual PyTorch training loop, including:\n",
    "- Data loading from preprocessed JSONL files.\n",
    "- A custom Dataset class with efficient pre-tokenization.\n",
    "- Definition of an SSM-based Encoder followed by a Linear Decoder head.\n",
    "- A standard training loop with optimizer steps and loss calculation.\n",
    "- Validation and testing loops calculating exact sequence match accuracy.\n",
    "- Device management (CPU/GPU).\n",
    "\n",
    "Note: This implementation uses a hypothetical `StateSpaceLayer` from a library\n",
    "      named 'mamba'. Ensure such a library/layer compatible with PyTorch exists\n",
    "      and is installed in your environment. The 'decoder' here is a simple\n",
    "      linear projection, predicting the target sequence in parallel based on\n",
    "      encoder outputs, not an auto-regressive generative decoder.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "# --- Hypothetical SSM Layer Import ---\n",
    "# Ensure 'mamba' library and 'StateSpaceLayer' are correctly installed/defined\n",
    "try:\n",
    "    # Attempt to import the specific layer needed\n",
    "    from mamba_ssm import Mamba # Example using official 'mamba_ssm' package\n",
    "    # If using a different library, adjust the import accordingly.\n",
    "    # For this example, let's assume Mamba serves as the StateSpaceLayer\n",
    "    StateSpaceLayer = Mamba # Alias for clarity in the model definition below\n",
    "    print(\"[INFO] Using Mamba layer from 'mamba_ssm' as StateSpaceLayer.\")\n",
    "except ImportError:\n",
    "    print(\"[ERROR] 'mamba_ssm' library not found. Please install it (`pip install mamba_ssm causal-conv1d>=1.1.0`) or replace StateSpaceLayer with your implementation.\", file=sys.stderr)\n",
    "    # Define a dummy layer to allow script structure analysis if library missing\n",
    "    class StateSpaceLayer(nn.Module):\n",
    "        def __init__(self, d_model, *args, **kwargs):\n",
    "             super().__init__()\n",
    "             print(\"[Warning] Using Dummy StateSpaceLayer as 'mamba_ssm' not found.\")\n",
    "             self.layer = nn.Linear(d_model, d_model) # Simple placeholder\n",
    "        def forward(self, x): return self.layer(x)\n",
    "    # sys.exit(1) # Optionally exit if the real layer is crucial\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# File Paths\n",
    "TRAIN_FILE = Path('qed_expressions_train.jsonl')\n",
    "VAL_FILE   = Path('qed_expressions_val.jsonl')\n",
    "TEST_FILE  = Path('qed_expressions_test.jsonl')\n",
    "OUTPUT_DIR = Path('qed_ssm_model_output')\n",
    "CHECKPOINT_NAME = \"ssm_seq2seq_best.pt\"\n",
    "\n",
    "# Tokenizer Configuration\n",
    "TOKENIZER_ID = \"bert-base-uncased\"\n",
    "MAX_SEQ_LENGTH = 256 # Max sequence length for tokenization\n",
    "\n",
    "# Model Hyperparameters\n",
    "D_MODEL = 512       # Core dimensionality of the model embeddings and layers\n",
    "N_LAYERS = 6        # Number of stacked SSM layers in the encoder\n",
    "# SSM_KERNEL_SIZE = 64 # Parameter specific to hypothetical 'StateSpaceLayer' - adjust based on actual layer args\n",
    "# Mamba specific args (replace StateSpaceLayer args if using the library directly)\n",
    "MAMBA_D_STATE = 16  # Typical value for Mamba state dimension\n",
    "MAMBA_D_CONV = 4    # Typical value for Mamba conv dimension\n",
    "MAMBA_EXPAND = 2    # Typical value for Mamba expansion factor\n",
    "\n",
    "# Training Hyperparameters\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 16     # Adjust based on GPU memory\n",
    "LEARNING_RATE = 3e-4\n",
    "OPTIMIZER_EPS = 1e-8\n",
    "WEIGHT_DECAY = 0.01\n",
    "LR_SCHEDULER_TYPE = \"linear\" # Type of learning rate scheduler\n",
    "WARMUP_RATIO = 0.1           # Percentage of training steps for warmup\n",
    "LOG_INTERVAL = 50            # Log training loss every N steps\n",
    "\n",
    "# --- Helper Functions and Classes ---\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads data from a JSON Lines file, handling basic errors.\"\"\"\n",
    "    # (Same implementation as previous script)\n",
    "    data = []\n",
    "    file_path = Path(file_path)\n",
    "    print(f\"[INFO] Loading data from: {file_path}\")\n",
    "    if not file_path.is_file():\n",
    "        print(f\"[Error] Data file not found: {file_path}\", file=sys.stderr)\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"[Warning] Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "                        continue\n",
    "        print(f\"[INFO] Successfully loaded {len(data)} records.\")\n",
    "        if not data:\n",
    "             print(f\"[Warning] No valid records loaded from {file_path}.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load data from {file_path}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "def convert_tokens_to_strings(raw_data_list):\n",
    "    \"\"\"Converts lists of tokens from loaded data into source and target strings.\"\"\"\n",
    "    # (Same implementation as previous script)\n",
    "    source_strings = []\n",
    "    target_strings = []\n",
    "    if not raw_data_list:\n",
    "        return source_strings, target_strings\n",
    "\n",
    "    skipped_count = 0\n",
    "    for i, item in enumerate(raw_data_list):\n",
    "        input_toks = item.get('input_tokens')\n",
    "        target_toks = item.get('target_tokens')\n",
    "\n",
    "        if isinstance(input_toks, list) and isinstance(target_toks, list):\n",
    "            source_strings.append(\" \".join(map(str, input_toks)))\n",
    "            target_strings.append(\" \".join(map(str, target_toks)))\n",
    "        else:\n",
    "            print(f\"[Warning] Skipping item at index {i} due to missing/invalid keys or non-list values: {item}\")\n",
    "            skipped_count += 1\n",
    "\n",
    "    print(f\"[INFO] Converted {len(source_strings)} items to strings (skipped {skipped_count}).\")\n",
    "    if not source_strings:\n",
    "        print(\"[Warning] No items successfully converted to strings.\")\n",
    "    return source_strings, target_strings\n",
    "\n",
    "\n",
    "class PretokenizedSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset storing pre-tokenized sequences for efficiency.\n",
    "\n",
    "    Assumes tokenization is done once during initialization.\n",
    "    \"\"\"\n",
    "    def __init__(self, source_strings, target_strings, tokenizer, max_len):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by tokenizing all source and target strings.\n",
    "\n",
    "        Args:\n",
    "            source_strings (list[str]): List of source sequences.\n",
    "            target_strings (list[str]): List of target sequences.\n",
    "            tokenizer: Initialized Hugging Face tokenizer instance.\n",
    "            max_len (int): Maximum sequence length for tokenization.\n",
    "        \"\"\"\n",
    "        print(f\"[INFO] Pre-tokenizing dataset with max_length={max_len}...\")\n",
    "        if len(source_strings) != len(target_strings):\n",
    "            raise ValueError(\"Source and target string lists must have the same length.\")\n",
    "\n",
    "        # Tokenize source sequences\n",
    "        source_encodings = tokenizer(\n",
    "            source_strings,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=None # Get lists of IDs first\n",
    "        )\n",
    "\n",
    "        # Tokenize target sequences to get labels\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            target_encodings = tokenizer(\n",
    "                target_strings,\n",
    "                max_length=max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors=None\n",
    "            )\n",
    "\n",
    "        self.input_ids = source_encodings['input_ids']\n",
    "        self.attention_mask = source_encodings['attention_mask']\n",
    "        self.labels = target_encodings['input_ids']\n",
    "\n",
    "        self.length = len(self.input_ids)\n",
    "        if self.length == 0:\n",
    "            raise ValueError(\"Tokenization resulted in an empty dataset.\")\n",
    "        print(f\"[INFO] Pre-tokenization complete. Dataset size: {self.length} examples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieves pre-tokenized data for a given index.\"\"\"\n",
    "        if not 0 <= idx < self.length:\n",
    "            raise IndexError(f\"Index {idx} out of bounds for dataset of length {self.length}.\")\n",
    "        # Return data as a dictionary. DataLoader will handle batching and tensor conversion.\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "class SSMEncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A sequence-to-sequence model using an SSM-based encoder and a linear decoder head.\n",
    "\n",
    "    Note: The 'decoder' predicts the entire target sequence in parallel based on the\n",
    "          final hidden states of the encoder, not auto-regressively.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=512, n_layers=4, max_len=256, pad_token_id=0, **ssm_kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the SSM Encoder-Decoder model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "            d_model (int): The dimensionality of embeddings and hidden states.\n",
    "            n_layers (int): The number of SSM layers in the encoder stack.\n",
    "            max_len (int): Maximum sequence length for positional embeddings.\n",
    "            pad_token_id (int): The ID of the padding token for loss calculation.\n",
    "            **ssm_kwargs: Additional keyword arguments passed to the StateSpaceLayer\n",
    "                          (e.g., d_state, d_conv, expand for Mamba).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Input Embeddings: Token + Absolute Positional\n",
    "        # Padding_idx prevents the padding token embedding from being updated.\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_token_id)\n",
    "        # Learnable absolute positional embeddings\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "        # Optional: Dropout after embeddings\n",
    "        self.emb_dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Encoder Stack: Consists of multiple State Space Model layers.\n",
    "        # These layers process the sequence sequentially, updating an internal state.\n",
    "        self.encoder_ssm_stack = nn.ModuleList([\n",
    "            StateSpaceLayer(d_model=d_model, **ssm_kwargs)\n",
    "            # Example Mamba args: d_state=16, d_conv=4, expand=2\n",
    "            # Ensure ssm_kwargs match the expected arguments of your StateSpaceLayer\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        # Optional: Layer Normalization after the SSM stack\n",
    "        self.encoder_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Decoder Head: Simple linear layer projecting final hidden states to vocabulary logits.\n",
    "        # Predicts each target token independently based on the corresponding encoder output state.\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        print(f\"[INFO] Initialized SSMEncoderDecoder model:\")\n",
    "        print(f\"  - Vocab Size: {vocab_size}\")\n",
    "        print(f\"  - Embedding Dim (d_model): {d_model}\")\n",
    "        print(f\"  - Max Sequence Length: {max_len}\")\n",
    "        print(f\"  - Encoder SSM Layers: {n_layers}\")\n",
    "        print(f\"  - SSM Kwargs: {ssm_kwargs}\")\n",
    "        print(f\"  - Pad Token ID: {pad_token_id}\")\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Tensor of input token IDs (batch_size, seq_len).\n",
    "            attention_mask (torch.Tensor, optional): Mask indicating non-padding tokens.\n",
    "                                                    Currently unused by this simple SSM stack\n",
    "                                                    but kept for API consistency.\n",
    "            labels (torch.Tensor, optional): Tensor of target token IDs for loss calculation\n",
    "                                             (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing 'loss' (if labels provided) and 'logits'.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        # 1. Embeddings\n",
    "        token_embeddings = self.token_emb(input_ids) # (batch_size, seq_len, d_model)\n",
    "        # Add positional embeddings (slice to match input seq_len)\n",
    "        positional_embeddings = self.pos_emb[:, :seq_len, :] # (1, seq_len, d_model)\n",
    "        x = token_embeddings + positional_embeddings # (batch_size, seq_len, d_model)\n",
    "        x = self.emb_dropout(x)\n",
    "\n",
    "        # 2. Encoder SSM Stack\n",
    "        # Note: Basic SSM layers might not inherently use an attention mask.\n",
    "        # More advanced implementations might incorporate masking.\n",
    "        for ssm_layer in self.encoder_ssm_stack:\n",
    "            x = ssm_layer(x) # (batch_size, seq_len, d_model) - state is internal\n",
    "\n",
    "        x = self.encoder_norm(x) # Apply layer norm after the stack\n",
    "\n",
    "        # 3. Output Projection (Decoder Head)\n",
    "        # Project final hidden states to vocabulary logits for each position\n",
    "        logits = self.output_projection(x) # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        # 4. Loss Calculation (if labels are provided)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # CrossEntropyLoss expects logits as (N, C) and labels as (N)\n",
    "            # N = batch_size * seq_len, C = vocab_size\n",
    "            # Flatten logits and labels, ignoring padding tokens in labels.\n",
    "            loss = nn.functional.cross_entropy(\n",
    "                logits.view(-1, self.vocab_size), # (batch_size * seq_len, vocab_size)\n",
    "                labels.view(-1),                  # (batch_size * seq_len)\n",
    "                ignore_index=self.pad_token_id    # Ignore padding tokens when calculating loss\n",
    "            )\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "\n",
    "def calculate_sequence_accuracy(logits, labels, pad_token_id):\n",
    "    \"\"\"\n",
    "    Calculates exact sequence match accuracy, ignoring padding.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Model output logits (batch_size, seq_len, vocab_size).\n",
    "        labels (torch.Tensor): Ground truth labels (batch_size, seq_len).\n",
    "        pad_token_id (int): The ID of the padding token.\n",
    "\n",
    "    Returns:\n",
    "        float: The fraction of sequences that match the labels exactly (excluding padding).\n",
    "    \"\"\"\n",
    "    if logits.shape[0] == 0: return 0.0 # Handle empty batch case\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1) # Get predicted token IDs (batch_size, seq_len)\n",
    "\n",
    "    # Create mask for non-padding tokens in labels\n",
    "    non_pad_mask = (labels != pad_token_id)\n",
    "\n",
    "    # Check equality only for non-padded positions\n",
    "    correct_tokens = (predictions == labels) & non_pad_mask # (batch_size, seq_len) boolean\n",
    "\n",
    "    # Check if all non-padded tokens in each sequence are correct\n",
    "    # Sum correct tokens and mask per sequence. If sums match, the sequence is correct.\n",
    "    correct_sequences = (torch.sum(correct_tokens, dim=1) == torch.sum(non_pad_mask, dim=1))\n",
    "\n",
    "    # Calculate mean accuracy across the batch\n",
    "    accuracy = torch.mean(correct_sequences.float()) # Convert bool tensor to float for mean\n",
    "\n",
    "    return accuracy.item() # Return as Python float\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Orchestrates the SSM model training and evaluation pipeline.\"\"\"\n",
    "    print(\"[INFO] Starting Custom SSM Sequence-to-Sequence Script...\")\n",
    "    try:\n",
    "        current_time_str = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
    "    except Exception:\n",
    "        current_time_str = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC (naive)')\n",
    "    print(f\"[INFO] Current date/time (UTC): {current_time_str}\")\n",
    "\n",
    "    # --- 1. Setup Device ---\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"[INFO] Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    elif torch.backends.mps.is_available(): # Check for Apple Silicon GPU\n",
    "         device = torch.device(\"mps\")\n",
    "         print(\"[INFO] Using MPS device (Apple Silicon GPU)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"[INFO] Using CPU\")\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- 2. Initialize Tokenizer ---\n",
    "    print(f\"[INFO] Initializing Tokenizer: {TOKENIZER_ID}\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_ID)\n",
    "        # Check for pad token; add if missing (though BERT usually has one)\n",
    "        if tokenizer.pad_token is None:\n",
    "            print(\"[Warning] Tokenizer does not have a default pad token. Adding '[PAD]'.\")\n",
    "            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            # Resize model embeddings later if vocab size changes\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed to initialize tokenizer '{TOKENIZER_ID}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    print(f\"[INFO] Tokenizer Vocab Size: {vocab_size}, Pad Token ID: {pad_token_id}\")\n",
    "\n",
    "    # --- 3. Load and Prepare Data ---\n",
    "    try:\n",
    "        train_raw = load_jsonl(TRAIN_FILE)\n",
    "        val_raw = load_jsonl(VAL_FILE)\n",
    "        test_raw = load_jsonl(TEST_FILE)\n",
    "\n",
    "        train_src, train_tgt = convert_tokens_to_strings(train_raw)\n",
    "        val_src,   val_tgt   = convert_tokens_to_strings(val_raw)\n",
    "        test_src,  test_tgt  = convert_tokens_to_strings(test_raw)\n",
    "\n",
    "        # Create Datasets (with pre-tokenization)\n",
    "        train_dataset = PretokenizedSequenceDataset(train_src, train_tgt, tokenizer, MAX_SEQ_LENGTH)\n",
    "        val_dataset   = PretokenizedSequenceDataset(val_src, val_tgt, tokenizer, MAX_SEQ_LENGTH)\n",
    "        test_dataset  = PretokenizedSequenceDataset(test_src, test_tgt, tokenizer, MAX_SEQ_LENGTH)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed during data loading or preprocessing: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not train_dataset or not val_dataset or not test_dataset:\n",
    "         print(\"[FATAL] One or more datasets are empty after processing. Exiting.\", file=sys.stderr)\n",
    "         sys.exit(1)\n",
    "\n",
    "    # --- 4. Create DataLoaders ---\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_dataloader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_dataloader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    print(\"[INFO] DataLoaders created.\")\n",
    "\n",
    "    # --- 5. Initialize Model ---\n",
    "    print(\"[INFO] Initializing SSMEncoderDecoder model...\")\n",
    "    # Define SSM specific arguments based on configuration\n",
    "    ssm_params = {\n",
    "        \"d_state\": MAMBA_D_STATE,\n",
    "        \"d_conv\": MAMBA_D_CONV,\n",
    "        \"expand\": MAMBA_EXPAND\n",
    "        # Add other args like 'kernel_size' if your StateSpaceLayer uses it\n",
    "    }\n",
    "    model = SSMEncoderDecoder(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=D_MODEL,\n",
    "        n_layers=N_LAYERS,\n",
    "        max_len=MAX_SEQ_LENGTH,\n",
    "        pad_token_id=pad_token_id,\n",
    "        **ssm_params\n",
    "    )\n",
    "\n",
    "    # If pad token was added, resize embeddings\n",
    "    if len(tokenizer) > vocab_size: # Vocab size increased\n",
    "        print(f\"[INFO] Resizing token embeddings to accommodate new vocab size: {len(tokenizer)}\")\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        # Ensure new vocab size is used if needed elsewhere\n",
    "        # vocab_size = len(tokenizer) # Update if needed\n",
    "\n",
    "    model.to(device) # Move model to the appropriate device\n",
    "\n",
    "    # --- 6. Initialize Optimizer and Scheduler ---\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        eps=OPTIMIZER_EPS,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    total_training_steps = len(train_dataloader) * NUM_EPOCHS\n",
    "    num_warmup_steps = int(total_training_steps * WARMUP_RATIO)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=total_training_steps\n",
    "    )\n",
    "    print(\"[INFO] Optimizer and Learning Rate Scheduler initialized.\")\n",
    "\n",
    "    # --- 7. Training Loop ---\n",
    "    print(f\"[INFO] Starting training for {NUM_EPOCHS} epochs...\")\n",
    "    best_val_accuracy = -1.0\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "\n",
    "        # --- Training Phase ---\n",
    "        model.train() # Set model to training mode\n",
    "        total_train_loss = 0.0\n",
    "        train_pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\", leave=False)\n",
    "\n",
    "        for step, batch in enumerate(train_pbar):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device) # May be unused by model but good practice\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs['loss']\n",
    "\n",
    "            if loss is None:\n",
    "                 print(f\"[Warning] Loss is None at step {step}. Skipping backward pass.\")\n",
    "                 continue\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Optional: Gradient clipping\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step() # Update learning rate\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Log training loss periodically\n",
    "            if (step + 1) % LOG_INTERVAL == 0:\n",
    "                avg_loss = total_train_loss / LOG_INTERVAL\n",
    "                train_pbar.set_postfix({'Avg Loss': f'{avg_loss:.4f}', 'LR': f'{scheduler.get_last_lr()[0]:.2e}'})\n",
    "                total_train_loss = 0.0 # Reset loss accumulator\n",
    "\n",
    "        train_pbar.close()\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        total_val_accuracy = 0.0\n",
    "        total_val_samples = 0\n",
    "        val_pbar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1} Validation\", leave=False)\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient calculations\n",
    "            for batch in val_pbar:\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                # Forward pass to get logits\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=None) # No labels needed for inference here\n",
    "                logits = outputs['logits']\n",
    "\n",
    "                # Calculate accuracy for this batch\n",
    "                batch_accuracy = calculate_sequence_accuracy(logits, labels, pad_token_id)\n",
    "                batch_size = input_ids.size(0)\n",
    "                total_val_accuracy += batch_accuracy * batch_size # Accumulate weighted accuracy\n",
    "                total_val_samples += batch_size\n",
    "\n",
    "        val_pbar.close()\n",
    "        epoch_val_accuracy = total_val_accuracy / total_val_samples if total_val_samples > 0 else 0.0\n",
    "        print(f\"Epoch {epoch+1} Validation Sequence Accuracy: {epoch_val_accuracy:.4f}\")\n",
    "\n",
    "        # --- Save Best Model ---\n",
    "        if epoch_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = epoch_val_accuracy\n",
    "            best_epoch = epoch + 1\n",
    "            save_path = OUTPUT_DIR / CHECKPOINT_NAME\n",
    "            try:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"[INFO] New best model saved to {save_path} (Epoch {best_epoch}, Val Acc: {best_val_accuracy:.4f})\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed to save model checkpoint: {e}\", file=sys.stderr)\n",
    "\n",
    "    print(f\"\\n[INFO] Training complete. Best validation accuracy: {best_val_accuracy:.4f} at epoch {best_epoch}.\")\n",
    "\n",
    "    # --- 8. Final Test Evaluation ---\n",
    "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
    "    # Load the best model checkpoint\n",
    "    best_model_path = OUTPUT_DIR / CHECKPOINT_NAME\n",
    "    if best_model_path.exists():\n",
    "        print(f\"[INFO] Loading best model from: {best_model_path}\")\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        except Exception as e:\n",
    "             print(f\"[Error] Failed to load best model checkpoint: {e}. Evaluating with the last state.\", file=sys.stderr)\n",
    "    else:\n",
    "        print(\"[Warning] Best model checkpoint not found. Evaluating with the final model state.\", file=sys.stderr)\n",
    "\n",
    "    model.eval() # Ensure model is in eval mode\n",
    "    total_test_accuracy = 0.0\n",
    "    total_test_samples = 0\n",
    "    test_pbar = tqdm(test_dataloader, desc=\"Testing\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=None)\n",
    "            logits = outputs['logits']\n",
    "\n",
    "            batch_accuracy = calculate_sequence_accuracy(logits, labels, pad_token_id)\n",
    "            batch_size = input_ids.size(0)\n",
    "            total_test_accuracy += batch_accuracy * batch_size\n",
    "            total_test_samples += batch_size\n",
    "\n",
    "    test_pbar.close()\n",
    "    final_test_accuracy = total_test_accuracy / total_test_samples if total_test_samples > 0 else 0.0\n",
    "    print(f\"\\nFinal Test Sequence Accuracy: {final_test_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\n[INFO] Script finished successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Optional: Add argument parsing (argparse) here for flexibility\n",
    "    # Optional: Set random seeds for reproducibility\n",
    "    # SEED = 42\n",
    "    # torch.manual_seed(SEED)\n",
    "    # np.random.seed(SEED)\n",
    "    # import random; random.seed(SEED)\n",
    "    # if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3: Transformer Models for Symbolic Regression\n",
    "Model: Transformer model with a contemporary innovation added such as KAN layers, reinforcement learning, genetic algorithms, specialized long-sequence attention, etc. which improves the performance compared to a basic transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting Reformer Fine-tuning Script for Symbolic Regression...\n",
      "[INFO] Current date/time (UTC): 2025-04-08 17:37:32 UTC\n",
      "[INFO] Using model: google/reformer-enwik8 with Tokenizer: google/reformer-enwik8\n",
      "[INFO] Loading data from: qed_expressions_train.jsonl\n",
      "[INFO] Successfully loaded 12441 records.\n",
      "[INFO] Loading data from: qed_expressions_val.jsonl\n",
      "[INFO] Successfully loaded 1555 records.\n",
      "[INFO] Loading data from: qed_expressions_test.jsonl\n",
      "[INFO] Successfully loaded 1556 records.\n",
      "[INFO] Converted 12441 items to strings (skipped 0).\n",
      "[INFO] Converted 1555 items to strings (skipped 0).\n",
      "[INFO] Converted 1556 items to strings (skipped 0).\n",
      "[INFO] Initializing Tokenizer: google/reformer-enwik8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[FATAL] Failed to initialize tokenizer 'google/reformer-enwik8': \n",
      "ReformerTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikitas/anaconda3/envs/torch_cpu_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Fine-tuning Script for a Reformer Encoder-Decoder Model on Symbolic Regression Data.\n",
    "\n",
    "This script fine-tunes a Reformer model (`google/reformer-enwik8`) configured\n",
    "as an encoder-decoder for a sequence-to-sequence task, likely symbolic regression,\n",
    "using preprocessed data. Reformer employs Locality-Sensitive Hashing (LSH)\n",
    "attention for computational and memory efficiency, particularly suitable for\n",
    "longer sequences.\n",
    "\n",
    "Key features and techniques utilized:\n",
    "- Reformer Model: Uses LSH attention and other efficiency techniques.\n",
    "- Encoder-Decoder Architecture: Standard setup for sequence-to-sequence tasks.\n",
    "- Gradient Checkpointing: Reduces GPU memory usage during training.\n",
    "- Mixed Precision Training (FP16): Accelerates training and saves memory on\n",
    "  compatible GPUs.\n",
    "- Label Smoothing: Regularizes the model to prevent overconfidence.\n",
    "- Beam Search Generation: Used during evaluation for potentially better outputs.\n",
    "- Exact Match Accuracy: Evaluation metric comparing decoded generated sequences\n",
    "  against reference sequences.\n",
    "- Hugging Face Trainer API: Leverages `Seq2SeqTrainer` for streamlined training\n",
    "  and evaluation.\n",
    "\n",
    "Workflow:\n",
    "1. Load pre-split training, validation, and test datasets from JSONL files.\n",
    "2. Reconstruct source and target strings from token lists.\n",
    "3. Initialize the Reformer tokenizer and build an Encoder-Decoder model from\n",
    "   pre-trained Reformer weights.\n",
    "4. Configure the model (special tokens, tie weights, gradient checkpointing).\n",
    "5. Define a function to tokenize source/target string pairs efficiently.\n",
    "6. Wrap tokenized data into PyTorch Dataset objects.\n",
    "7. Instantiate `DataCollatorForSeq2Seq` for dynamic padding and label handling.\n",
    "8. Define the `compute_metrics` function for sequence accuracy calculation.\n",
    "9. Configure `Seq2SeqTrainingArguments` with hyperparameters and advanced features.\n",
    "10. Initialize and run the `Seq2SeqTrainer`.\n",
    "11. Evaluate the final model on the test set.\n",
    "\n",
    "Note on Tokenizer Choice: The script uses `google/reformer-enwik8` tokenizer,\n",
    "which is character-level based. Ensure this aligns with the tokenization used\n",
    "in your preprocessed `data_*.jsonl` files. If your data uses subword tokens\n",
    "(like from BERT or T5), using this character-level tokenizer might lead to\n",
    "suboptimal results or errors. Adjust `TOKENIZER_ID` and potentially the\n",
    "`MODEL_ID` if necessary.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    ReformerTokenizer,\n",
    "    ReformerModel,            # Base Reformer model (used for encoder/decoder components)\n",
    "    EncoderDecoderModel,      # Wrapper to combine encoder and decoder\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainingArguments       # Explicit import\n",
    ")\n",
    "from tqdm.auto import tqdm # Optional, if adding manual loops or progress bars later\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# File Paths\n",
    "TRAIN_FILE = Path('qed_expressions_train.jsonl') # Assumes qed_expressions prefix from preprocessing\n",
    "VAL_FILE   = Path('qed_expressions_val.jsonl')\n",
    "TEST_FILE  = Path('qed_expressions_test.jsonl')\n",
    "OUTPUT_DIR = Path('reformer_symbolic_regression_output')\n",
    "\n",
    "# Model Configuration\n",
    "# Warning: google/reformer-enwik8 is character-level. Verify compatibility with your data tokens.\n",
    "MODEL_ID = \"google/reformer-enwik8\" # Pre-trained Reformer model identifier\n",
    "TOKENIZER_ID = \"google/reformer-enwik8\" # Typically same as model ID\n",
    "\n",
    "# Tokenizer and Data Processing Configuration\n",
    "MAX_SEQ_LENGTH = 256 # Maximum sequence length (Reformer can handle longer, adjust if needed)\n",
    "\n",
    "# Training Hyperparameters\n",
    "NUM_TRAIN_EPOCHS = 5\n",
    "# Reformer can be memory intensive; adjust batch size carefully.\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 8\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 3e-4          # Starting point for fine-tuning\n",
    "WEIGHT_DECAY = 0.01           # Weight decay regularization\n",
    "WARMUP_STEPS = 300            # Linear LR warmup steps\n",
    "LOGGING_STEPS = 50            # Log metrics frequency\n",
    "EVALUATION_STRATEGY = \"epoch\"\n",
    "SAVE_STRATEGY = \"epoch\"\n",
    "\n",
    "# Advanced Training Feature Flags/Values\n",
    "USE_FP16 = torch.cuda.is_available() # Enable mixed precision if CUDA available\n",
    "USE_GRADIENT_CHECKPOINTING = True    # Enable gradient checkpointing\n",
    "LABEL_SMOOTHING_FACTOR = 0.1         # Apply label smoothing (0.0 disables)\n",
    "\n",
    "# Generation Configuration (for evaluation)\n",
    "GENERATION_NUM_BEAMS = 4             # Beam search width\n",
    "\n",
    "# --- Helper Functions and Classes ---\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads data from a JSON Lines file, handling basic errors.\"\"\"\n",
    "    data = []\n",
    "    file_path = Path(file_path)\n",
    "    print(f\"[INFO] Loading data from: {file_path}\")\n",
    "    if not file_path.is_file():\n",
    "        print(f\"[Error] Data file not found: {file_path}\", file=sys.stderr)\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"[Warning] Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "                        continue\n",
    "        print(f\"[INFO] Successfully loaded {len(data)} records.\")\n",
    "        if not data:\n",
    "             print(f\"[Warning] No valid records loaded from {file_path}.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load data from {file_path}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "\n",
    "def convert_tokens_to_strings(raw_data_list):\n",
    "    \"\"\"Converts lists of tokens from loaded data into source and target strings.\"\"\"\n",
    "    source_strings = []\n",
    "    target_strings = []\n",
    "    if not raw_data_list:\n",
    "        return source_strings, target_strings\n",
    "\n",
    "    skipped_count = 0\n",
    "    for i, item in enumerate(raw_data_list):\n",
    "        input_toks = item.get('input_tokens')\n",
    "        target_toks = item.get('target_tokens')\n",
    "\n",
    "        if isinstance(input_toks, list) and isinstance(target_toks, list):\n",
    "            source_strings.append(\" \".join(map(str, input_toks)))\n",
    "            target_strings.append(\" \".join(map(str, target_toks)))\n",
    "        else:\n",
    "            print(f\"[Warning] Skipping item at index {i} due to missing/invalid keys or non-list values: {item}\")\n",
    "            skipped_count += 1\n",
    "\n",
    "    print(f\"[INFO] Converted {len(source_strings)} items to strings (skipped {skipped_count}).\")\n",
    "    if not source_strings:\n",
    "        print(\"[Warning] No items successfully converted to strings.\")\n",
    "    return source_strings, target_strings\n",
    "\n",
    "\n",
    "def encode_sequences(tokenizer, source_texts, target_texts, max_len):\n",
    "    \"\"\"\n",
    "    Tokenizes source and target text sequences using the provided tokenizer.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Initialized Hugging Face tokenizer instance.\n",
    "        source_texts (list[str]): List of source sequences for the encoder.\n",
    "        target_texts (list[str]): List of target sequences for the decoder labels.\n",
    "        max_len (int): Maximum sequence length for padding and truncation.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing lists of 'input_ids', 'attention_mask', and 'labels'.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Encoding sequence pairs with max_length={max_len}...\")\n",
    "\n",
    "    # Tokenize source texts (encoder input)\n",
    "    encoder_inputs = tokenizer(\n",
    "        source_texts,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',   # Pad to max_len\n",
    "        truncation=True,        # Truncate sequences longer than max_len\n",
    "        return_tensors=None     # Return lists\n",
    "    )\n",
    "\n",
    "    # Tokenize target texts to create labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        decoder_labels = tokenizer(\n",
    "            target_texts,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "    # Assign the tokenized target IDs as 'labels'\n",
    "    encoder_inputs['labels'] = decoder_labels['input_ids']\n",
    "\n",
    "    print(f\"[INFO] Encoding complete.\")\n",
    "    # Basic validation\n",
    "    if not encoder_inputs.get('input_ids') or not encoder_inputs.get('labels'):\n",
    "         print(\"[Warning] Encoding resulted in empty input_ids or labels.\")\n",
    "    elif len(encoder_inputs['input_ids']) != len(encoder_inputs['labels']):\n",
    "         print(\"[Warning] Mismatch in length between encoded inputs and labels.\")\n",
    "\n",
    "    return encoder_inputs\n",
    "\n",
    "\n",
    "class SequencePairDataset(Dataset):\n",
    "    \"\"\"Simple PyTorch Dataset for holding tokenized sequence pair data (as lists).\"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encodings (dict): Dictionary from tokenizer, e.g.,\n",
    "                              {'input_ids': [[...], ...], 'attention_mask': [[...], ...], 'labels': [[...], ...]}.\n",
    "                              Values should be lists of lists/token IDs.\n",
    "        \"\"\"\n",
    "        if not isinstance(encodings, dict) or 'input_ids' not in encodings:\n",
    "            raise ValueError(\"Encodings must be a dictionary containing at least 'input_ids'.\")\n",
    "        self.encodings = encodings\n",
    "        try:\n",
    "            self.length = len(encodings['input_ids'])\n",
    "            # Validate other keys have the same length\n",
    "            for key in encodings:\n",
    "                if not isinstance(encodings[key], list) or len(encodings[key]) != self.length:\n",
    "                    raise ValueError(f\"Encoding key '{key}' is not a list or has inconsistent length.\")\n",
    "        except Exception as e:\n",
    "             raise ValueError(f\"Failed to validate encodings: {e}\")\n",
    "\n",
    "        if self.length == 0:\n",
    "            raise ValueError(\"Input encodings are empty.\")\n",
    "        print(f\"[INFO] Created Dataset with {self.length} examples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieves the tokenized data (as lists) for a single sample index.\"\"\"\n",
    "        if not 0 <= idx < self.length:\n",
    "             raise IndexError(f\"Index {idx} out of bounds for dataset of length {self.length}.\")\n",
    "        try:\n",
    "            # Return a dictionary containing the data for the specified index\n",
    "            # DataCollator will handle conversion to tensors.\n",
    "            return {key: self.encodings[key][idx] for key in self.encodings}\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to retrieve item at index {idx}: {e}\", file=sys.stderr)\n",
    "            raise\n",
    "\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Orchestrates the Reformer fine-tuning and evaluation pipeline.\"\"\"\n",
    "    print(\"[INFO] Starting Reformer Fine-tuning Script for Symbolic Regression...\")\n",
    "    try:\n",
    "        current_time_str = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
    "    except Exception:\n",
    "        current_time_str = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC (naive)')\n",
    "    print(f\"[INFO] Current date/time (UTC): {current_time_str}\")\n",
    "    print(f\"[INFO] Using model: {MODEL_ID} with Tokenizer: {TOKENIZER_ID}\")\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    try:\n",
    "        train_raw_data = load_jsonl(TRAIN_FILE)\n",
    "        val_raw_data = load_jsonl(VAL_FILE)\n",
    "        test_raw_data = load_jsonl(TEST_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Critical error during data loading: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not train_raw_data or not val_raw_data or not test_raw_data:\n",
    "        print(\"[FATAL] One or more required datasets are empty or failed to load. Exiting.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 2. Prepare Data (Tokens to Strings) ---\n",
    "    train_sources, train_targets = convert_tokens_to_strings(train_raw_data)\n",
    "    val_sources,   val_targets   = convert_tokens_to_strings(val_raw_data)\n",
    "    test_sources,  test_targets  = convert_tokens_to_strings(test_raw_data)\n",
    "\n",
    "    if not train_sources or not val_sources or not test_sources:\n",
    "        print(\"[FATAL] Data conversion resulted in empty lists. Check input data format. Exiting.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 3. Initialize Tokenizer ---\n",
    "    print(f\"[INFO] Initializing Tokenizer: {TOKENIZER_ID}\")\n",
    "    try:\n",
    "        # ReformerTokenizer requires sentencepiece if not installed\n",
    "        # pip install sentencepiece\n",
    "        tokenizer = ReformerTokenizer.from_pretrained(TOKENIZER_ID)\n",
    "        # Check for necessary special tokens\n",
    "        if tokenizer.pad_token is None: tokenizer.add_special_tokens({'pad_token': '<pad>'}) # Reformer uses <pad>\n",
    "        if tokenizer.cls_token is None: tokenizer.add_special_tokens({'cls_token': '<s>'}) # Common start token\n",
    "        if tokenizer.sep_token is None: tokenizer.add_special_tokens({'sep_token': '</s>'}) # Common end token\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed to initialize tokenizer '{TOKENIZER_ID}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Store for metrics function\n",
    "    global tokenizer_for_metrics\n",
    "    tokenizer_for_metrics = tokenizer\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    print(f\"[INFO] Tokenizer Pad Token ID: {pad_token_id}\")\n",
    "    print(f\"[INFO] Tokenizer Vocab Size (initial): {tokenizer.vocab_size}\")\n",
    "\n",
    "    # --- 4. Tokenize Data ---\n",
    "    try:\n",
    "        train_encodings = encode_sequences(tokenizer, train_sources, train_targets, MAX_SEQ_LENGTH)\n",
    "        val_encodings   = encode_sequences(tokenizer, val_sources,   val_targets,   MAX_SEQ_LENGTH)\n",
    "        test_encodings  = encode_sequences(tokenizer, test_sources,  test_targets,  MAX_SEQ_LENGTH)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed during data tokenization: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Validate encodings\n",
    "    if not train_encodings.get('input_ids') or not val_encodings.get('input_ids') or not test_encodings.get('input_ids'):\n",
    "         print(\"[FATAL] Tokenization resulted in empty encodings for one or more splits. Exiting.\", file=sys.stderr)\n",
    "         sys.exit(1)\n",
    "\n",
    "\n",
    "    # --- 5. Create Datasets ---\n",
    "    try:\n",
    "        train_dataset = SequencePairDataset(train_encodings)\n",
    "        val_dataset   = SequencePairDataset(val_encodings)\n",
    "        test_dataset  = SequencePairDataset(test_encodings)\n",
    "    except ValueError as e:\n",
    "        print(f\"[FATAL] Failed to create Dataset objects: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 6. Initialize Model ---\n",
    "    print(f\"[INFO] Initializing Reformer Encoder-Decoder Model from: {MODEL_ID}\")\n",
    "    try:\n",
    "        # Create encoder-decoder model from Reformer base model\n",
    "        # This loads the Reformer weights into both encoder and decoder components.\n",
    "        model = EncoderDecoderModel.from_encoder_decoder_pretrained(MODEL_ID, MODEL_ID)\n",
    "\n",
    "        # Resize embeddings if tokenizer vocab size changed due to added special tokens\n",
    "        if model.config.encoder.vocab_size != len(tokenizer):\n",
    "             print(f\"[INFO] Resizing model embeddings from {model.config.encoder.vocab_size} to {len(tokenizer)}\")\n",
    "             model.resize_token_embeddings(len(tokenizer))\n",
    "             # Ensure config reflects this if needed later, although resize_token_embeddings usually updates it.\n",
    "             model.config.encoder.vocab_size = len(tokenizer)\n",
    "             model.config.decoder.vocab_size = len(tokenizer)\n",
    "\n",
    "        # Configure model for sequence-to-sequence tasks\n",
    "        model.config.decoder_start_token_id = tokenizer.cls_token_id # Use CLS as start token\n",
    "        model.config.eos_token_id = tokenizer.sep_token_id           # Use SEP as end token\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id           # Use PAD for padding\n",
    "\n",
    "        # Important for Reformer: Set decoder's is_decoder flag and add cross-attention\n",
    "        model.config.decoder.is_decoder = True\n",
    "        model.config.decoder.add_cross_attention = True\n",
    "\n",
    "        # Tie weights between encoder and decoder (embeddings and potentially output projection)\n",
    "        # Reduces parameter count and often improves performance for shared vocabularies.\n",
    "        print(\"[INFO] Tying encoder and decoder weights.\")\n",
    "        model.tie_weights() # Ties input/output embeddings usually\n",
    "\n",
    "        # Set generation parameters\n",
    "        model.config.max_length = MAX_SEQ_LENGTH\n",
    "        model.config.early_stopping = True # Optional: For generation\n",
    "        model.config.num_beams = GENERATION_NUM_BEAMS # Default beam size for generation\n",
    "\n",
    "        # Enable Gradient Checkpointing if configured\n",
    "        if USE_GRADIENT_CHECKPOINTING:\n",
    "            model.gradient_checkpointing_enable()\n",
    "            print(\"[INFO] Gradient Checkpointing enabled on the model.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed to initialize or configure the Reformer model: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 7. Initialize Data Collator ---\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model, # Pass model for automatic decoder_input_ids creation\n",
    "        label_pad_token_id=-100, # Ignore padding in loss calculation\n",
    "        pad_to_multiple_of=8 if USE_FP16 else None # Pad efficiently for FP16\n",
    "    )\n",
    "    print(\"[INFO] Data collator initialized.\")\n",
    "\n",
    "    # --- 8. Define Metrics Computation ---\n",
    "    def compute_metrics_fn(eval_pred):\n",
    "        \"\"\"Calculates exact sequence match accuracy after decoding.\"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        if not isinstance(predictions, np.ndarray): predictions = np.array(predictions)\n",
    "        if not isinstance(labels, np.ndarray): labels = np.array(labels)\n",
    "\n",
    "        # Replace -100 (ignore index) with pad_token_id for decoding\n",
    "        labels = np.where(labels != -100, labels, tokenizer_for_metrics.pad_token_id)\n",
    "\n",
    "        try:\n",
    "            decoded_preds = tokenizer_for_metrics.batch_decode(predictions, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer_for_metrics.batch_decode(labels, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "             print(f\"[Error] Decoding failed in compute_metrics: {e}\", file=sys.stderr)\n",
    "             return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "        # Post-process: strip whitespace\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "        if len(decoded_preds) != len(decoded_labels):\n",
    "            print(f\"[Warning] Mismatch prediction/label count: {len(decoded_preds)} vs {len(decoded_labels)}\", file=sys.stderr)\n",
    "            return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "        matches = [pred == label for pred, label in zip(decoded_preds, decoded_labels)]\n",
    "        accuracy = np.mean(matches) if matches else 0.0\n",
    "        return {\"sequence_accuracy\": float(accuracy)}\n",
    "\n",
    "    # --- 9. Define Training Arguments ---\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR),\n",
    "        # Training schedule\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        # Logging, saving, evaluation\n",
    "        logging_dir=str(OUTPUT_DIR / 'logs'),\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        evaluation_strategy=EVALUATION_STRATEGY,\n",
    "        save_strategy=SAVE_STRATEGY,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"sequence_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        # Generation configuration\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=MAX_SEQ_LENGTH,\n",
    "        generation_num_beams=GENERATION_NUM_BEAMS,\n",
    "        # Advanced features\n",
    "        fp16=USE_FP16,\n",
    "        label_smoothing_factor=LABEL_SMOOTHING_FACTOR,\n",
    "        # Pass gradient checkpointing flag based on earlier successful enablement\n",
    "        # Note: Reformer might have internal GC controls too; check docs if issues arise.\n",
    "        gradient_checkpointing=(USE_GRADIENT_CHECKPOINTING and hasattr(model, 'is_gradient_checkpointing') and model.is_gradient_checkpointing),\n",
    "        # report_to=\"tensorboard\", # Optional\n",
    "    )\n",
    "    print(\"[INFO] Training arguments defined.\")\n",
    "    print(f\"[INFO] Effective Mixed Precision (FP16): {'Enabled' if USE_FP16 else 'Disabled'}\")\n",
    "    print(f\"[INFO] Effective Label Smoothing Factor: {LABEL_SMOOTHING_FACTOR}\")\n",
    "    print(f\"[INFO] Effective Gradient Checkpointing in Args: {training_args.gradient_checkpointing}\")\n",
    "\n",
    "\n",
    "    # --- 10. Initialize Trainer ---\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "    )\n",
    "    print(\"[INFO] Seq2SeqTrainer initialized.\")\n",
    "\n",
    "    # --- 11. Train the Model ---\n",
    "    print(f\"[INFO] Starting model training for {NUM_TRAIN_EPOCHS} epochs...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        # Log/save final metrics and state\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(\"[INFO] Training finished successfully.\")\n",
    "        if trainer.state.best_model_checkpoint:\n",
    "             print(f\"[INFO] Best model checkpoint saved at: {trainer.state.best_model_checkpoint}\")\n",
    "        else:\n",
    "             print(\"[Warning] No best model checkpoint recorded.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Training loop encountered an error: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 12. Evaluate on Test Set ---\n",
    "    print(\"[INFO] Evaluating final model on the test set...\")\n",
    "    try:\n",
    "        test_results = trainer.evaluate(\n",
    "            eval_dataset=test_dataset,\n",
    "            metric_key_prefix=\"test\"\n",
    "        )\n",
    "        trainer.log_metrics(\"test\", test_results)\n",
    "        trainer.save_metrics(\"test\", test_results)\n",
    "\n",
    "        if 'test_sequence_accuracy' in test_results:\n",
    "             print(f\"\\n--- Test Set Results ---\")\n",
    "             print(f\"Test Sequence Accuracy: {test_results['test_sequence_accuracy']:.4f}\")\n",
    "             print(f\"------------------------\")\n",
    "        else:\n",
    "             print(\"[Warning] 'test_sequence_accuracy' not found in test results.\")\n",
    "             print(\"Full test results:\", test_results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Evaluation on test set failed: {e}\", file=sys.stderr)\n",
    "        sys.exit(1) # Exit with error status\n",
    "\n",
    "    print(\"[INFO] Script finished successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Optional: Add command-line argument parsing (argparse)\n",
    "    # Optional: Set random seeds for reproducibility\n",
    "    # SEED = 42\n",
    "    # torch.manual_seed(SEED); np.random.seed(SEED); import random; random.seed(SEED)\n",
    "    # if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4: Titans for squared amplitude calculation\n",
    "Model: One of the core architectures from Google’s paper introducing Titans concept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed to import TitanTokenizer or TitanForConditionalGeneration.\n",
      "Please ensure these classes are defined in your transformers library or environment.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "End-to-end Fine-tuning Script for a Hypothetical \"Titan\" Sequence-to-Sequence Model\n",
    "on the Preprocessed QED Squared Amplitude Calculation Dataset.\n",
    "\n",
    "This script demonstrates fine-tuning a sequence-to-sequence model, referred to\n",
    "here as \"Titan\" (e.g., `google/titan-small`), presumably incorporating unique\n",
    "architectural features like time-mixing and recursion modules as specified in\n",
    "the configuration. It utilizes the Hugging Face Transformers library, including\n",
    "the `Seq2SeqTrainer` API.\n",
    "\n",
    "Key features demonstrated:\n",
    "- Hypothetical Titan Model: Assumes existence of `TitanTokenizer` and\n",
    "  `TitanForConditionalGeneration` with specific config flags (`use_time_mixing`,\n",
    "  `use_recursion`).\n",
    "- Advanced Training Techniques: Includes Gradient Checkpointing, Mixed Precision\n",
    "  Training (FP16), Label Smoothing, and Beam Search during evaluation.\n",
    "- Standard Workflow: Follows a common pattern of data loading, preprocessing,\n",
    "  tokenization, model configuration, training, and evaluation.\n",
    "- Exact Match Accuracy: Uses sequence-level exact match as the evaluation metric.\n",
    "\n",
    "Workflow:\n",
    "1. Load pre-split data (train, validation, test) from JSONL files.\n",
    "2. Reconstruct source (input amplitude) and target (squared amplitude) strings.\n",
    "3. Initialize the Titan tokenizer and model.\n",
    "4. Configure Titan-specific features (time-mixing, recursion) and enable\n",
    "   gradient checkpointing on the model instance.\n",
    "5. Define a function to tokenize source/target string pairs.\n",
    "6. Create PyTorch Dataset objects from the tokenized data.\n",
    "7. Instantiate `DataCollatorForSeq2Seq` for dynamic batching.\n",
    "8. Define the `compute_metrics` function for evaluation using decoded sequences.\n",
    "9. Configure `Seq2SeqTrainingArguments` with hyperparameters and features.\n",
    "10. Initialize and run the `Seq2SeqTrainer`.\n",
    "11. Evaluate the final model on the test set.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "# --- Hypothetical Titan Model Imports ---\n",
    "# Ensure these classes exist in your transformers installation or define placeholders\n",
    "try:\n",
    "    from transformers import (\n",
    "        TitanTokenizer,                # Hypothetical Tokenizer\n",
    "        TitanForConditionalGeneration, # Hypothetical Model\n",
    "        DataCollatorForSeq2Seq,\n",
    "        Seq2SeqTrainer,\n",
    "        Seq2SeqTrainingArguments,\n",
    "        TrainingArguments           # Explicit import\n",
    "    )\n",
    "    print(\"[INFO] Successfully imported hypothetical Titan classes.\")\n",
    "except ImportError:\n",
    "    print(\"[ERROR] Failed to import TitanTokenizer or TitanForConditionalGeneration.\", file=sys.stderr)\n",
    "    print(\"Please ensure these classes are defined in your transformers library or environment.\", file=sys.stderr)\n",
    "    # Define dummy classes if needed for script structure analysis:\n",
    "    # class TitanTokenizer: @staticmethod def from_pretrained(s): raise NotImplementedError\n",
    "    # class TitanForConditionalGeneration: @staticmethod def from_pretrained(s): raise NotImplementedError\n",
    "    # from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments, TrainingArguments\n",
    "    sys.exit(1) # Exit if the core classes are missing\n",
    "\n",
    "from tqdm.auto import tqdm # Optional progress bars\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# File Paths\n",
    "TRAIN_FILE = Path('qed_expressions_train.jsonl') # Use consistent naming if applicable\n",
    "VAL_FILE   = Path('qed_expressions_val.jsonl')\n",
    "TEST_FILE  = Path('qed_expressions_test.jsonl')\n",
    "OUTPUT_DIR = Path('titan_qed_squared_amplitude_output')\n",
    "\n",
    "# Model Configuration\n",
    "# Replace with actual model ID if 'google/titan-small' is a placeholder\n",
    "MODEL_ID = \"google/titan-small\" # Hypothetical Titan model identifier\n",
    "TOKENIZER_ID = \"google/titan-small\" # Usually same as model ID\n",
    "\n",
    "# Titan-Specific Configuration (Hypothetical)\n",
    "USE_TIME_MIXING = True\n",
    "USE_RECURSION   = True\n",
    "\n",
    "# Tokenizer and Data Processing Configuration\n",
    "MAX_SEQ_LENGTH = 512 # Max sequence length\n",
    "\n",
    "# Training Hyperparameters\n",
    "NUM_TRAIN_EPOCHS = 4 # Adjust based on convergence\n",
    "# Adjust batch size based on Titan model size and GPU memory\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 4\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_STEPS = 200\n",
    "LOGGING_STEPS = 50\n",
    "EVALUATION_STRATEGY = \"epoch\"\n",
    "SAVE_STRATEGY = \"epoch\"\n",
    "\n",
    "# Advanced Training Feature Flags/Values\n",
    "USE_FP16 = torch.cuda.is_available()\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "LABEL_SMOOTHING_FACTOR = 0.1\n",
    "\n",
    "# Generation Configuration (for evaluation)\n",
    "GENERATION_NUM_BEAMS = 4\n",
    "\n",
    "# --- Helper Functions and Classes ---\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads data from a JSON Lines file, handling basic errors.\"\"\"\n",
    "    data = []\n",
    "    file_path = Path(file_path)\n",
    "    print(f\"[INFO] Loading data from: {file_path}\")\n",
    "    if not file_path.is_file():\n",
    "        print(f\"[Error] Data file not found: {file_path}\", file=sys.stderr)\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"[Warning] Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "                        continue\n",
    "        print(f\"[INFO] Successfully loaded {len(data)} records.\")\n",
    "        if not data:\n",
    "             print(f\"[Warning] No valid records loaded from {file_path}.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load data from {file_path}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "\n",
    "def convert_tokens_to_strings(raw_data_list):\n",
    "    \"\"\"Converts lists of tokens from loaded data into source and target strings.\"\"\"\n",
    "    source_strings = []\n",
    "    target_strings = []\n",
    "    if not raw_data_list:\n",
    "        return source_strings, target_strings\n",
    "\n",
    "    skipped_count = 0\n",
    "    for i, item in enumerate(raw_data_list):\n",
    "        input_toks = item.get('input_tokens')\n",
    "        target_toks = item.get('target_tokens')\n",
    "\n",
    "        if isinstance(input_toks, list) and isinstance(target_toks, list):\n",
    "            source_strings.append(\" \".join(map(str, input_toks)))\n",
    "            target_strings.append(\" \".join(map(str, target_toks)))\n",
    "        else:\n",
    "            print(f\"[Warning] Skipping item at index {i} due to missing/invalid keys or non-list values: {item}\")\n",
    "            skipped_count += 1\n",
    "\n",
    "    print(f\"[INFO] Converted {len(source_strings)} items to strings (skipped {skipped_count}).\")\n",
    "    if not source_strings:\n",
    "        print(\"[Warning] No items successfully converted to strings.\")\n",
    "    return source_strings, target_strings\n",
    "\n",
    "\n",
    "def encode_sequences(tokenizer, source_texts, target_texts, max_len):\n",
    "    \"\"\"\n",
    "    Tokenizes source and target text sequences using the provided tokenizer.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Initialized Hugging Face tokenizer instance (e.g., TitanTokenizer).\n",
    "        source_texts (list[str]): List of source sequences.\n",
    "        target_texts (list[str]): List of target sequences for labels.\n",
    "        max_len (int): Maximum sequence length for padding and truncation.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing lists of 'input_ids', 'attention_mask', and 'labels'.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Encoding sequence pairs with max_length={max_len}...\")\n",
    "\n",
    "    # Tokenize source texts\n",
    "    encoder_inputs = tokenizer(\n",
    "        source_texts,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=None # Return lists\n",
    "    )\n",
    "\n",
    "    # Tokenize target texts to create labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        decoder_labels = tokenizer(\n",
    "            target_texts,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "    # Assign target input_ids as 'labels'\n",
    "    encoder_inputs['labels'] = decoder_labels['input_ids']\n",
    "\n",
    "    print(f\"[INFO] Encoding complete.\")\n",
    "    # Basic validation\n",
    "    if not encoder_inputs.get('input_ids') or not encoder_inputs.get('labels'):\n",
    "         print(\"[Warning] Encoding resulted in empty input_ids or labels.\")\n",
    "    elif len(encoder_inputs['input_ids']) != len(encoder_inputs['labels']):\n",
    "         print(\"[Warning] Mismatch in length between encoded inputs and labels.\")\n",
    "\n",
    "    return encoder_inputs\n",
    "\n",
    "\n",
    "class SequencePairDataset(Dataset):\n",
    "    \"\"\"Simple PyTorch Dataset for holding tokenized sequence pair data (as lists).\"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encodings (dict): Dictionary from tokenizer {'input_ids': [...], ...}.\n",
    "                              Values should be lists of token IDs or masks.\n",
    "        \"\"\"\n",
    "        if not isinstance(encodings, dict) or 'input_ids' not in encodings:\n",
    "            raise ValueError(\"Encodings must be a dictionary containing at least 'input_ids'.\")\n",
    "        self.encodings = encodings\n",
    "        try:\n",
    "            self.length = len(encodings['input_ids'])\n",
    "            for key in encodings:\n",
    "                if not isinstance(encodings[key], list) or len(encodings[key]) != self.length:\n",
    "                    raise ValueError(f\"Encoding key '{key}' is not a list or has inconsistent length.\")\n",
    "        except Exception as e:\n",
    "             raise ValueError(f\"Failed to validate encodings: {e}\")\n",
    "\n",
    "        if self.length == 0:\n",
    "            raise ValueError(\"Input encodings are empty.\")\n",
    "        print(f\"[INFO] Created Dataset with {self.length} examples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieves the tokenized data (as lists) for a single sample index.\"\"\"\n",
    "        if not 0 <= idx < self.length:\n",
    "             raise IndexError(f\"Index {idx} out of bounds for dataset of length {self.length}.\")\n",
    "        try:\n",
    "            return {key: self.encodings[key][idx] for key in self.encodings}\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to retrieve item at index {idx}: {e}\", file=sys.stderr)\n",
    "            raise\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Orchestrates the Titan model fine-tuning and evaluation pipeline.\"\"\"\n",
    "    print(\"[INFO] Starting Hypothetical Titan Model Fine-tuning Script...\")\n",
    "    try:\n",
    "        current_time_str = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
    "    except Exception:\n",
    "        current_time_str = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC (naive)')\n",
    "    print(f\"[INFO] Current time: {current_time_str} (local: San Diego, CA)\") # Include location context\n",
    "    print(f\"[INFO] Using model: {MODEL_ID} with Tokenizer: {TOKENIZER_ID}\")\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    try:\n",
    "        train_raw_data = load_jsonl(TRAIN_FILE)\n",
    "        val_raw_data = load_jsonl(VAL_FILE)\n",
    "        test_raw_data = load_jsonl(TEST_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Critical error during data loading: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not train_raw_data or not val_raw_data or not test_raw_data:\n",
    "        print(\"[FATAL] One or more required datasets are empty or failed to load. Exiting.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 2. Prepare Data (Tokens to Strings) ---\n",
    "    train_sources, train_targets = convert_tokens_to_strings(train_raw_data)\n",
    "    val_sources,   val_targets   = convert_tokens_to_strings(val_raw_data)\n",
    "    test_sources,  test_targets  = convert_tokens_to_strings(test_raw_data)\n",
    "\n",
    "    if not train_sources or not val_sources or not test_sources:\n",
    "        print(\"[FATAL] Data conversion resulted in empty lists. Check input data format. Exiting.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 3. Initialize Tokenizer ---\n",
    "    print(f\"[INFO] Initializing Tokenizer: {TOKENIZER_ID}\")\n",
    "    try:\n",
    "        # Assuming TitanTokenizer exists and works like standard tokenizers\n",
    "        tokenizer = TitanTokenizer.from_pretrained(TOKENIZER_ID)\n",
    "        # Add checks/additions for special tokens if necessary for Titan\n",
    "        if tokenizer.pad_token is None: tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "        # Determine start/end tokens based on tokenizer properties or model requirements\n",
    "        if tokenizer.bos_token is None: tokenizer.add_special_tokens({'bos_token': '<s>'})\n",
    "        if tokenizer.eos_token is None: tokenizer.add_special_tokens({'eos_token': '</s>'})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed to initialize tokenizer '{TOKENIZER_ID}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Store globally for metric computation\n",
    "    global tokenizer_for_metrics\n",
    "    tokenizer_for_metrics = tokenizer\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    print(f\"[INFO] Tokenizer Pad Token ID: {pad_token_id}\")\n",
    "    print(f\"[INFO] Tokenizer Vocab Size (initial): {tokenizer.vocab_size}\")\n",
    "\n",
    "\n",
    "    # --- 4. Initialize Model ---\n",
    "    print(f\"[INFO] Initializing Model: {MODEL_ID}\")\n",
    "    try:\n",
    "        model = TitanForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "\n",
    "        # Resize embeddings if vocab size changed\n",
    "        if model.config.vocab_size != len(tokenizer):\n",
    "             print(f\"[INFO] Resizing model embeddings from {model.config.vocab_size} to {len(tokenizer)}\")\n",
    "             model.resize_token_embeddings(len(tokenizer))\n",
    "             model.config.vocab_size = len(tokenizer) # Ensure config is updated\n",
    "\n",
    "        # Configure hypothetical Titan features - use try/except for robustness\n",
    "        config_updated = False\n",
    "        try:\n",
    "            if USE_TIME_MIXING and hasattr(model.config, 'use_time_mixing'):\n",
    "                model.config.use_time_mixing = True\n",
    "                print(\"[INFO] Enabled model.config.use_time_mixing\")\n",
    "                config_updated = True\n",
    "            elif USE_TIME_MIXING:\n",
    "                 print(\"[Warning] Configured USE_TIME_MIXING=True, but 'use_time_mixing' not found in model config.\")\n",
    "\n",
    "            if USE_RECURSION and hasattr(model.config, 'use_recursion'):\n",
    "                model.config.use_recursion = True\n",
    "                print(\"[INFO] Enabled model.config.use_recursion\")\n",
    "                config_updated = True\n",
    "            elif USE_RECURSION:\n",
    "                 print(\"[Warning] Configured USE_RECURSION=True, but 'use_recursion' not found in model config.\")\n",
    "\n",
    "        except Exception as config_e:\n",
    "             print(f\"[Warning] Error applying hypothetical Titan config options: {config_e}\")\n",
    "\n",
    "        # Configure standard seq2seq settings\n",
    "        model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        # Set generation defaults\n",
    "        model.config.max_length = MAX_SEQ_LENGTH\n",
    "        model.config.num_beams = GENERATION_NUM_BEAMS\n",
    "\n",
    "        # Enable Gradient Checkpointing if configured\n",
    "        if USE_GRADIENT_CHECKPOINTING:\n",
    "            try:\n",
    "                 model.gradient_checkpointing_enable()\n",
    "                 print(\"[INFO] Gradient Checkpointing enabled on the model.\")\n",
    "            except Exception as gc_e:\n",
    "                 print(f\"[Warning] Failed to enable gradient checkpointing on model: {gc_e}\")\n",
    "                 # Ensure training arg reflects this failure\n",
    "                 global USE_GRADIENT_CHECKPOINTING_EFFECTIVE\n",
    "                 USE_GRADIENT_CHECKPOINTING_EFFECTIVE = False\n",
    "        else:\n",
    "            USE_GRADIENT_CHECKPOINTING_EFFECTIVE = False\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed to initialize or configure the Titan model '{MODEL_ID}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 5. Tokenize Data ---\n",
    "    try:\n",
    "        train_encodings = encode_sequences(tokenizer, train_sources, train_targets, MAX_SEQ_LENGTH)\n",
    "        val_encodings   = encode_sequences(tokenizer, val_sources,   val_targets,   MAX_SEQ_LENGTH)\n",
    "        test_encodings  = encode_sequences(tokenizer, test_sources,  test_targets,  MAX_SEQ_LENGTH)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed during data tokenization: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not train_encodings.get('input_ids') or not val_encodings.get('input_ids') or not test_encodings.get('input_ids'):\n",
    "         print(\"[FATAL] Tokenization resulted in empty encodings for one or more splits. Exiting.\", file=sys.stderr)\n",
    "         sys.exit(1)\n",
    "\n",
    "    # --- 6. Create Datasets ---\n",
    "    try:\n",
    "        train_dataset = SequencePairDataset(train_encodings)\n",
    "        val_dataset   = SequencePairDataset(val_encodings)\n",
    "        test_dataset  = SequencePairDataset(test_encodings)\n",
    "    except ValueError as e:\n",
    "        print(f\"[FATAL] Failed to create Dataset objects: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 7. Initialize Data Collator ---\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100, # Ignore pad tokens in loss\n",
    "        pad_to_multiple_of=8 if USE_FP16 else None # Optimize padding for FP16\n",
    "    )\n",
    "    print(\"[INFO] Data collator initialized.\")\n",
    "\n",
    "    # --- 8. Define Metrics Computation ---\n",
    "    def compute_metrics_fn(eval_pred):\n",
    "        \"\"\"Calculates exact sequence match accuracy after decoding.\"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        if not isinstance(predictions, np.ndarray): predictions = np.array(predictions)\n",
    "        if not isinstance(labels, np.ndarray): labels = np.array(labels)\n",
    "\n",
    "        # Replace -100 with pad_token_id for decoding\n",
    "        labels = np.where(labels != -100, labels, tokenizer_for_metrics.pad_token_id)\n",
    "\n",
    "        try:\n",
    "            decoded_preds = tokenizer_for_metrics.batch_decode(predictions, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer_for_metrics.batch_decode(labels, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "             print(f\"[Error] Decoding failed in compute_metrics: {e}\", file=sys.stderr)\n",
    "             return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "        # Post-process and compare\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "        if len(decoded_preds) != len(decoded_labels):\n",
    "            print(f\"[Warning] Mismatch prediction/label count: {len(decoded_preds)} vs {len(decoded_labels)}\", file=sys.stderr)\n",
    "            return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "        matches = [pred == label for pred, label in zip(decoded_preds, decoded_labels)]\n",
    "        accuracy = np.mean(matches) if matches else 0.0\n",
    "        return {\"sequence_accuracy\": float(accuracy)}\n",
    "\n",
    "    # --- 9. Define Training Arguments ---\n",
    "    # Use effective GC flag based on earlier attempt\n",
    "    effective_gc = USE_GRADIENT_CHECKPOINTING if 'USE_GRADIENT_CHECKPOINTING_EFFECTIVE' not in globals() else USE_GRADIENT_CHECKPOINTING_EFFECTIVE\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR),\n",
    "        # Schedule\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        # Logging / Saving / Evaluation\n",
    "        logging_dir=str(OUTPUT_DIR / 'logs'),\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        evaluation_strategy=EVALUATION_STRATEGY,\n",
    "        save_strategy=SAVE_STRATEGY,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"sequence_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        # Generation\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=MAX_SEQ_LENGTH,\n",
    "        generation_num_beams=GENERATION_NUM_BEAMS,\n",
    "        # Advanced features\n",
    "        fp16=USE_FP16,\n",
    "        label_smoothing_factor=LABEL_SMOOTHING_FACTOR,\n",
    "        gradient_checkpointing=effective_gc, # Use the effective flag\n",
    "        # report_to=\"tensorboard\", # Optional\n",
    "    )\n",
    "    print(\"[INFO] Training arguments defined.\")\n",
    "    print(f\"[INFO] Effective Mixed Precision (FP16): {'Enabled' if USE_FP16 else 'Disabled'}\")\n",
    "    print(f\"[INFO] Effective Label Smoothing Factor: {LABEL_SMOOTHING_FACTOR}\")\n",
    "    print(f\"[INFO] Effective Gradient Checkpointing in Args: {training_args.gradient_checkpointing}\")\n",
    "\n",
    "\n",
    "    # --- 10. Initialize Trainer ---\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "    )\n",
    "    print(\"[INFO] Seq2SeqTrainer initialized.\")\n",
    "\n",
    "    # --- 11. Train the Model ---\n",
    "    print(f\"[INFO] Starting model training for {NUM_TRAIN_EPOCHS} epochs...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(\"[INFO] Training finished successfully.\")\n",
    "        if trainer.state.best_model_checkpoint:\n",
    "             print(f\"[INFO] Best model checkpoint saved at: {trainer.state.best_model_checkpoint}\")\n",
    "        else:\n",
    "             print(\"[Warning] No best model checkpoint recorded.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Training loop encountered an error: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 12. Evaluate on Test Set ---\n",
    "    print(\"[INFO] Evaluating final model on the test set...\")\n",
    "    try:\n",
    "        test_results = trainer.evaluate(\n",
    "            eval_dataset=test_dataset,\n",
    "            metric_key_prefix=\"test\"\n",
    "        )\n",
    "        trainer.log_metrics(\"test\", test_results)\n",
    "        trainer.save_metrics(\"test\", test_results)\n",
    "\n",
    "        if 'test_sequence_accuracy' in test_results:\n",
    "             print(f\"\\n--- Test Set Results ---\")\n",
    "             print(f\"Test Sequence Accuracy: {test_results['test_sequence_accuracy']:.4f}\")\n",
    "             print(f\"------------------------\")\n",
    "        else:\n",
    "             print(\"[Warning] 'test_sequence_accuracy' not found in test results.\")\n",
    "             print(\"Full test results:\", test_results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Evaluation on test set failed: {e}\", file=sys.stderr)\n",
    "        sys.exit(1) # Exit with error status\n",
    "\n",
    "    print(\"[INFO] Script finished successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Optional: Add argument parsing (argparse)\n",
    "    # Optional: Set random seeds\n",
    "    # SEED = 42; torch.manual_seed(SEED); np.random.seed(SEED); import random; random.seed(SEED)\n",
    "    # if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5: Evolutionary and Transformer Models for Symbolic Regression\n",
    "Model: Transformer model integrated with an evolutionary pipeline. It’s possible to start from previous year’s projects but should introduce a substantial innovation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 56\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     49\u001b[0m     AutoTokenizer,\n\u001b[1;32m     50\u001b[0m     EncoderDecoderModel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     TrainingArguments \u001b[38;5;66;03m# Explicit import\u001b[39;00m\n\u001b[1;32m     55\u001b[0m )\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m base, creator, tools, algorithms\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;66;03m# Optional, useful if adding manual loops or progress bars later\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# --- Configuration ---\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# File Paths\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Ensure these point to your actual preprocessed data files\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deap'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Evolutionary Hyperparameter Optimization for Transformer-Based Symbolic Regression.\n",
    "\n",
    "This script employs a Genetic Algorithm (GA), using the DEAP library, to optimize\n",
    "key hyperparameters (learning rate and dropout probability) for a standard\n",
    "Transformer Encoder-Decoder model (BERT-to-BERT) applied to a symbolic regression\n",
    "task using the QED 2→2 dataset.\n",
    "\n",
    "The optimization process works as follows:\n",
    "1.  A small, fixed subset of the training and validation data is selected for\n",
    "    rapid evaluation during the GA.\n",
    "2.  DEAP initializes a population of candidate hyperparameter sets (individuals).\n",
    "3.  Each individual is evaluated by:\n",
    "    a. Initializing a fresh BERT-to-BERT `EncoderDecoderModel`.\n",
    "    b. Configuring the model with the dropout specified by the individual.\n",
    "    c. Setting up `Seq2SeqTrainingArguments` with the learning rate from the\n",
    "       individual and settings suitable for a short training run.\n",
    "    d. Initializing a `Seq2SeqTrainer`.\n",
    "    e. Training the model for a fixed, small number of steps/epochs on the\n",
    "       training subset.\n",
    "    f. Evaluating the trained model on the validation subset using sequence\n",
    "       accuracy.\n",
    "    g. Returning the validation accuracy as the individual's fitness.\n",
    "4.  DEAP applies genetic operators (selection, crossover, mutation) to evolve\n",
    "    the population over several generations, aiming to maximize fitness\n",
    "    (validation accuracy).\n",
    "5.  After the GA completes, the best hyperparameter set found is identified.\n",
    "6.  A final `EncoderDecoderModel` is initialized.\n",
    "7.  A full training run is performed using the best hyperparameters found by the\n",
    "    GA, training on the complete training dataset and validating on the complete\n",
    "    validation dataset.\n",
    "8.  The final performance is reported based on evaluation on the held-out test set.\n",
    "\n",
    "Note: This script assumes the `deap` library is installed (`pip install deap`).\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    EncoderDecoderModel,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainingArguments # Explicit import\n",
    ")\n",
    "from deap import base, creator, tools, algorithms\n",
    "from tqdm.auto import tqdm # Optional, useful if adding manual loops or progress bars later\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# File Paths\n",
    "# Ensure these point to your actual preprocessed data files\n",
    "TRAIN_FILE = Path('qed_expressions_train.jsonl') # Adjust prefix if needed\n",
    "VAL_FILE   = Path('qed_expressions_val.jsonl')\n",
    "TEST_FILE  = Path('qed_expressions_test.jsonl')\n",
    "OUTPUT_DIR = Path('ga_transformer_symbolic_regression_output') # Main output directory\n",
    "GA_LOG_FILE = OUTPUT_DIR / \"ga_logbook.json\"                   # Log of GA progress\n",
    "BEST_HP_FILE = OUTPUT_DIR / \"best_hyperparameters.json\"         # Best HPs found\n",
    "GA_EVAL_OUTPUT_DIR = OUTPUT_DIR / \"ga_eval_runs\" # Temp dir for GA evaluation runs\n",
    "\n",
    "# Model and Tokenizer Configuration\n",
    "MODEL_ID = \"bert-base-uncased\" # Base model for Encoder-Decoder\n",
    "TOKENIZER_ID = \"bert-base-uncased\"\n",
    "MAX_SEQ_LENGTH = 128          # Max sequence length for tokenization\n",
    "\n",
    "# Genetic Algorithm Configuration (DEAP)\n",
    "POPULATION_SIZE = 20        # Number of HP sets evaluated per generation\n",
    "N_GENERATIONS = 10          # Total number of GA generations\n",
    "CX_PROB = 0.7               # Probability of crossover between individuals\n",
    "MUT_PROB = 0.2              # Probability of mutating an individual\n",
    "# Mutation parameters for Gaussian perturbation\n",
    "MUT_SIGMA_LR = 1e-5         # Std deviation for learning rate mutation\n",
    "MUT_SIGMA_DROPOUT = 0.05    # Std deviation for dropout mutation\n",
    "# Boundaries for hyperparameter values\n",
    "LR_BOUND_LOW = 1e-6\n",
    "LR_BOUND_HIGH = 1e-3\n",
    "DROPOUT_BOUND_LOW = 0.0\n",
    "DROPOUT_BOUND_HIGH = 0.5\n",
    "TOURNAMENT_SIZE = 3         # Selection pressure for tournament selection\n",
    "\n",
    "# GA Evaluation Run Configuration (Short runs on subsets for speed)\n",
    "GA_EVAL_TRAIN_SUBSET_SIZE = 512 # Number of training examples per GA eval run\n",
    "GA_EVAL_VAL_SUBSET_SIZE = 128   # Number of validation examples per GA eval run\n",
    "GA_EVAL_EPOCHS = 1              # Epochs per GA eval run (keep very small)\n",
    "# GA_EVAL_MAX_STEPS = 200       # Alternative: Fixed steps per GA eval run\n",
    "GA_EVAL_BATCH_SIZE = 16         # Batch size for GA eval runs\n",
    "\n",
    "# Final Training Configuration (using best HPs on full data)\n",
    "FINAL_TRAIN_EPOCHS = 5\n",
    "FINAL_BATCH_SIZE = 16\n",
    "FINAL_WEIGHT_DECAY = 0.01\n",
    "FINAL_WARMUP_STEPS = 300\n",
    "FINAL_LOGGING_STEPS = 50\n",
    "FINAL_USE_FP16 = torch.cuda.is_available() # Use FP16 if available for final run\n",
    "FINAL_LABEL_SMOOTHING = 0.1     # Optional label smoothing for final run\n",
    "FINAL_SAVE_TOTAL_LIMIT = 2      # Keep only the best and latest checkpoints\n",
    "\n",
    "# --- Helper Functions and Classes ---\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads data from a JSON Lines file, handling basic errors.\"\"\"\n",
    "    data = []\n",
    "    file_path = Path(file_path)\n",
    "    print(f\"[INFO] Loading data from: {file_path}\")\n",
    "    if not file_path.is_file():\n",
    "        print(f\"[Error] Data file not found: {file_path}\", file=sys.stderr)\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try: data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e: print(f\"[Warning] Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "        print(f\"[INFO] Successfully loaded {len(data)} records.\")\n",
    "        if not data: print(f\"[Warning] No valid records loaded from {file_path}.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load data from {file_path}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "def convert_tokens_to_strings(raw_data_list):\n",
    "    \"\"\"Converts lists of tokens from loaded data into source and target strings.\"\"\"\n",
    "    source_strings, target_strings, skipped_count = [], [], 0\n",
    "    if not raw_data_list: return source_strings, target_strings\n",
    "    for i, item in enumerate(raw_data_list):\n",
    "        input_toks, target_toks = item.get('input_tokens'), item.get('target_tokens')\n",
    "        if isinstance(input_toks, list) and isinstance(target_toks, list):\n",
    "            source_strings.append(\" \".join(map(str, input_toks)))\n",
    "            target_strings.append(\" \".join(map(str, target_toks)))\n",
    "        else:\n",
    "            print(f\"[Warning] Skipping item at index {i} due to missing/invalid data: {item}\"); skipped_count += 1\n",
    "    print(f\"[INFO] Converted {len(source_strings)} items to strings (skipped {skipped_count}).\")\n",
    "    if not source_strings: print(\"[Warning] No items successfully converted.\")\n",
    "    return source_strings, target_strings\n",
    "\n",
    "def encode_sequences(tokenizer, source_texts, target_texts, max_len):\n",
    "    \"\"\"Tokenizes source and target sequences, returning lists of IDs/masks.\"\"\"\n",
    "    print(f\"[INFO] Encoding sequence pairs with max_length={max_len}...\")\n",
    "    encoder_inputs = tokenizer(source_texts, max_length=max_len, padding='max_length', truncation=True, return_tensors=None)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        decoder_labels = tokenizer(target_texts, max_length=max_len, padding='max_length', truncation=True, return_tensors=None)\n",
    "    encoder_inputs['labels'] = decoder_labels['input_ids']\n",
    "    print(f\"[INFO] Encoding complete.\")\n",
    "    if not encoder_inputs.get('input_ids') or not encoder_inputs.get('labels') or len(encoder_inputs['input_ids']) != len(encoder_inputs['labels']):\n",
    "         print(\"[Warning] Encoding resulted in empty lists or length mismatch.\")\n",
    "    return encoder_inputs\n",
    "\n",
    "class SequencePairDataset(Dataset):\n",
    "    \"\"\"Simple PyTorch Dataset for holding tokenized sequence pair data (as lists).\"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        if not isinstance(encodings, dict) or 'input_ids' not in encodings: raise ValueError(\"Invalid encodings format.\")\n",
    "        self.encodings = encodings\n",
    "        try:\n",
    "            self.length = len(encodings['input_ids'])\n",
    "            for key in encodings:\n",
    "                 if not isinstance(encodings[key], list) or len(encodings[key]) != self.length: raise ValueError(f\"Inconsistent length for key '{key}'.\")\n",
    "        except Exception as e: raise ValueError(f\"Validation failed: {e}\")\n",
    "        if self.length == 0: raise ValueError(\"Input encodings are empty.\")\n",
    "        print(f\"[INFO] Created Dataset with {self.length} examples.\")\n",
    "\n",
    "    def __len__(self): return self.length\n",
    "    def __getitem__(self, idx):\n",
    "        if not 0 <= idx < self.length: raise IndexError(f\"Index {idx} out of bounds.\")\n",
    "        try: return {key: self.encodings[key][idx] for key in self.encodings}\n",
    "        except Exception as e: print(f\"[Error] Failed retrieval at index {idx}: {e}\", file=sys.stderr); raise\n",
    "\n",
    "# --- Global variables needed by evaluation function ---\n",
    "# These are populated in main() before the GA starts\n",
    "ga_train_dataset_subset = None\n",
    "ga_val_dataset_subset = None\n",
    "tokenizer_for_eval = None\n",
    "data_collator_for_eval = None\n",
    "compute_metrics_internal = None # Holds the metric calculation logic\n",
    "\n",
    "# --- DEAP Evaluation Function ---\n",
    "\n",
    "def evaluate_hyperparams(individual):\n",
    "    \"\"\"\n",
    "    Evaluates a hyperparameter set [learning_rate, dropout_rate] by training\n",
    "    a small model on data subsets and returning validation accuracy.\n",
    "\n",
    "    Args:\n",
    "        individual (deap.creator.Individual): List containing [LR, Dropout].\n",
    "\n",
    "    Returns:\n",
    "        tuple: Fitness value (validation_accuracy,). Must be a tuple.\n",
    "    \"\"\"\n",
    "    learning_rate, dropout_rate = individual[0], individual[1]\n",
    "    dropout_rate = max(DROPOUT_BOUND_LOW, min(DROPOUT_BOUND_HIGH, dropout_rate)) # Clamp dropout\n",
    "\n",
    "    run_id = f\"lr_{learning_rate:.1e}_drop_{dropout_rate:.3f}_{random.randint(1000,9999)}\"\n",
    "    run_output_dir = GA_EVAL_OUTPUT_DIR / run_id\n",
    "\n",
    "    print(f\"[GA Eval] Evaluating LR={learning_rate:.3e}, Dropout={dropout_rate:.4f}\")\n",
    "\n",
    "    try:\n",
    "        # 1. Initialize NEW model instance\n",
    "        model = EncoderDecoderModel.from_encoder_decoder_pretrained(MODEL_ID, MODEL_ID)\n",
    "        if model.config.encoder.vocab_size != len(tokenizer_for_eval):\n",
    "            model.resize_token_embeddings(len(tokenizer_for_eval))\n",
    "            model.config.encoder.vocab_size = len(tokenizer_for_eval)\n",
    "            model.config.decoder.vocab_size = len(tokenizer_for_eval)\n",
    "\n",
    "        # 2. Apply dropout from individual\n",
    "        model.config.dropout = dropout_rate\n",
    "        model.config.attention_dropout = dropout_rate\n",
    "        if hasattr(model.config, 'encoder'): model.config.encoder.dropout = dropout_rate; model.config.encoder.attention_dropout = dropout_rate\n",
    "        if hasattr(model.config, 'decoder'): model.config.decoder.dropout = dropout_rate; model.config.decoder.attention_dropout = dropout_rate\n",
    "\n",
    "        # 3. Configure standard seq2seq settings\n",
    "        model.config.decoder_start_token_id = tokenizer_for_eval.cls_token_id\n",
    "        model.config.eos_token_id = tokenizer_for_eval.sep_token_id\n",
    "        model.config.pad_token_id = tokenizer_for_eval.pad_token_id\n",
    "        model.config.max_length = MAX_SEQ_LENGTH\n",
    "\n",
    "        # 4. Minimal Training Arguments for GA eval run\n",
    "        eval_training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=str(run_output_dir),\n",
    "            num_train_epochs=GA_EVAL_EPOCHS,\n",
    "            # max_steps=GA_EVAL_MAX_STEPS, # Alternative\n",
    "            per_device_train_batch_size=GA_EVAL_BATCH_SIZE,\n",
    "            per_device_eval_batch_size=GA_EVAL_BATCH_SIZE,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=0.0, # Keep simple for GA eval\n",
    "            logging_steps=1000, # Reduce logging noise\n",
    "            evaluation_strategy=\"no\",\n",
    "            save_strategy=\"no\",\n",
    "            predict_with_generate=True, # Use generation for accuracy\n",
    "            generation_max_length=MAX_SEQ_LENGTH,\n",
    "            generation_num_beams=1, # Greedy search for speed\n",
    "            fp16=FINAL_USE_FP16, # Consistent FP16 usage\n",
    "            report_to=\"none\",\n",
    "            disable_tqdm=True,\n",
    "        )\n",
    "\n",
    "        # 5. Initialize Trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=eval_training_args,\n",
    "            train_dataset=ga_train_dataset_subset,\n",
    "            eval_dataset=ga_val_dataset_subset,\n",
    "            data_collator=data_collator_for_eval,\n",
    "            tokenizer=tokenizer_for_eval,\n",
    "            compute_metrics=compute_metrics_internal # Use the predefined metric logic\n",
    "        )\n",
    "\n",
    "        # 6. Train briefly\n",
    "        trainer.train()\n",
    "\n",
    "        # 7. Evaluate\n",
    "        eval_results = trainer.evaluate(eval_dataset=ga_val_dataset_subset)\n",
    "        accuracy = eval_results.get(\"eval_sequence_accuracy\", 0.0)\n",
    "\n",
    "        print(f\"[GA Eval Result] LR={learning_rate:.3e}, Dropout={dropout_rate:.4f} -> Val Acc={accuracy:.4f}\")\n",
    "        # Optional: Clean up temp dir: import shutil; shutil.rmtree(run_output_dir)\n",
    "\n",
    "        return (accuracy,) # Return fitness tuple\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[GA Eval Error] Failed for LR={learning_rate:.3e}, Dropout={dropout_rate:.4f}: {e}\", file=sys.stderr)\n",
    "        return (0.0,) # Return poor fitness on failure\n",
    "\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Orchestrates data loading, GA optimization, and final model training.\"\"\"\n",
    "    print(\"[INFO] Starting Evolutionary Hyperparameter Optimization Script...\")\n",
    "    # --- ***** Update Timestamp/Location ***** ---\n",
    "    try:\n",
    "        # Attempt to get timezone-aware UTC time\n",
    "        current_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "        current_time_str = current_time.strftime('%Y-%m-%d %H:%M:%S %Z') # Includes timezone name\n",
    "    except Exception:\n",
    "        # Fallback to naive UTC time\n",
    "        current_time = datetime.datetime.utcnow()\n",
    "        current_time_str = current_time.strftime('%Y-%m-%d %H:%M:%S UTC (naive)')\n",
    "    # Incorporate provided context\n",
    "    print(f\"[INFO] Current time: {current_time_str}\")\n",
    "    print(f\"[INFO] Location context: San Diego, CA, USA\")\n",
    "    print(f\"[INFO] Using Base Model: {MODEL_ID}\")\n",
    "\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    GA_EVAL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    try:\n",
    "        train_raw = load_jsonl(TRAIN_FILE)\n",
    "        val_raw = load_jsonl(VAL_FILE)\n",
    "        test_raw = load_jsonl(TEST_FILE)\n",
    "    except Exception as e: print(f\"[FATAL] Data loading failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "    if not train_raw or not val_raw or not test_raw: print(\"[FATAL] Datasets empty after loading.\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 2. Prepare Data Strings ---\n",
    "    train_src, train_tgt = convert_tokens_to_strings(train_raw)\n",
    "    val_src,   val_tgt   = convert_tokens_to_strings(val_raw)\n",
    "    test_src,  test_tgt  = convert_tokens_to_strings(test_raw)\n",
    "    if not train_src or not val_src or not test_src: print(\"[FATAL] Data conversion failed.\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 3. Initialize Tokenizer (Global for GA Eval) ---\n",
    "    global tokenizer_for_eval\n",
    "    try:\n",
    "        tokenizer_for_eval = AutoTokenizer.from_pretrained(TOKENIZER_ID)\n",
    "        if tokenizer_for_eval.pad_token is None: tokenizer_for_eval.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        if tokenizer_for_eval.cls_token is None: tokenizer_for_eval.add_special_tokens({'cls_token': '[CLS]'})\n",
    "        if tokenizer_for_eval.sep_token is None: tokenizer_for_eval.add_special_tokens({'sep_token': '[SEP]'})\n",
    "        pad_token_id = tokenizer_for_eval.pad_token_id\n",
    "    except Exception as e: print(f\"[FATAL] Tokenizer init failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "    print(f\"[INFO] Tokenizer initialized (Vocab: {len(tokenizer_for_eval)}, Pad ID: {pad_token_id}).\")\n",
    "\n",
    "    # --- 4. Tokenize Data & Create Subsets ---\n",
    "    try:\n",
    "        print(\"[INFO] Tokenizing full datasets...\")\n",
    "        train_enc = encode_sequences(tokenizer_for_eval, train_src, train_tgt, MAX_SEQ_LENGTH)\n",
    "        val_enc   = encode_sequences(tokenizer_for_eval, val_src,   val_tgt,   MAX_SEQ_LENGTH)\n",
    "        test_enc  = encode_sequences(tokenizer_for_eval, test_src,  test_tgt,  MAX_SEQ_LENGTH)\n",
    "\n",
    "        full_train_dataset = SequencePairDataset(train_enc)\n",
    "        full_val_dataset   = SequencePairDataset(val_enc)\n",
    "        full_test_dataset  = SequencePairDataset(test_enc)\n",
    "\n",
    "        print(f\"[INFO] Creating GA evaluation subsets (Train: {GA_EVAL_TRAIN_SUBSET_SIZE}, Val: {GA_EVAL_VAL_SUBSET_SIZE})\")\n",
    "        train_subset_indices = random.sample(range(len(full_train_dataset)), min(GA_EVAL_TRAIN_SUBSET_SIZE, len(full_train_dataset)))\n",
    "        val_subset_indices = random.sample(range(len(full_val_dataset)), min(GA_EVAL_VAL_SUBSET_SIZE, len(full_val_dataset)))\n",
    "\n",
    "        def subset_encodings(enc, indices): return {key: [enc[key][i] for i in indices] for key in enc}\n",
    "\n",
    "        global ga_train_dataset_subset, ga_val_dataset_subset\n",
    "        ga_train_dataset_subset = SequencePairDataset(subset_encodings(train_enc, train_subset_indices))\n",
    "        ga_val_dataset_subset = SequencePairDataset(subset_encodings(val_enc, val_subset_indices))\n",
    "\n",
    "    except Exception as e: print(f\"[FATAL] Data tokenization or subsetting failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 5. Setup Global Collator & Metrics Logic ---\n",
    "    global data_collator_for_eval, compute_metrics_internal\n",
    "    temp_model = EncoderDecoderModel.from_encoder_decoder_pretrained(MODEL_ID, MODEL_ID) # Need instance for collator setup\n",
    "    data_collator_for_eval = DataCollatorForSeq2Seq(tokenizer_for_eval, model=temp_model, label_pad_token_id=-100, pad_to_multiple_of=8 if FINAL_USE_FP16 else None)\n",
    "    del temp_model\n",
    "    print(\"[INFO] Data collator prepared.\")\n",
    "\n",
    "    # Define the internal metric calculation logic once\n",
    "    def compute_metrics_logic(eval_pred):\n",
    "        \"\"\"Internal logic for calculating sequence accuracy.\"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        if not isinstance(predictions, np.ndarray): predictions = np.array(predictions)\n",
    "        if not isinstance(labels, np.ndarray): labels = np.array(labels)\n",
    "        labels = np.where(labels != -100, labels, tokenizer_for_eval.pad_token_id)\n",
    "        try:\n",
    "            decoded_preds = tokenizer_for_eval.batch_decode(predictions, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer_for_eval.batch_decode(labels, skip_special_tokens=True)\n",
    "            decoded_preds = [p.strip() for p in decoded_preds]\n",
    "            decoded_labels = [l.strip() for l in decoded_labels]\n",
    "            if len(decoded_preds) != len(decoded_labels): return {\"sequence_accuracy\": 0.0}\n",
    "            matches = [p == l for p, l in zip(decoded_preds, decoded_labels)]\n",
    "            accuracy = np.mean(matches) if matches else 0.0\n",
    "            return {\"sequence_accuracy\": float(accuracy)}\n",
    "        except Exception as dec_e:\n",
    "             print(f\"[Metrics Error] Decoding failed: {dec_e}\", file=sys.stderr); return {\"sequence_accuracy\": 0.0}\n",
    "    compute_metrics_internal = compute_metrics_logic # Assign to global var\n",
    "\n",
    "    # --- 6. DEAP Genetic Algorithm Setup ---\n",
    "    print(\"[INFO] Setting up Genetic Algorithm (DEAP)...\")\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_lr\", random.uniform, LR_BOUND_LOW, LR_BOUND_HIGH)\n",
    "    toolbox.register(\"attr_dropout\", random.uniform, DROPOUT_BOUND_LOW, DROPOUT_BOUND_HIGH)\n",
    "    toolbox.register(\"individual\", tools.initCycle, creator.Individual, (toolbox.attr_lr, toolbox.attr_dropout), n=1)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "    # Ensure mutation respects boundaries - DEAP's mutGaussian doesn't inherently\n",
    "    # We handle clamping within the evaluate function, alternative is custom mutation\n",
    "    toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=[MUT_SIGMA_LR, MUT_SIGMA_DROPOUT], indpb=0.2)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=TOURNAMENT_SIZE)\n",
    "    toolbox.register(\"evaluate\", evaluate_hyperparams)\n",
    "\n",
    "    # --- 7. Run Genetic Algorithm ---\n",
    "    print(f\"[INFO] Starting GA optimization ({N_GENERATIONS} gens, Pop: {POPULATION_SIZE})...\")\n",
    "    population = toolbox.population(n=POPULATION_SIZE)\n",
    "    hall_of_fame = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean); stats.register(\"std\", np.std); stats.register(\"min\", np.min); stats.register(\"max\", np.max)\n",
    "\n",
    "    try:\n",
    "        population, logbook = algorithms.eaSimple(population, toolbox, cxpb=CX_PROB, mutpb=MUT_PROB, ngen=N_GENERATIONS, stats=stats, halloffame=hall_of_fame, verbose=True)\n",
    "        ga_success = True\n",
    "    except Exception as ga_e: print(f\"[ERROR] GA execution failed: {ga_e}\", file=sys.stderr); ga_success = False; logbook = None; best_individual = None\n",
    "\n",
    "    if logbook:\n",
    "        try:\n",
    "            with open(GA_LOG_FILE, 'w') as f: json.dump(logbook, f, indent=2)\n",
    "            print(f\"[INFO] GA logbook saved to {GA_LOG_FILE}\")\n",
    "        except Exception as log_e: print(f\"[Error] Failed to save GA logbook: {log_e}\", file=sys.stderr)\n",
    "\n",
    "    if not ga_success or not hall_of_fame: print(\"[FATAL] GA optimization failed or found no best individual. Exiting.\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    best_individual = hall_of_fame[0]\n",
    "    best_lr, best_dropout = best_individual[0], max(DROPOUT_BOUND_LOW, min(DROPOUT_BOUND_HIGH, best_individual[1])) # Clamp final best dropout\n",
    "    best_fitness = best_individual.fitness.values[0]\n",
    "\n",
    "    print(\"\\n--- GA Optimization Complete ---\")\n",
    "    print(f\"Best Individual: LR={best_lr:.3e}, Dropout={best_dropout:.4f}\")\n",
    "    print(f\"Best Validation Accuracy (on subset): {best_fitness:.4f}\")\n",
    "    print(\"----------------------------------\")\n",
    "\n",
    "    best_hp = {'learning_rate': best_lr, 'dropout_rate': best_dropout, 'validation_accuracy': best_fitness}\n",
    "    try:\n",
    "        with open(BEST_HP_FILE, 'w') as f: json.dump(best_hp, f, indent=2)\n",
    "        print(f\"[INFO] Best hyperparameters saved to {BEST_HP_FILE}\")\n",
    "    except Exception as hp_e: print(f\"[Error] Failed to save best hyperparameters: {hp_e}\", file=sys.stderr)\n",
    "\n",
    "    # --- 8. Final Training with Best Hyperparameters ---\n",
    "    print(\"\\n[INFO] Starting final model training using best hyperparameters found...\")\n",
    "    try:\n",
    "        final_model = EncoderDecoderModel.from_encoder_decoder_pretrained(MODEL_ID, MODEL_ID)\n",
    "        if final_model.config.encoder.vocab_size != len(tokenizer_for_eval):\n",
    "            final_model.resize_token_embeddings(len(tokenizer_for_eval))\n",
    "            final_model.config.encoder.vocab_size = len(tokenizer_for_eval)\n",
    "            final_model.config.decoder.vocab_size = len(tokenizer_for_eval)\n",
    "\n",
    "        final_model.config.dropout = best_dropout\n",
    "        final_model.config.attention_dropout = best_dropout\n",
    "        if hasattr(final_model.config, 'encoder'): final_model.config.encoder.dropout = best_dropout; final_model.config.encoder.attention_dropout = best_dropout\n",
    "        if hasattr(final_model.config, 'decoder'): final_model.config.decoder.dropout = best_dropout; final_model.config.decoder.attention_dropout = best_dropout\n",
    "\n",
    "        final_model.config.decoder_start_token_id = tokenizer_for_eval.cls_token_id\n",
    "        final_model.config.eos_token_id = tokenizer_for_eval.sep_token_id\n",
    "        final_model.config.pad_token_id = tokenizer_for_eval.pad_token_id\n",
    "        final_model.config.max_length = MAX_SEQ_LENGTH\n",
    "        final_model.config.num_beams = 4 # Use beam search for final evaluation\n",
    "        final_model.tie_weights()\n",
    "        # Optional final gradient checkpointing\n",
    "        # if FINAL_USE_GRADIENT_CHECKPOINTING: final_model.gradient_checkpointing_enable()\n",
    "\n",
    "    except Exception as model_e: print(f\"[FATAL] Failed to initialize final model: {model_e}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    final_training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR / \"final_model\"),\n",
    "        num_train_epochs=FINAL_TRAIN_EPOCHS,\n",
    "        per_device_train_batch_size=FINAL_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=FINAL_BATCH_SIZE,\n",
    "        learning_rate=best_lr, # Use best LR\n",
    "        weight_decay=FINAL_WEIGHT_DECAY,\n",
    "        warmup_steps=FINAL_WARMUP_STEPS,\n",
    "        logging_dir=str(OUTPUT_DIR / \"final_model\" / 'logs'),\n",
    "        logging_steps=FINAL_LOGGING_STEPS,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=FINAL_SAVE_TOTAL_LIMIT,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"sequence_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=MAX_SEQ_LENGTH,\n",
    "        generation_num_beams=4,\n",
    "        fp16=FINAL_USE_FP16,\n",
    "        label_smoothing_factor=FINAL_LABEL_SMOOTHING,\n",
    "        # gradient_checkpointing=FINAL_USE_GRADIENT_CHECKPOINTING, # Optional\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    final_trainer = Seq2SeqTrainer(\n",
    "        model=final_model, args=final_training_args,\n",
    "        train_dataset=full_train_dataset, eval_dataset=full_val_dataset,\n",
    "        data_collator=data_collator_for_eval, tokenizer=tokenizer_for_eval,\n",
    "        compute_metrics=compute_metrics_internal,\n",
    "    )\n",
    "\n",
    "    print(f\"[INFO] Starting final training run ({FINAL_TRAIN_EPOCHS} epochs)...\")\n",
    "    try:\n",
    "        train_result = final_trainer.train()\n",
    "        final_trainer.log_metrics(\"final_train\", train_result.metrics)\n",
    "        final_trainer.save_metrics(\"final_train\", train_result.metrics)\n",
    "        final_trainer.save_state()\n",
    "        print(\"[INFO] Final training complete.\")\n",
    "        if final_trainer.state.best_model_checkpoint: print(f\"[INFO] Best final model checkpoint: {final_trainer.state.best_model_checkpoint}\")\n",
    "    except Exception as train_e: print(f\"[FATAL] Final training failed: {train_e}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 9. Final Test Evaluation ---\n",
    "    print(\"[INFO] Evaluating final best model on the full test set...\")\n",
    "    try:\n",
    "        test_results = final_trainer.evaluate(eval_dataset=full_test_dataset, metric_key_prefix=\"test\")\n",
    "        final_trainer.log_metrics(\"test\", test_results)\n",
    "        final_trainer.save_metrics(\"test\", test_results)\n",
    "        if 'test_sequence_accuracy' in test_results:\n",
    "             print(f\"\\n--- Final Test Set Results ---\"); print(f\"Test Sequence Accuracy: {test_results['test_sequence_accuracy']:.4f}\"); print(f\"-----------------------------\")\n",
    "        else: print(\"[Warning] 'test_sequence_accuracy' not found.\", \"Full test results:\", test_results)\n",
    "    except Exception as test_e: print(f\"[Error] Final evaluation failed: {test_e}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    print(\"\\n[INFO] Script finished successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SEED = 42\n",
    "    random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6: Symbolic empirical representation of squared amplitudes in high-energy physics\n",
    "Model: Transformer with novel approach for tokenization, data representation and/or preprocessing that leads to better performance than basic tokenization with normalized indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting Depth-Aware Transformer Script...\n",
      "[INFO] Current time: 2025-04-08 17:47:44 UTC\n",
      "[INFO] Location context: San Diego, CA, USA\n",
      "[INFO] Loading data from: qed_expressions_train.jsonl\n",
      "[INFO] Successfully loaded 12441 records.\n",
      "[INFO] Loading data from: qed_expressions_val.jsonl\n",
      "[INFO] Successfully loaded 1555 records.\n",
      "[INFO] Loading data from: qed_expressions_test.jsonl\n",
      "[INFO] Successfully loaded 1556 records.\n",
      "[INFO] Training new BPE tokenizer (Vocab: 8000, Min Freq: 2)...\n",
      "[INFO] Training from iterator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Error] Tokenizer training failed: tokenizers.trainers.BpeTrainer() got multiple values for keyword argument 'special_tokens'\n",
      "[FATAL] Tokenizer training or loading failed: tokenizers.trainers.BpeTrainer() got multiple values for keyword argument 'special_tokens'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Fine-tuning Script for a Depth-Aware Transformer for Symbolic Regression.\n",
    "\n",
    "This script implements and trains a custom Transformer Encoder-Decoder model\n",
    "designed for symbolic regression tasks, specifically predicting squared amplitudes\n",
    "in High Energy Physics (HEP) from input amplitudes.\n",
    "\n",
    "Key features:\n",
    "- Custom BPE Tokenizer: Trains a Byte-Pair Encoding tokenizer from scratch on the\n",
    "  combined input and target sequences of the dataset, potentially capturing\n",
    "  domain-specific physics identifiers and operators more effectively.\n",
    "- Depth-Aware Embeddings: Calculates the parenthesis nesting depth for each token\n",
    "  in the input sequence and incorporates this information via dedicated depth\n",
    "  embeddings, adding structural awareness to the model's input representation.\n",
    "- Custom Transformer Model: Implements a standard Transformer encoder-decoder\n",
    "  using PyTorch's `nn.TransformerEncoderLayer` and `nn.TransformerDecoderLayer`,\n",
    "  modified to accept the combined token, position, and depth embeddings.\n",
    "- Manual PyTorch Training Loop: Includes standard training, validation, and\n",
    "  testing phases with loss calculation, optimization, metric computation (sequence\n",
    "  accuracy), and basic checkpointing.\n",
    "\n",
    "Workflow:\n",
    "1. Load raw data splits from JSONL files.\n",
    "2. Train a new BPE tokenizer on the corpus (if not already trained and saved)\n",
    "   or load a previously trained tokenizer.\n",
    "3. Define a custom `DepthDataset` that performs tokenization and calculates\n",
    "   parenthesis depth for each input sequence during initialization.\n",
    "4. Define the `DepthTransformer` model architecture using PyTorch's nn.Module.\n",
    "5. Set up DataLoaders, optimizer, loss criterion, and device (GPU/CPU).\n",
    "6. Implement the training loop, iterating through epochs and batches, performing\n",
    "   forward/backward passes, and updating model weights.\n",
    "7. Implement validation loop to monitor performance (sequence accuracy) and save\n",
    "   the best model checkpoint based on validation results.\n",
    "8. Implement the final test loop to evaluate the best model on the held-out\n",
    "   test set.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizerFast # Using RobertaTokenizer for BPE training\n",
    "from tqdm.auto import tqdm # Progress bars\n",
    "# Optional: Learning rate scheduler\n",
    "# from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# File Paths\n",
    "TRAIN_FILE = Path('qed_expressions_train.jsonl') # Adjust if using different names\n",
    "VAL_FILE   = Path('qed_expressions_val.jsonl')\n",
    "TEST_FILE  = Path('qed_expressions_test.jsonl')\n",
    "OUTPUT_DIR = Path('depth_aware_transformer_output')\n",
    "TOKENIZER_SAVE_DIR = OUTPUT_DIR / \"bpe_physics_tokenizer\" # Directory to save/load trained tokenizer\n",
    "CHECKPOINT_NAME = \"depth_transformer_best.pt\" # Name for the best model checkpoint\n",
    "\n",
    "# Tokenizer Configuration\n",
    "VOCAB_SIZE = 8000         # Target vocabulary size for BPE tokenizer\n",
    "MIN_FREQUENCY = 2         # Minimum frequency for tokens in BPE training\n",
    "# Special tokens common for sequence models\n",
    "SPECIAL_TOKENS = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "\n",
    "# Data and Model Configuration\n",
    "MAX_SEQ_LENGTH = 128      # Max sequence length for tokenization and model\n",
    "MAX_DEPTH = 50            # Maximum expected parenthesis nesting depth + buffer\n",
    "\n",
    "# Model Hyperparameters (DepthTransformer)\n",
    "D_MODEL = 512             # Embedding and hidden state dimension\n",
    "N_HEAD = 8                # Number of attention heads\n",
    "NUM_ENCODER_LAYERS = 6    # Number of layers in the Transformer encoder\n",
    "NUM_DECODER_LAYERS = 6    # Number of layers in the Transformer decoder\n",
    "DIM_FEEDFORWARD = 2048    # Dimension of the feed-forward networks\n",
    "DROPOUT = 0.1             # Dropout rate (can be added to model layers)\n",
    "\n",
    "# Training Hyperparameters\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 16           # Adjust based on GPU memory\n",
    "LEARNING_RATE = 5e-4\n",
    "OPTIMIZER_EPS = 1e-8\n",
    "WEIGHT_DECAY = 0.01\n",
    "# LR_SCHEDULER_TYPE = \"linear\" # Optional scheduler type\n",
    "# WARMUP_RATIO = 0.1         # Optional warmup ratio\n",
    "LOG_INTERVAL = 50         # Log training stats every N batches\n",
    "\n",
    "# --- Helper Functions and Classes ---\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads data from a JSON Lines file, handling basic errors.\"\"\"\n",
    "    # (Same implementation as previous script)\n",
    "    data = []\n",
    "    file_path = Path(file_path)\n",
    "    print(f\"[INFO] Loading data from: {file_path}\")\n",
    "    if not file_path.is_file():\n",
    "        print(f\"[Error] Data file not found: {file_path}\", file=sys.stderr)\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try: data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e: print(f\"[Warning] Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "        print(f\"[INFO] Successfully loaded {len(data)} records.\")\n",
    "        if not data: print(f\"[Warning] No valid records loaded from {file_path}.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load data from {file_path}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "\n",
    "def train_or_load_tokenizer(corpus_iterator, save_dir, vocab_size, min_freq, special_tokens):\n",
    "    \"\"\"Trains a new BPE tokenizer or loads it if it already exists.\"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    config_file = save_dir / \"tokenizer_config.json\"\n",
    "\n",
    "    if save_dir.exists() and config_file.exists():\n",
    "        print(f\"[INFO] Loading existing tokenizer from {save_dir}\")\n",
    "        try:\n",
    "            tokenizer = RobertaTokenizerFast.from_pretrained(str(save_dir))\n",
    "            # Verify core special tokens are present\n",
    "            if any(tok not in tokenizer.get_vocab() for tok in ['<s>', '<pad>', '</s>', '<unk>']):\n",
    "                 print(\"[Warning] Loaded tokenizer missing some standard special tokens. Recheck config.\")\n",
    "            return tokenizer\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Failed to load existing tokenizer: {e}. Attempting to retrain.\", file=sys.stderr)\n",
    "\n",
    "    print(f\"[INFO] Training new BPE tokenizer (Vocab: {vocab_size}, Min Freq: {min_freq})...\")\n",
    "    try:\n",
    "        # Initialize a base tokenizer (like Roberta's structure) but don't load weights\n",
    "        # We use RobertaTokenizerFast as it supports training from iterator\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\") # Base structure\n",
    "        print(\"[INFO] Training from iterator...\")\n",
    "        # The train_new_from_iterator method might not exist directly on RobertaTokenizerFast.\n",
    "        # Often, you use the underlying `tokenizers` library for this.\n",
    "        # Let's adapt using the `tokenizers` library approach if direct method fails.\n",
    "\n",
    "        try:\n",
    "             # Attempt direct method if available in specific transformers version\n",
    "             tokenizer.train_new_from_iterator(corpus_iterator, vocab_size=vocab_size, min_frequency=min_freq, special_tokens=special_tokens)\n",
    "        except AttributeError:\n",
    "             print(\"[INFO] `train_new_from_iterator` not found, using `tokenizers` library directly.\")\n",
    "             from tokenizers import ByteLevelBPETokenizer as TokenizersBPETokenizer\n",
    "\n",
    "             # Initialize BPE tokenizer from the `tokenizers` library\n",
    "             tk_lib_tokenizer = TokenizersBPETokenizer()\n",
    "             tk_lib_tokenizer.train_from_iterator(\n",
    "                 corpus_iterator,\n",
    "                 vocab_size=vocab_size,\n",
    "                 min_frequency=min_freq,\n",
    "                 special_tokens=special_tokens\n",
    "             )\n",
    "             # Need to save this intermediate tokenizer and then load it into RobertaTokenizerFast\n",
    "             temp_save_path = save_dir / \"temp_tokenizer_files\"\n",
    "             temp_save_path.mkdir(parents=True, exist_ok=True)\n",
    "             tk_lib_tokenizer.save_model(str(temp_save_path)) # Saves vocab.json and merges.txt\n",
    "\n",
    "             # Load the trained vocab/merges into the desired HF Tokenizer class\n",
    "             tokenizer = RobertaTokenizerFast.from_pretrained(str(temp_save_path), max_len=MAX_SEQ_LENGTH) # Or appropriate max_len\n",
    "             # Clean up temp files\n",
    "             import shutil; shutil.rmtree(temp_save_path)\n",
    "\n",
    "\n",
    "        print(\"[INFO] Tokenizer training complete.\")\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        tokenizer.save_pretrained(str(save_dir))\n",
    "        print(f\"[INFO] Tokenizer saved to {save_dir}\")\n",
    "        return tokenizer\n",
    "    except ImportError as e:\n",
    "         print(f\"[Error] Required library (`tokenizers`) not found for training: {e}. Please install it.\", file=sys.stderr)\n",
    "         raise\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Tokenizer training failed: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "\n",
    "class DepthDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that tokenizes sequences and calculates parenthesis nesting depth.\n",
    "\n",
    "    Tokenization and depth calculation are performed once during initialization.\n",
    "    Stores token IDs, attention masks, depth IDs, and label IDs.\n",
    "\n",
    "    Note: Stores all processed examples in memory. May be unsuitable for\n",
    "          extremely large datasets without modification (e.g., lazy processing\n",
    "          or memory mapping).\n",
    "    \"\"\"\n",
    "    def __init__(self, raw_data, tokenizer, max_len=128, max_depth=50):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_data (list[dict]): List of dictionaries, each with 'input_tokens'\n",
    "                                   and 'target_tokens' keys containing lists of strings.\n",
    "            tokenizer: Initialized Hugging Face tokenizer instance.\n",
    "            max_len (int): Maximum sequence length for padding/truncation.\n",
    "            max_depth (int): Maximum depth value to cap at; also determines depth embedding size.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.max_depth = max_depth\n",
    "        self.examples = [] # List to store processed examples\n",
    "\n",
    "        print(f\"[INFO] Processing dataset for DepthDataset (Max Len: {max_len}, Max Depth: {max_depth})...\")\n",
    "        skipped_count = 0\n",
    "        for i, ex in enumerate(tqdm(raw_data, desc=\"Processing Raw Data\")):\n",
    "            input_tokens = ex.get(\"input_tokens\")\n",
    "            target_tokens = ex.get(\"target_tokens\")\n",
    "\n",
    "            if not isinstance(input_tokens, list) or not isinstance(target_tokens, list):\n",
    "                print(f\"[Warning] Skipping example {i} due to missing or invalid token lists.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Join tokens for BPE tokenizer\n",
    "            src_str = \" \".join(map(str, input_tokens))\n",
    "            tgt_str = \" \".join(map(str, target_tokens))\n",
    "\n",
    "            # Tokenize source and target\n",
    "            # Note: We don't use return_tensors='pt' here; store lists/ints\n",
    "            src_enc = tokenizer(src_str, max_length=max_len, truncation=True, padding=\"max_length\", return_tensors=None)\n",
    "            # Use context manager for target tokenization (good practice)\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                tgt_enc = tokenizer(tgt_str, max_length=max_len, truncation=True, padding=\"max_length\", return_tensors=None)\n",
    "\n",
    "            # --- Calculate Parenthesis Depth ---\n",
    "            # Operates on the *original* input_tokens before BPE tokenization\n",
    "            current_depth = 0\n",
    "            depth_sequence = []\n",
    "            for tok in input_tokens:\n",
    "                # Assign depth *before* potential change for closing parenthesis\n",
    "                if tok == \")\":\n",
    "                    current_depth = max(0, current_depth - 1) # Decrement after assigning for ')'\n",
    "                    depth_sequence.append(min(current_depth, max_depth - 1)) # Cap depth\n",
    "                elif tok == \"(\":\n",
    "                    depth_sequence.append(min(current_depth, max_depth - 1)) # Cap depth\n",
    "                    current_depth += 1 # Increment after assigning for '('\n",
    "                else:\n",
    "                    depth_sequence.append(min(current_depth, max_depth - 1)) # Assign current depth\n",
    "\n",
    "            # --- Align Depth with Tokenized Sequence (Simple Approach) ---\n",
    "            # This is a simplification. A robust alignment would map original token\n",
    "            # positions to BPE token positions. Here, we just pad/truncate the\n",
    "            # depth sequence calculated on original tokens to match the BPE sequence length.\n",
    "            # This might misalign depth for tokens split by BPE.\n",
    "            # TODO: Implement a more robust alignment if needed.\n",
    "            if len(depth_sequence) >= max_len:\n",
    "                aligned_depth_ids = depth_sequence[:max_len]\n",
    "            else:\n",
    "                # Pad with depth 0 (assuming root level)\n",
    "                aligned_depth_ids = depth_sequence + [0] * (max_len - len(depth_sequence))\n",
    "\n",
    "            self.examples.append({\n",
    "                \"input_ids\":      src_enc[\"input_ids\"],\n",
    "                \"attention_mask\": src_enc[\"attention_mask\"],\n",
    "                \"depth_ids\":      aligned_depth_ids,\n",
    "                \"labels\":         tgt_enc[\"input_ids\"], # Target token IDs\n",
    "            })\n",
    "\n",
    "        if skipped_count > 0:\n",
    "            print(f\"[Warning] Skipped {skipped_count} examples during dataset processing.\")\n",
    "        if not self.examples:\n",
    "             raise ValueError(\"Dataset processing resulted in zero valid examples.\")\n",
    "        print(f\"[INFO] DepthDataset processed. Total examples: {len(self.examples)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of processed examples.\"\"\"\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a processed example by index.\n",
    "        Returns data as lists/ints; DataLoader handles tensor conversion.\n",
    "        \"\"\"\n",
    "        if not 0 <= idx < len(self.examples):\n",
    "            raise IndexError(f\"Index {idx} out of bounds.\")\n",
    "        return self.examples[idx]\n",
    "\n",
    "\n",
    "class DepthTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder-Decoder model incorporating token depth embeddings.\n",
    "\n",
    "    Input representation combines token, position, and parenthesis depth embeddings.\n",
    "    Uses standard PyTorch Transformer layers for sequence processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, max_len=128,\n",
    "                 max_depth=50, dropout=0.1, pad_token_id=None):\n",
    "        \"\"\"\n",
    "        Initializes the Depth-Aware Transformer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            d_model (int): Dimension of embeddings and hidden states.\n",
    "            nhead (int): Number of attention heads.\n",
    "            num_encoder_layers (int): Number of layers in the encoder stack.\n",
    "            num_decoder_layers (int): Number of layers in the decoder stack.\n",
    "            dim_feedforward (int): Dimension of the feed-forward networks.\n",
    "            max_len (int): Maximum sequence length for positional embeddings.\n",
    "            max_depth (int): Maximum depth value + 1 (size of depth embedding table).\n",
    "            dropout (float): Dropout rate.\n",
    "            pad_token_id (int): ID of the padding token for embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pad_token_id = pad_token_id if pad_token_id is not None else 0 # Default assumption\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model, padding_idx=self.pad_token_id)\n",
    "        self.positional_embedding = nn.Embedding(max_len, d_model) # Absolute positional\n",
    "        self.depth_embedding = nn.Embedding(max_depth, d_model) # Depth information\n",
    "\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Standard PyTorch Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        # Standard PyTorch Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        # Output Projection Layer\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        print(f\"[INFO] Initialized DepthTransformer:\")\n",
    "        print(f\"  - Vocab Size: {vocab_size}, d_model: {d_model}, Heads: {nhead}\")\n",
    "        print(f\"  - Max Len: {max_len}, Max Depth: {max_depth}\")\n",
    "        print(f\"  - Enc Layers: {num_encoder_layers}, Dec Layers: {num_decoder_layers}\")\n",
    "        print(f\"  - Feedforward Dim: {dim_feedforward}, Dropout: {dropout}\")\n",
    "\n",
    "\n",
    "    def forward(self, src_input_ids, src_attention_mask, src_depth_ids, tgt_input_ids):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the depth-aware Transformer.\n",
    "\n",
    "        Args:\n",
    "            src_input_ids (torch.Tensor): Source sequence token IDs (batch_size, src_seq_len).\n",
    "            src_attention_mask (torch.Tensor): Source sequence attention mask (batch_size, src_seq_len).\n",
    "                                               (1 for real tokens, 0 for padding).\n",
    "            src_depth_ids (torch.Tensor): Source sequence depth IDs (batch_size, src_seq_len).\n",
    "            tgt_input_ids (torch.Tensor): Target sequence token IDs (batch_size, tgt_seq_len),\n",
    "                                          typically shifted right for teacher forcing.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits (batch_size, tgt_seq_len, vocab_size).\n",
    "        \"\"\"\n",
    "        batch_size, src_seq_len = src_input_ids.shape\n",
    "        _, tgt_seq_len = tgt_input_ids.shape\n",
    "\n",
    "        # 1. Source Embeddings (Token + Position + Depth)\n",
    "        # Create position IDs (0 to seq_len-1)\n",
    "        src_pos_ids = torch.arange(src_seq_len, device=src_input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        src_emb = self.token_embedding(src_input_ids)           # (B, S, D)\n",
    "        src_pos = self.positional_embedding(src_pos_ids)       # (B, S, D)\n",
    "        src_dep = self.depth_embedding(src_depth_ids)         # (B, S, D)\n",
    "\n",
    "        src_combined_emb = src_emb + src_pos + src_dep # Combine embeddings\n",
    "        src_combined_emb = self.embedding_dropout(src_combined_emb)\n",
    "\n",
    "        # 2. Encoder\n",
    "        # PyTorch Transformer layers expect masks where True indicates a position *not* to attend to.\n",
    "        # src_attention_mask is (B, S) with 1 for non-pad, 0 for pad. Need (B, S) with True for pad.\n",
    "        src_key_padding_mask = (src_attention_mask == 0) # True where padded\n",
    "\n",
    "        # Pass through encoder stack\n",
    "        memory = self.encoder(src_combined_emb, src_key_padding_mask=src_key_padding_mask) # (B, S, D)\n",
    "\n",
    "        # 3. Target Embeddings (Token + Position) - No depth for target typically\n",
    "        # Target IDs are usually shifted right for teacher forcing (e.g., start with BOS, end before EOS)\n",
    "        tgt_pos_ids = torch.arange(tgt_seq_len, device=tgt_input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        tgt_emb = self.token_embedding(tgt_input_ids)           # (B, T, D)\n",
    "        tgt_pos = self.positional_embedding(tgt_pos_ids)       # (B, T, D)\n",
    "\n",
    "        tgt_combined_emb = tgt_emb + tgt_pos # Combine target embeddings\n",
    "        tgt_combined_emb = self.embedding_dropout(tgt_combined_emb)\n",
    "\n",
    "        # 4. Decoder\n",
    "        # Create target padding mask (True for padded positions)\n",
    "        # Assuming target uses same pad token id\n",
    "        # Need a mask for the target sequence itself\n",
    "        tgt_key_padding_mask = (tgt_input_ids == self.pad_token_id)\n",
    "\n",
    "        # Create causal mask (autoregressive mask) to prevent attending to future tokens\n",
    "        # Shape: (T, T)\n",
    "        tgt_causal_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len, device=tgt_input_ids.device)\n",
    "\n",
    "        # Pass through decoder stack\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=tgt_combined_emb,           # Target sequence embeddings (B, T, D)\n",
    "            memory=memory,                  # Encoder output (B, S, D)\n",
    "            tgt_mask=tgt_causal_mask,       # Causal mask (T, T)\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask, # Target padding mask (B, T)\n",
    "            memory_key_padding_mask=src_key_padding_mask # Source padding mask (B, S)\n",
    "        ) # Output shape: (B, T, D)\n",
    "\n",
    "        # 5. Output Projection\n",
    "        logits = self.output_projection(decoder_output) # (B, T, VocabSize)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "def calculate_sequence_accuracy(logits, labels, pad_token_id):\n",
    "    \"\"\"Calculates exact sequence match accuracy using PyTorch tensors.\"\"\"\n",
    "    if logits.shape[0] == 0: return 0.0\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    non_pad_mask = (labels != pad_token_id)\n",
    "    correct_tokens = (predictions == labels) & non_pad_mask\n",
    "    correct_sequences = (torch.sum(correct_tokens, dim=1) == torch.sum(non_pad_mask, dim=1))\n",
    "    accuracy = torch.mean(correct_sequences.float())\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Orchestrates tokenizer training, data processing, model training, and evaluation.\"\"\"\n",
    "    print(\"[INFO] Starting Depth-Aware Transformer Script...\")\n",
    "    try: current_time_str = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
    "    except Exception: current_time_str = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC (naive)')\n",
    "    # --- ***** Update Timestamp/Location ***** ---\n",
    "    print(f\"[INFO] Current time: {current_time_str}\")\n",
    "    print(f\"[INFO] Location context: San Diego, CA, USA\")\n",
    "\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- 1. Load Raw Data ---\n",
    "    try:\n",
    "        train_raw = load_jsonl(TRAIN_FILE)\n",
    "        val_raw = load_jsonl(VAL_FILE)\n",
    "        test_raw = load_jsonl(TEST_FILE)\n",
    "    except Exception as e: print(f\"[FATAL] Data loading failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "    if not train_raw or not val_raw or not test_raw: print(\"[FATAL] Datasets empty after loading.\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 2. Prepare Corpus and Train/Load Tokenizer ---\n",
    "    def corpus_gen(): # Generator to avoid loading all strings into memory at once\n",
    "        print(\"[INFO] Preparing corpus for tokenizer training...\")\n",
    "        count = 0\n",
    "        for split in (train_raw, val_raw, test_raw):\n",
    "            for ex in split:\n",
    "                if ex.get(\"input_tokens\"): yield \" \".join(map(str, ex[\"input_tokens\"]))\n",
    "                if ex.get(\"target_tokens\"): yield \" \".join(map(str, ex[\"target_tokens\"]))\n",
    "                count += 2\n",
    "        print(f\"[INFO] Corpus generator ready ({count} sequences).\")\n",
    "\n",
    "    try:\n",
    "        tokenizer = train_or_load_tokenizer(\n",
    "            corpus_iterator=corpus_gen(),\n",
    "            save_dir=TOKENIZER_SAVE_DIR,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            min_freq=MIN_FREQUENCY,\n",
    "            special_tokens=SPECIAL_TOKENS\n",
    "        )\n",
    "        vocab_size = len(tokenizer) # Get actual vocab size after training/loading\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "        if pad_token_id is None:\n",
    "             print(\"[Error] Tokenizer loaded without a pad token ID!\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Tokenizer training or loading failed: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    print(f\"[INFO] Tokenizer ready (Vocab size: {vocab_size}, Pad ID: {pad_token_id}).\")\n",
    "\n",
    "\n",
    "    # --- 3. Create Datasets ---\n",
    "    try:\n",
    "        print(\"[INFO] Creating DepthDatasets...\")\n",
    "        train_dataset = DepthDataset(train_raw, tokenizer, MAX_SEQ_LENGTH, MAX_DEPTH)\n",
    "        val_dataset   = DepthDataset(val_raw,   tokenizer, MAX_SEQ_LENGTH, MAX_DEPTH)\n",
    "        test_dataset  = DepthDataset(test_raw,  tokenizer, MAX_SEQ_LENGTH, MAX_DEPTH)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed to create datasets: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    if not train_dataset or not val_dataset or not test_dataset:\n",
    "         print(\"[FATAL] One or more datasets are empty after processing. Exiting.\", file=sys.stderr)\n",
    "         sys.exit(1)\n",
    "\n",
    "\n",
    "    # --- 4. Create DataLoaders ---\n",
    "    # Define a simple collate function to handle batching of list data into tensors\n",
    "    def collate_batch(batch):\n",
    "        elem = batch[0]\n",
    "        collated = {}\n",
    "        for key in elem.keys():\n",
    "            collated[key] = torch.tensor([d[key] for d in batch])\n",
    "        return collated\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch, num_workers=4, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch, num_workers=4, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch, num_workers=4, pin_memory=True)\n",
    "    print(\"[INFO] DataLoaders created.\")\n",
    "\n",
    "\n",
    "    # --- 5. Initialize Model, Optimizer, Criterion ---\n",
    "    print(f\"[INFO] Initializing DepthTransformer model...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        model = DepthTransformer(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=D_MODEL,\n",
    "            nhead=N_HEAD,\n",
    "            num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "            num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "            dim_feedforward=DIM_FEEDFORWARD,\n",
    "            max_len=MAX_SEQ_LENGTH,\n",
    "            max_depth=MAX_DEPTH,\n",
    "            dropout=DROPOUT,\n",
    "            pad_token_id=pad_token_id\n",
    "        ).to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed to initialize model: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        eps=OPTIMIZER_EPS,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id) # Ignore padding in loss\n",
    "    print(\"[INFO] Model, Optimizer, and Criterion initialized.\")\n",
    "\n",
    "    # Optional: Learning Rate Scheduler\n",
    "    # total_steps = len(train_loader) * NUM_EPOCHS\n",
    "    # warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "    # --- 6. Training Loop ---\n",
    "    print(f\"[INFO] Starting training for {NUM_EPOCHS} epochs...\")\n",
    "    best_val_accuracy = -1.0\n",
    "    best_epoch = -1\n",
    "    checkpoint_path = OUTPUT_DIR / CHECKPOINT_NAME\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False)\n",
    "\n",
    "        for step, batch in enumerate(train_pbar):\n",
    "            # Move batch data to the device\n",
    "            src_ids = batch[\"input_ids\"].to(device)\n",
    "            src_mask = batch[\"attention_mask\"].to(device) # Encoder padding mask source\n",
    "            depth_ids = batch[\"depth_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)          # Target sequence (B, T)\n",
    "\n",
    "            # Prepare decoder inputs (shifted right) and targets\n",
    "            # Decoder input: BOS token + sequence (excluding last token)\n",
    "            # Decoder target: sequence (excluding first token) + EOS token (implicitly handled by loss shifting)\n",
    "            # For CrossEntropyLoss, logits (B, T, V) should align with targets (B, T)\n",
    "            tgt_input = labels[:, :-1] # (B, T-1), excludes last token\n",
    "            tgt_output = labels[:, 1:]  # (B, T-1), excludes first token (BOS/CLS)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(src_input_ids=src_ids,\n",
    "                           src_attention_mask=src_mask,\n",
    "                           src_depth_ids=depth_ids,\n",
    "                           tgt_input_ids=tgt_input) # Pass shifted input (B, T-1, V)\n",
    "\n",
    "            # Calculate loss - compare logits with shifted *output* targets\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), # (B*(T-1), V)\n",
    "                             tgt_output.reshape(-1))         # (B*(T-1))\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            # Optional: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            # if scheduler: scheduler.step() # Update LR if scheduler is used\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Log training progress\n",
    "            if (step + 1) % LOG_INTERVAL == 0:\n",
    "                avg_loss = total_train_loss / LOG_INTERVAL\n",
    "                # current_lr = scheduler.get_last_lr()[0] if scheduler else LEARNING_RATE\n",
    "                current_lr = LEARNING_RATE # Simpler if no scheduler\n",
    "                train_pbar.set_postfix({'Avg Loss': f'{avg_loss:.4f}', 'LR': f'{current_lr:.2e}'})\n",
    "                total_train_loss = 0.0 # Reset accumulator\n",
    "\n",
    "        train_pbar.close()\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        total_val_accuracy = 0.0\n",
    "        total_val_samples = 0\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_pbar:\n",
    "                src_ids = batch[\"input_ids\"].to(device)\n",
    "                src_mask = batch[\"attention_mask\"].to(device)\n",
    "                depth_ids = batch[\"depth_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                tgt_input = labels[:, :-1]\n",
    "                tgt_output = labels[:, 1:]\n",
    "\n",
    "                logits = model(src_ids, src_mask, depth_ids, tgt_input)\n",
    "\n",
    "                batch_accuracy = calculate_sequence_accuracy(logits, tgt_output, pad_token_id)\n",
    "                total_val_accuracy += batch_accuracy * src_ids.size(0) # Weighted by batch size\n",
    "                total_val_samples += src_ids.size(0)\n",
    "\n",
    "        val_pbar.close()\n",
    "        epoch_val_accuracy = total_val_accuracy / total_val_samples if total_val_samples > 0 else 0.0\n",
    "        print(f\"Epoch {epoch+1} Validation Sequence Accuracy: {epoch_val_accuracy:.4f}\")\n",
    "\n",
    "        # --- Save Best Model Checkpoint ---\n",
    "        if epoch_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = epoch_val_accuracy\n",
    "            best_epoch = epoch + 1\n",
    "            try:\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"[INFO] New best model saved to {checkpoint_path} (Epoch {best_epoch}, Val Acc: {best_val_accuracy:.4f})\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed to save model checkpoint: {e}\", file=sys.stderr)\n",
    "\n",
    "    print(f\"\\n[INFO] Training complete. Best validation accuracy: {best_val_accuracy:.4f} at epoch {best_epoch}.\")\n",
    "\n",
    "    # --- 7. Final Test Evaluation ---\n",
    "    print(\"\\n--- Final Test Set Evaluation ---\")\n",
    "    if checkpoint_path.exists():\n",
    "        print(f\"[INFO] Loading best model from: {checkpoint_path}\")\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "        except Exception as e:\n",
    "             print(f\"[Error] Failed to load best model checkpoint: {e}. Evaluating with the final state.\", file=sys.stderr)\n",
    "    else:\n",
    "        print(\"[Warning] Best model checkpoint not found. Evaluating with the final model state.\", file=sys.stderr)\n",
    "\n",
    "    model.eval()\n",
    "    total_test_accuracy = 0.0\n",
    "    total_test_samples = 0\n",
    "    test_pbar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_pbar:\n",
    "            src_ids = batch[\"input_ids\"].to(device)\n",
    "            src_mask = batch[\"attention_mask\"].to(device)\n",
    "            depth_ids = batch[\"depth_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            tgt_input = labels[:, :-1]\n",
    "            tgt_output = labels[:, 1:]\n",
    "\n",
    "            logits = model(src_ids, src_mask, depth_ids, tgt_input)\n",
    "\n",
    "            batch_accuracy = calculate_sequence_accuracy(logits, tgt_output, pad_token_id)\n",
    "            total_test_accuracy += batch_accuracy * src_ids.size(0)\n",
    "            total_test_samples += src_ids.size(0)\n",
    "\n",
    "    test_pbar.close()\n",
    "    final_test_accuracy = total_test_accuracy / total_test_samples if total_test_samples > 0 else 0.0\n",
    "    print(f\"\\nFinal Test Sequence Accuracy: {final_test_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\n[INFO] Script finished successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Optional: Seed everything\n",
    "    # SEED = 42\n",
    "    # torch.manual_seed(SEED); np.random.seed(SEED); import random; random.seed(SEED)\n",
    "    # if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7: Foundation models for symbolic regression tasks\n",
    "Model: Novel foundation model for symbolic regression tasks. Should be sufficiently novel beyond the current literature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'T5Block' from 'transformers' (/home/nikitas/anaconda3/envs/torch_cpu_env/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     55\u001b[0m     T5Tokenizer,\n\u001b[1;32m     56\u001b[0m     T5ForConditionalGeneration,\n\u001b[1;32m     57\u001b[0m     T5Config, \u001b[38;5;66;03m# To inspect T5 block structure if needed\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     T5Block, \u001b[38;5;66;03m# To check layer types\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     DataCollatorForSeq2Seq,\n\u001b[1;32m     60\u001b[0m     Seq2SeqTrainer,\n\u001b[1;32m     61\u001b[0m     Seq2SeqTrainingArguments,\n\u001b[1;32m     62\u001b[0m     TrainingArguments \u001b[38;5;66;03m# Explicit import\u001b[39;00m\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;66;03m# Optional progress bars\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# --- Configuration ---\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# File Paths\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'T5Block' from 'transformers' (/home/nikitas/anaconda3/envs/torch_cpu_env/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Fine-tuning Script for a T5 Model with Mixture-of-Experts (MoE) Layers\n",
    "for Symbolic Regression (Squared Amplitude Calculation).\n",
    "\n",
    "This script demonstrates modifying a standard T5 (`t5-small`) model by replacing\n",
    "its Feed-Forward Networks (FFNs) with custom Mixture-of-Experts (MoE) layers.\n",
    "The MoE layers potentially allow different \"expert\" networks within the model to\n",
    "specialize in processing different types of symbolic patterns found in the input\n",
    "sequences (e.g., polynomials, trigonometric functions).\n",
    "\n",
    "The script utilizes the Hugging Face Transformers library, including the\n",
    "`Seq2SeqTrainer` API for streamlined training and evaluation.\n",
    "\n",
    "Key features demonstrated:\n",
    "- T5 Base Model: Uses `t5-small` as the starting point.\n",
    "- Custom MoE Layer: Defines and integrates a `MoEFeedForward` module.\n",
    "- Model Patching: Dynamically replaces the standard FFNs in the loaded T5 model\n",
    "  with the custom MoE layers.\n",
    "- Advanced Training Techniques: Incorporates Gradient Checkpointing, Mixed\n",
    "  Precision Training (FP16), and Label Smoothing.\n",
    "- Standard Workflow: Follows data loading, preprocessing, tokenization, model\n",
    "  configuration, training, and evaluation steps.\n",
    "- Exact Match Accuracy (Logit-based): Evaluates performance using sequence\n",
    "  accuracy calculated directly from model logits (`predict_with_generate=False`).\n",
    "\n",
    "Workflow:\n",
    "1. Load pre-split data from JSONL files.\n",
    "2. Reconstruct source and target strings from token lists.\n",
    "3. Initialize the T5 tokenizer.\n",
    "4. Define the custom `MoEFeedForward` nn.Module.\n",
    "5. Load the base T5 model and patch its encoder/decoder blocks by replacing\n",
    "   standard FFN layers with instances of `MoEFeedForward`.\n",
    "6. Configure the modified model (special tokens, gradient checkpointing).\n",
    "7. Define a function to tokenize source/target string pairs.\n",
    "8. Wrap tokenized data into PyTorch Dataset objects.\n",
    "9. Instantiate `DataCollatorForSeq2Seq`.\n",
    "10. Define the `compute_metrics` function for sequence accuracy from logits.\n",
    "11. Configure `Seq2SeqTrainingArguments`.\n",
    "12. Initialize and run the `Seq2SeqTrainer`.\n",
    "13. Evaluate the final model on the test set.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Config, # To inspect T5 block structure if needed\n",
    "    T5Block, # To check layer types\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainingArguments # Explicit import\n",
    ")\n",
    "from tqdm.auto import tqdm # Optional progress bars\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# File Paths\n",
    "TRAIN_FILE = Path('qed_expressions_train.jsonl') # Adjust if needed\n",
    "VAL_FILE   = Path('qed_expressions_val.jsonl')\n",
    "TEST_FILE  = Path('qed_expressions_test.jsonl')\n",
    "OUTPUT_DIR = Path('t5_moe_symbolic_regression_output') # Output directory\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_ID = \"t5-small\" # Base T5 model identifier\n",
    "TOKENIZER_ID = \"t5-small\"\n",
    "\n",
    "# MoE Configuration\n",
    "NUM_EXPERTS = 4             # Number of experts in each MoE layer\n",
    "# d_ff (intermediate FFN dimension) will be taken from the base T5 config\n",
    "\n",
    "# Tokenizer and Data Processing Configuration\n",
    "MAX_SEQ_LENGTH = 128        # Max sequence length\n",
    "\n",
    "# Training Hyperparameters\n",
    "NUM_TRAIN_EPOCHS = 5\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 8\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_STEPS = 200\n",
    "LOGGING_STEPS = 50\n",
    "EVALUATION_STRATEGY = \"epoch\"\n",
    "SAVE_STRATEGY = \"epoch\"\n",
    "\n",
    "# Advanced Training Feature Flags/Values\n",
    "USE_FP16 = torch.cuda.is_available()\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "LABEL_SMOOTHING_FACTOR = 0.1\n",
    "\n",
    "# --- Helper Functions and Classes ---\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads data from a JSON Lines file, handling basic errors.\"\"\"\n",
    "    data = []\n",
    "    file_path = Path(file_path)\n",
    "    print(f\"[INFO] Loading data from: {file_path}\")\n",
    "    if not file_path.is_file():\n",
    "        print(f\"[Error] Data file not found: {file_path}\", file=sys.stderr)\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    try:\n",
    "        with file_path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try: data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e: print(f\"[Warning] Skipping invalid JSON on line {i+1} in {file_path}: {e}\")\n",
    "        print(f\"[INFO] Successfully loaded {len(data)} records.\")\n",
    "        if not data: print(f\"[Warning] No valid records loaded from {file_path}.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load data from {file_path}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "def convert_tokens_to_strings(raw_data_list):\n",
    "    \"\"\"Converts lists of tokens from loaded data into source and target strings.\"\"\"\n",
    "    source_strings, target_strings, skipped_count = [], [], 0\n",
    "    if not raw_data_list: return source_strings, target_strings\n",
    "    for i, item in enumerate(raw_data_list):\n",
    "        input_toks, target_toks = item.get('input_tokens'), item.get('target_tokens')\n",
    "        if isinstance(input_toks, list) and isinstance(target_toks, list):\n",
    "            source_strings.append(\" \".join(map(str, input_toks)))\n",
    "            target_strings.append(\" \".join(map(str, target_toks)))\n",
    "        else:\n",
    "            print(f\"[Warning] Skipping item at index {i} due to missing/invalid data: {item}\"); skipped_count += 1\n",
    "    print(f\"[INFO] Converted {len(source_strings)} items to strings (skipped {skipped_count}).\")\n",
    "    if not source_strings: print(\"[Warning] No items successfully converted.\")\n",
    "    return source_strings, target_strings\n",
    "\n",
    "def encode_sequences(tokenizer, source_texts, target_texts, max_len):\n",
    "    \"\"\"Tokenizes source and target sequences, returning lists of IDs/masks.\"\"\"\n",
    "    print(f\"[INFO] Encoding sequence pairs with max_length={max_len}...\")\n",
    "    encoder_inputs = tokenizer(source_texts, max_length=max_len, padding='max_length', truncation=True, return_tensors=None)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        decoder_labels = tokenizer(target_texts, max_length=max_len, padding='max_length', truncation=True, return_tensors=None)\n",
    "    encoder_inputs['labels'] = decoder_labels['input_ids']\n",
    "    print(f\"[INFO] Encoding complete.\")\n",
    "    if not encoder_inputs.get('input_ids') or not encoder_inputs.get('labels') or len(encoder_inputs['input_ids']) != len(encoder_inputs['labels']):\n",
    "         print(\"[Warning] Encoding resulted in empty lists or length mismatch.\")\n",
    "    return encoder_inputs\n",
    "\n",
    "class SequencePairDataset(Dataset):\n",
    "    \"\"\"Simple PyTorch Dataset for holding tokenized sequence pair data (as lists).\"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        if not isinstance(encodings, dict) or 'input_ids' not in encodings: raise ValueError(\"Invalid encodings format.\")\n",
    "        self.encodings = encodings\n",
    "        try:\n",
    "            self.length = len(encodings['input_ids'])\n",
    "            for key in encodings:\n",
    "                 if not isinstance(encodings[key], list) or len(encodings[key]) != self.length: raise ValueError(f\"Inconsistent length for key '{key}'.\")\n",
    "        except Exception as e: raise ValueError(f\"Validation failed: {e}\")\n",
    "        if self.length == 0: raise ValueError(\"Input encodings are empty.\")\n",
    "        print(f\"[INFO] Created Dataset with {self.length} examples.\")\n",
    "\n",
    "    def __len__(self): return self.length\n",
    "    def __getitem__(self, idx):\n",
    "        if not 0 <= idx < self.length: raise IndexError(f\"Index {idx} out of bounds.\")\n",
    "        try: return {key: self.encodings[key][idx] for key in self.encodings}\n",
    "        except Exception as e: print(f\"[Error] Failed retrieval at index {idx}: {e}\", file=sys.stderr); raise\n",
    "\n",
    "# --- Custom MoE Layer ---\n",
    "\n",
    "class MoEFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Experts Feed-Forward Network Layer.\n",
    "\n",
    "    Routes input tokens to different 'expert' FFNs based on a gating mechanism.\n",
    "    The gating mechanism here uses the mean-pooled representation of the sequence\n",
    "    to decide the weights for combining expert outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, n_experts=4, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the MoE layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Input and output dimension of the layer.\n",
    "            d_ff (int): Intermediate dimension of the expert FFNs.\n",
    "            n_experts (int): Number of expert networks.\n",
    "            dropout_rate (float): Dropout rate for expert FFNs.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.n_experts = n_experts\n",
    "\n",
    "        # Define the expert networks (simple two-layer MLPs)\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, d_ff),\n",
    "                nn.ReLU(), # Or GeLU, SiLU etc. matching base model if known\n",
    "                nn.Dropout(dropout_rate), # Add dropout within experts\n",
    "                nn.Linear(d_ff, d_model)\n",
    "            ) for _ in range(n_experts)\n",
    "        ])\n",
    "\n",
    "        # Gating network: maps pooled sequence representation to expert weights\n",
    "        # Takes mean across sequence length dimension as input\n",
    "        self.gate = nn.Linear(d_model, n_experts)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        Forward pass through the MoE layer.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): Input tensor (batch_size, seq_len, d_model).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # hidden_states: (B, S, D)\n",
    "        batch_size, seq_len, d_model = hidden_states.shape\n",
    "\n",
    "        # 1. Gating: Use mean pooling for simplicity to get sequence-level weights\n",
    "        # Alternative: Could implement token-level gating for finer control.\n",
    "        pooled_hidden_states = hidden_states.mean(dim=1) # (B, D)\n",
    "\n",
    "        # Calculate gating scores (logits) and normalize to probabilities (weights)\n",
    "        gating_logits = self.gate(pooled_hidden_states) # (B, N_experts)\n",
    "        gating_weights = torch.softmax(gating_logits, dim=-1) # (B, N_experts)\n",
    "\n",
    "        # 2. Expert Evaluation\n",
    "        # Pass the full sequence through each expert\n",
    "        expert_outputs = []\n",
    "        for expert in self.experts:\n",
    "            expert_outputs.append(expert(hidden_states))\n",
    "        # Stack expert outputs: List[ (B, S, D) ] -> Tensor(N_experts, B, S, D)\n",
    "        expert_outputs_stacked = torch.stack(expert_outputs, dim=0)\n",
    "\n",
    "        # 3. Weighted Combination\n",
    "        # Reshape weights for broadcasting: (B, N_experts) -> (B, N_experts, 1, 1)\n",
    "        gating_weights_reshaped = gating_weights.unsqueeze(-1).unsqueeze(-1)\n",
    "        # Reshape expert outputs for broadcasting: (N_experts, B, S, D) -> (B, N_experts, S, D)\n",
    "        expert_outputs_permuted = expert_outputs_stacked.permute(1, 0, 2, 3)\n",
    "\n",
    "        # Weighted sum: (B, N_experts, 1, 1) * (B, N_experts, S, D) -> sum over N_experts axis\n",
    "        # Result: (B, S, D)\n",
    "        output = torch.sum(gating_weights_reshaped * expert_outputs_permuted, dim=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def replace_ffn_with_moe(model, n_experts):\n",
    "    \"\"\"\n",
    "    Replaces the standard FFN layers in a T5 model with MoEFeedForward layers.\n",
    "\n",
    "    Iterates through encoder and decoder blocks and replaces the appropriate layer.\n",
    "    Requires knowledge of the T5Block structure.\n",
    "\n",
    "    Args:\n",
    "        model: The T5ForConditionalGeneration model instance.\n",
    "        n_experts (int): Number of experts for the MoE layers.\n",
    "\n",
    "    Returns:\n",
    "        The modified model instance.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Patching T5 model with MoE layers (Experts: {n_experts})...\")\n",
    "    replaced_count = 0\n",
    "    try:\n",
    "        # Encoder blocks\n",
    "        if hasattr(model, 'encoder') and hasattr(model.encoder, 'block'):\n",
    "            for i, block in enumerate(model.encoder.block):\n",
    "                # T5Block structure: [LayerNorm, SelfAttention, Dropout], [LayerNorm, DenseReluDense, Dropout]\n",
    "                # The FFN (`DenseReluDense`) is typically the second layer in the second sub-block group.\n",
    "                # Accessing via attribute name is safer if known, e.g., block.layer[1].DenseReluDense\n",
    "                # Let's try accessing the assumed attribute location and verify type.\n",
    "                ffn_layer_attr = None\n",
    "                if hasattr(block, 'layer') and len(block.layer) > 1 and \\\n",
    "                   hasattr(block.layer[1], 'DenseReluDense') and \\\n",
    "                   isinstance(block.layer[1].DenseReluDense, nn.Module): # Check if it looks like the FFN component\n",
    "                    ffn_layer_attr = block.layer[1].DenseReluDense\n",
    "                    layer_target = block.layer[1]\n",
    "                    attr_name = 'DenseReluDense'\n",
    "\n",
    "                if ffn_layer_attr is not None:\n",
    "                    print(f\"  - Replacing Encoder Block {i} FFN...\")\n",
    "                    # Extract config needed for MoE layer\n",
    "                    d_model = model.config.d_model\n",
    "                    d_ff = model.config.d_ff\n",
    "                    dropout_rate = model.config.dropout_rate # Get dropout from config\n",
    "\n",
    "                    # Create and assign the new MoE layer\n",
    "                    setattr(layer_target, attr_name, MoEFeedForward(d_model, d_ff, n_experts, dropout_rate))\n",
    "                    replaced_count += 1\n",
    "                else:\n",
    "                     print(f\"  - [Warning] Could not find/replace FFN in Encoder Block {i} structure.\")\n",
    "\n",
    "        # Decoder blocks\n",
    "        if hasattr(model, 'decoder') and hasattr(model.decoder, 'block'):\n",
    "            for i, block in enumerate(model.decoder.block):\n",
    "                # T5DecoderBlock structure: [LN, SelfAttn, Drop], [LN, CrossAttn, Drop], [LN, FFN(DenseReluDense), Drop]\n",
    "                # The FFN is typically the second layer in the *third* sub-block group.\n",
    "                ffn_layer_attr = None\n",
    "                if hasattr(block, 'layer') and len(block.layer) > 2 and \\\n",
    "                   hasattr(block.layer[2], 'DenseReluDense') and \\\n",
    "                   isinstance(block.layer[2].DenseReluDense, nn.Module):\n",
    "                    ffn_layer_attr = block.layer[2].DenseReluDense\n",
    "                    layer_target = block.layer[2]\n",
    "                    attr_name = 'DenseReluDense'\n",
    "\n",
    "                if ffn_layer_attr is not None:\n",
    "                    print(f\"  - Replacing Decoder Block {i} FFN...\")\n",
    "                    d_model = model.config.d_model\n",
    "                    d_ff = model.config.d_ff\n",
    "                    dropout_rate = model.config.dropout_rate\n",
    "\n",
    "                    setattr(layer_target, attr_name, MoEFeedForward(d_model, d_ff, n_experts, dropout_rate))\n",
    "                    replaced_count += 1\n",
    "                else:\n",
    "                    print(f\"  - [Warning] Could not find/replace FFN in Decoder Block {i} structure.\")\n",
    "\n",
    "        if replaced_count == 0:\n",
    "             print(\"[Warning] No FFN layers were replaced. Check model structure and replacement logic.\")\n",
    "        else:\n",
    "             print(f\"[INFO] Successfully replaced {replaced_count} FFN layers with MoE.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed during model patching: {e}\", file=sys.stderr)\n",
    "        # Depending on severity, may want to raise or exit\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Orchestrates data loading, MoE model patching, training, and evaluation.\"\"\"\n",
    "    print(\"[INFO] Starting T5 with Mixture-of-Experts Fine-tuning Script...\")\n",
    "    # --- ***** Update Timestamp/Location ***** ---\n",
    "    try:\n",
    "        current_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "        current_time_str = current_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "    except Exception:\n",
    "        current_time = datetime.datetime.utcnow()\n",
    "        current_time_str = current_time.strftime('%Y-%m-%d %H:%M:%S UTC (naive)')\n",
    "    print(f\"[INFO] Current time: {current_time_str}\")\n",
    "    print(f\"[INFO] Location context: San Diego, CA, USA\")\n",
    "    print(f\"[INFO] Using Base Model: {MODEL_ID}\")\n",
    "\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    try:\n",
    "        train_raw = load_jsonl(TRAIN_FILE)\n",
    "        val_raw = load_jsonl(VAL_FILE)\n",
    "        test_raw = load_jsonl(TEST_FILE)\n",
    "    except Exception as e: print(f\"[FATAL] Data loading failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "    if not train_raw or not val_raw or not test_raw: print(\"[FATAL] Datasets empty after loading.\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 2. Prepare Data Strings ---\n",
    "    train_src, train_tgt = convert_tokens_to_strings(train_raw)\n",
    "    val_src,   val_tgt   = convert_tokens_to_strings(val_raw)\n",
    "    test_src,  test_tgt  = convert_tokens_to_strings(test_raw)\n",
    "    if not train_src or not val_src or not test_src: print(\"[FATAL] Data conversion failed.\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 3. Initialize Tokenizer ---\n",
    "    global tokenizer_for_metrics # For metrics function access\n",
    "    try:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_ID)\n",
    "        # T5 generally uses <pad>, </s>, <unk> - check if needed\n",
    "        if tokenizer.pad_token is None: tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "        # T5 uses EOS as decoder start token ID by default, check if BOS needed\n",
    "        # if tokenizer.bos_token is None: tokenizer.add_special_tokens({'bos_token': '<s>'})\n",
    "        tokenizer_for_metrics = tokenizer\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "    except Exception as e: print(f\"[FATAL] Tokenizer init failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "    print(f\"[INFO] Tokenizer initialized (Vocab: {tokenizer.vocab_size}, Pad ID: {pad_token_id}).\")\n",
    "\n",
    "    # --- 4. Load Base Model and Patch with MoE ---\n",
    "    print(f\"[INFO] Loading base model: {MODEL_ID}\")\n",
    "    try:\n",
    "        model = T5ForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "\n",
    "        # Patch the loaded model\n",
    "        model = replace_ffn_with_moe(model, n_experts=NUM_EXPERTS)\n",
    "\n",
    "        # Resize embeddings if vocab changed\n",
    "        if model.config.vocab_size != len(tokenizer):\n",
    "             print(f\"[INFO] Resizing model embeddings to {len(tokenizer)}\")\n",
    "             model.resize_token_embeddings(len(tokenizer))\n",
    "             model.config.vocab_size = len(tokenizer)\n",
    "\n",
    "        # Configure standard seq2seq settings (T5 specific)\n",
    "        model.config.decoder_start_token_id = tokenizer.pad_token_id # T5 starts decoder with pad\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        # Configure generation defaults\n",
    "        model.config.max_length = MAX_SEQ_LENGTH\n",
    "        model.config.num_beams = 1 # Default to greedy for logits; beams set in TrainingArgs if needed\n",
    "\n",
    "        # Enable Gradient Checkpointing if configured\n",
    "        if USE_GRADIENT_CHECKPOINTING:\n",
    "            try:\n",
    "                 model.gradient_checkpointing_enable()\n",
    "                 print(\"[INFO] Gradient Checkpointing enabled on the model.\")\n",
    "            except Exception as gc_e:\n",
    "                 print(f\"[Warning] Failed to enable gradient checkpointing on model: {gc_e}\")\n",
    "                 global USE_GRADIENT_CHECKPOINTING_EFFECTIVE\n",
    "                 USE_GRADIENT_CHECKPOINTING_EFFECTIVE = False\n",
    "        else:\n",
    "             USE_GRADIENT_CHECKPOINTING_EFFECTIVE = False\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Failed to initialize or patch the T5 model: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    # --- 5. Tokenize Data ---\n",
    "    try:\n",
    "        train_encodings = encode_sequences(tokenizer, train_src, train_tgt, MAX_SEQ_LENGTH)\n",
    "        val_encodings   = encode_sequences(tokenizer, val_src,   val_tgt,   MAX_SEQ_LENGTH)\n",
    "        test_encodings  = encode_sequences(tokenizer, test_src,  test_tgt,  MAX_SEQ_LENGTH)\n",
    "    except Exception as e: print(f\"[FATAL] Data tokenization failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "    if not train_encodings.get('input_ids') or not val_encodings.get('input_ids') or not test_encodings.get('input_ids'):\n",
    "        print(\"[FATAL] Tokenization resulted in empty encodings.\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 6. Create Datasets ---\n",
    "    try:\n",
    "        train_dataset = SequencePairDataset(train_encodings)\n",
    "        val_dataset   = SequencePairDataset(val_encodings)\n",
    "        test_dataset  = SequencePairDataset(test_encodings)\n",
    "    except ValueError as e: print(f\"[FATAL] Dataset creation failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 7. Initialize Data Collator ---\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100, # Ignore pad tokens in loss\n",
    "        pad_to_multiple_of=8 if USE_FP16 else None\n",
    "    )\n",
    "    print(\"[INFO] Data collator initialized.\")\n",
    "\n",
    "    # --- 8. Define Metrics Computation (from Logits) ---\n",
    "    def compute_metrics_fn(eval_pred):\n",
    "        \"\"\"Calculates exact sequence match accuracy from logits.\"\"\"\n",
    "        logits, labels = eval_pred # Trainer passes logits when predict_with_generate=False\n",
    "        if not isinstance(logits, np.ndarray): logits = np.array(logits)\n",
    "        if not isinstance(labels, np.ndarray): labels = np.array(labels)\n",
    "\n",
    "        # Replace -100 in labels for accuracy calculation comparison if needed,\n",
    "        # but accuracy function should handle pad_token_id masking.\n",
    "        # labels_for_acc = np.where(labels != -100, labels, tokenizer_for_metrics.pad_token_id)\n",
    "\n",
    "        try:\n",
    "            # Use PyTorch tensor version for consistency and potential device placement\n",
    "            logits_torch = torch.from_numpy(logits)\n",
    "            labels_torch = torch.from_numpy(labels)\n",
    "            # Pass the original labels (with -100 potentially) and pad_token_id\n",
    "            accuracy = calculate_sequence_accuracy_torch(logits_torch, labels_torch, tokenizer_for_metrics.pad_token_id)\n",
    "            return {\"sequence_accuracy\": accuracy}\n",
    "        except Exception as met_e:\n",
    "             print(f\"[Error] compute_metrics failed: {met_e}\", file=sys.stderr)\n",
    "             return {\"sequence_accuracy\": 0.0}\n",
    "\n",
    "    def calculate_sequence_accuracy_torch(logits, labels, pad_token_id):\n",
    "        \"\"\"Calculates exact sequence match accuracy using PyTorch tensors.\"\"\"\n",
    "        # Ensure inputs are tensors\n",
    "        if not isinstance(logits, torch.Tensor): logits = torch.tensor(logits)\n",
    "        if not isinstance(labels, torch.Tensor): labels = torch.tensor(labels)\n",
    "\n",
    "        if logits.shape[0] == 0: return 0.0\n",
    "        predictions = torch.argmax(logits, dim=-1) # (Batch, SeqLen)\n",
    "\n",
    "        # Create mask for non-padding tokens in labels (use pad_token_id directly)\n",
    "        # Labels might contain -100, treat them as padding for accuracy check\n",
    "        non_pad_mask = (labels != pad_token_id) & (labels != -100)\n",
    "\n",
    "        correct_tokens = (predictions == labels) & non_pad_mask\n",
    "        correct_sequences = (torch.sum(correct_tokens, dim=1) == torch.sum(non_pad_mask, dim=1))\n",
    "        accuracy = torch.mean(correct_sequences.float())\n",
    "        return accuracy.item()\n",
    "\n",
    "\n",
    "    # --- 9. Define Training Arguments ---\n",
    "    effective_gc = USE_GRADIENT_CHECKPOINTING if 'USE_GRADIENT_CHECKPOINTING_EFFECTIVE' not in globals() else USE_GRADIENT_CHECKPOINTING_EFFECTIVE\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR),\n",
    "        # Schedule\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        # Logging / Saving / Evaluation\n",
    "        logging_dir=str(OUTPUT_DIR / 'logs'),\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        evaluation_strategy=EVALUATION_STRATEGY,\n",
    "        save_strategy=SAVE_STRATEGY,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"sequence_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        # Use logits for evaluation (faster if only accuracy needed)\n",
    "        predict_with_generate=False,\n",
    "        # Advanced features\n",
    "        fp16=USE_FP16,\n",
    "        label_smoothing_factor=LABEL_SMOOTHING_FACTOR,\n",
    "        gradient_checkpointing=effective_gc,\n",
    "        # report_to=\"tensorboard\", # Optional\n",
    "    )\n",
    "    print(\"[INFO] Training arguments defined.\")\n",
    "    print(f\"[INFO] Effective Mixed Precision (FP16): {'Enabled' if USE_FP16 else 'Disabled'}\")\n",
    "    print(f\"[INFO] Effective Label Smoothing Factor: {LABEL_SMOOTHING_FACTOR}\")\n",
    "    print(f\"[INFO] Effective Gradient Checkpointing in Args: {training_args.gradient_checkpointing}\")\n",
    "\n",
    "    # --- 10. Initialize Trainer ---\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "    )\n",
    "    print(\"[INFO] Seq2SeqTrainer initialized.\")\n",
    "\n",
    "    # --- 11. Train the Model ---\n",
    "    print(f\"[INFO] Starting model training ({NUM_TRAIN_EPOCHS} epochs)...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics); trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(\"[INFO] Training finished successfully.\")\n",
    "        if trainer.state.best_model_checkpoint: print(f\"[INFO] Best model checkpoint: {trainer.state.best_model_checkpoint}\")\n",
    "    except Exception as e: print(f\"[FATAL] Training failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    # --- 12. Evaluate on Test Set ---\n",
    "    print(\"[INFO] Evaluating final model on the test set...\")\n",
    "    try:\n",
    "        test_results = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")\n",
    "        trainer.log_metrics(\"test\", test_results); trainer.save_metrics(\"test\", test_results)\n",
    "        if 'test_sequence_accuracy' in test_results:\n",
    "             print(f\"\\n--- Test Set Results ---\"); print(f\"Test Sequence Accuracy: {test_results['test_sequence_accuracy']:.4f}\"); print(f\"-----------------------------\")\n",
    "        else: print(\"[Warning] Test accuracy metric not found.\", \"Results:\", test_results)\n",
    "    except Exception as e: print(f\"[Error] Final evaluation failed: {e}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    print(\"\\n[INFO] Script finished successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Optional: Seeds for reproducibility\n",
    "    # SEED = 42; random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "    # if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
